<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.8.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="sqoop 自动化脚本">




  <meta name="keywords" content="sqoop,">





  <link rel="alternate" href="/atom.xml" title="WHOAMI">




  <link rel="shortcut icon" type="image/x-icon" href="https://raw.githubusercontent.com/itweet/itweet.github.io/master/favicon.ico?v=1.1">



<link rel="canonical" href="http://itweet.github.io/2016/03/10/sqoop-自动化脚本/">


<meta name="description" content="Sqoop是一款开源的工具，主要用于在HADOOP(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS,NOSQL中，也可以将HDFS的数据导进到关系型数据库中。Sqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使">
<meta name="keywords" content="sqoop">
<meta property="og:type" content="article">
<meta property="og:title" content="sqoop 自动化脚本">
<meta property="og:url" content="http://itweet.github.io/2016/03/10/sqoop-自动化脚本/index.html">
<meta property="og:site_name" content="WHOAMI">
<meta property="og:description" content="Sqoop是一款开源的工具，主要用于在HADOOP(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS,NOSQL中，也可以将HDFS的数据导进到关系型数据库中。Sqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://www.itweet.cn/screenshots/sqoop-rdbms.gif">
<meta property="og:updated_time" content="2019-02-27T17:47:23.223Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="sqoop 自动化脚本">
<meta name="twitter:description" content="Sqoop是一款开源的工具，主要用于在HADOOP(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS,NOSQL中，也可以将HDFS的数据导进到关系型数据库中。Sqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使">
<meta name="twitter:image" content="https://www.itweet.cn/screenshots/sqoop-rdbms.gif">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  



    <title> sqoop 自动化脚本 - WHOAMI </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">WHOAMI</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/archives">
                            
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/atom.xml">
                            
                            
                                RSS
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          sqoop 自动化脚本
        
      </h1>

      <time class="post-time">
          3月 10 2016
      </time>
    </header>



    
            <div class="post-content">
            <p>Sqoop是一款开源的工具，主要用于在HADOOP(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS,NOSQL中，也可以将HDFS的数据导进到关系型数据库中。<br>Sqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使用者能够快速部署，也为了让开发人员能够更快速的迭代开发，Sqoop独立成为一个Apache项目。</p>
<p><img src="https://www.itweet.cn/screenshots/sqoop-rdbms.gif" alt></p>
<h1 id="1、sqoop-auto-hive-amp-amp-snappy"><a href="#1、sqoop-auto-hive-amp-amp-snappy" class="headerlink" title="1、sqoop auto hive &amp;&amp; snappy"></a>1、sqoop auto hive &amp;&amp; snappy</h1><ul>
<li><p>(1)-m 参数的说明，代表含义是使用多少个并行，如果参数值是1说明没有开启并功能。</p>
</li>
<li><p>(2)-m 参数的值调大，使用并行导入的功能，如果-m 4说明可以并行开启4个进程,同时进行数据导入。</p>
</li>
<li><p>(3)如果从Oracle中导入的表没有主键，那么会出现如下的错误提示：<br>   <code>ERROR tool.ImportTool: Error during import: No primary key could be</code><br>   <code>found for table tmp.tbls . Please specify one with --split-by or perform</code><br>   <code>a sequential import with &#39;-m 1&#39;.</code></p>
</li>
<li><p>(4)需要手动指定一个oracle字段，SQOOP开启并行导入功能，首先他会根据这个字段<br> 执行如下查询：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select min(local_code), max(local_code) from pu_web.bigdata_area_code where</span><br><span class="line">data_desc=&apos;2016-02-26&apos; [where 如果指定了where子句,没有则忽略] ==&gt; 0691,9999</span><br></pre></td></tr></table></figure>
<p>然后sqoop会根据并行导入-m值，进行拆分，比如上面的结果0691-9999，-m 16那么会被均分为16份，并行执行导入。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from table where 0 &lt;= id &lt; 0691+(9999-0691)/16;  累死这样的算法，均分16个范围，并行导入数据。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><code>注意，拆分的字段需要是整数。</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1、测试连接</span><br><span class="line">sqoop list-tables --connect jdbc:oracle:thin:@oracle_ip:1521:orcl --username TMP --password 123</span><br><span class="line"></span><br><span class="line">2、删除已存在表</span><br><span class="line">hive -e &apos;drop table if exists tmp.barea_code&apos;</span><br><span class="line"></span><br><span class="line">3、自动建立hive表，并且导入数据到相应目录</span><br><span class="line">sqoop import  -D mapreduce.job.queuename=mapred --connect jdbc:oracle:thin:@oracle_ip:1521:orcl --username TMP --password 123 --table BAREA_CODE --fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; --hive-import --create-hive-table --hive-overwrite --hive-table tmp.barea_code --null-string &apos;\\N&apos; --null-non-string &apos;\\N&apos;  --as-parquetfile --compression-codec &quot;org.apache.hadoop.io.compress.SnappyCodec&quot; -m 16 --split-by local_code</span><br><span class="line"></span><br><span class="line">4、重写已经存在hive表的数据</span><br><span class="line">sqoop import  -D mapreduce.job.queuename=mapred --connect jdbc:oracle:thin:@oracle_ip:1521:orcl --username TMP --password 123 --table BAREA_CODE --fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; --hive-import --hive-overwrite --hive-table tmp.barea_code --null-string &apos;\\N&apos; --null-non-string &apos;\\N&apos; --compression-codec &quot;org.apache.hadoop.io.compress.SnappyCodec&quot; -m 16 --split-by local_code</span><br></pre></td></tr></table></figure>
<blockquote>
<p>上面命令优点：<br>  1、任务放到mapred任务队列<br>  2、增加并行执行度，提升效率<br>  3、文件以snappy格式存放到集群,目前snappy压缩，是最佳选择。</p>
</blockquote>
<ul>
<li>(5)、auto rdbms to hive &amp;&amp; compress SnappyCodec<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">$ cat rdbms_to_hive_auto.py </span><br><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding: utf-8</span><br><span class="line">__author__ = &apos;whoami&apos;</span><br><span class="line">import commands,logging</span><br><span class="line"></span><br><span class="line">def is_num_by_except(num):</span><br><span class="line">        try:</span><br><span class="line">                int(num)</span><br><span class="line">                return True</span><br><span class="line">        except ValueError:</span><br><span class="line">                # print &quot;%s ValueError&quot; % num</span><br><span class="line">                return False</span><br><span class="line"></span><br><span class="line">def db_settings():</span><br><span class="line">        # db info format --&gt; id,db_username,jdbc_info,db_passswd</span><br><span class="line">        db_info = &#123;1:&apos;WEB,jdbc:oracle:thin:@host_ip:1521:orcl,123&apos;, </span><br><span class="line">                   2:&apos;TARI,jdbc:oracle:thin:@host_ip:1521:orcl,123&apos;,</span><br><span class="line">                   3:&apos;TEST,jdbc:oracle:thin:@host_ip:1521:orcl,123&apos;,</span><br><span class="line">                   4:&apos;mysql,com.mysql.jdbc.Driver,admin123&apos;&#125;</span><br><span class="line"></span><br><span class="line">        return db_info</span><br><span class="line"></span><br><span class="line">def database_info():</span><br><span class="line">        lines = db_settings()</span><br><span class="line">        print &apos;SQOOP:Oracle jdbc connect info...&apos;</span><br><span class="line">        for k,v in lines.items():</span><br><span class="line">                print &apos;         &apos;,k,v</span><br><span class="line"></span><br><span class="line">def get_matche_conn(db):</span><br><span class="line"></span><br><span class="line">        while 1:</span><br><span class="line">                number = raw_input(&quot;Please input number: &quot;)</span><br><span class="line">                if is_num_by_except(number):</span><br><span class="line">                        con = db.get(int(number))</span><br><span class="line">                        return con</span><br><span class="line">                        break</span><br><span class="line">                else:</span><br><span class="line">                        logging.error(&apos; Input types Error,please input number types... &apos;)</span><br><span class="line">                        continue</span><br><span class="line"></span><br><span class="line">def is_sqoop_split():</span><br><span class="line">        is_split = raw_input(&quot;Do you want support sqoop split [y/n] (n)?&quot;)</span><br><span class="line">        if &apos;y&apos; is is_split:</span><br><span class="line">                return True</span><br><span class="line">        else:</span><br><span class="line">                return False</span><br><span class="line"></span><br><span class="line">def sqoop_split_column():</span><br><span class="line">          while 1:</span><br><span class="line">                sqoop_split_cloumn = raw_input(&quot;Please input sqoop split cloumn,cloumn type is integer: &quot;)</span><br><span class="line">                number = raw_input(&quot;Please input sqoop split map concurrent execution number: &quot;)</span><br><span class="line">                </span><br><span class="line">                if is_num_by_except(number):</span><br><span class="line">                        return sqoop_split_cloumn,number</span><br><span class="line">                        break</span><br><span class="line">                else:</span><br><span class="line">                        logging.error(&apos; Input types Error,please input number types... &apos;)</span><br><span class="line">                        continue</span><br><span class="line"></span><br><span class="line">def delete_hive_table(table_name):</span><br><span class="line">        command = &quot;hive -e &apos;drop table if exists %s&apos;&quot; %(table_name)</span><br><span class="line">        logging.warn(&apos;starting drop table if exists %s ... &apos; %table_name)</span><br><span class="line">        status,result = commands.getstatusoutput(command)</span><br><span class="line"></span><br><span class="line">        if status == 0:</span><br><span class="line">                logging.warn(result)</span><br><span class="line">                logging.warn(&apos;drop hive table success... &apos;),</span><br><span class="line">        else:</span><br><span class="line">                logging.warn(&apos;drop hive table fail...&apos;)</span><br><span class="line"></span><br><span class="line">def raw_table():</span><br><span class="line">        rdbms_table_name = raw_input(&quot;Please input rdbms table name: &quot;)</span><br><span class="line"></span><br><span class="line">        hive_table_name = raw_input(&quot;Please input hive table name: &quot;)</span><br><span class="line"></span><br><span class="line">        return rdbms_table_name,hive_table_name</span><br><span class="line"></span><br><span class="line">def sqoop_task_parse(is_split,rdbms_table_name,hive_table_name,rdbms_username,rdbms_jdbc,rdbms_passwd):</span><br><span class="line">        if is_split:</span><br><span class="line">                split_column,split_m=sqoop_split_column()</span><br><span class="line">                sqoop_import = &apos;sqoop import  -D mapreduce.job.queuename=db --connect %s --username %s --password %s --table %s --fields-terminated-by &quot;\\t&quot; --lines-terminated-by &quot;\\n&quot; --hive-import --create-hive-table --hive-table %s --compression-codec &quot;org.apache.hadoop.io.compress.SnappyCodec&quot; -m %s --split-by %s&apos; %(rdbms_jdbc,rdbms_username,rdbms_passwd,rdbms_table_name,hive_table_name,split_m,split_column) + &quot; --null-string &apos;\\\N&apos; --null-non-string &apos;\\\N&apos;&quot;</span><br><span class="line">                return sqoop_import</span><br><span class="line">        else:</span><br><span class="line">                sqoop_import = &apos;sqoop import  -D mapreduce.job.queuename=db --connect %s --username %s --password %s --table %s --fields-terminated-by &quot;\\t&quot; --lines-terminated-by &quot;\\n&quot; --hive-import --create-hive-table --hive-table %s --compression-codec &quot;org.apache.hadoop.io.compress.SnappyCodec&quot; -m 1&apos; %(rdbms_jdbc,rdbms_username,rdbms_passwd,rdbms_table_name,hive_table_name) + &quot; --null-string &apos;\\\N&apos; --null-non-string &apos;\\\N&apos;&quot;</span><br><span class="line">                return sqoop_import</span><br><span class="line"></span><br><span class="line">def sqoop_task(sqoop_import):</span><br><span class="line">        logging.warn(&apos;starting sqoop import rdbms to hive ...&apos;)</span><br><span class="line">        status,result = commands.getstatusoutput(sqoop_import)</span><br><span class="line"></span><br><span class="line">        if status == 0:</span><br><span class="line">                logging.warn(result)</span><br><span class="line">                logging.warn(&apos;sqoop import rdbms to hive success... &apos;),</span><br><span class="line">        else:</span><br><span class="line">                logging.warn(&apos;sqoop import rdbms to hive fail...&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def task_execute(hive_table_name,sqoop_import):</span><br><span class="line">        delete_hive_table(hive_table_name)</span><br><span class="line">        print sqoop_import</span><br><span class="line">        sqoop_task(sqoop_import)</span><br><span class="line"></span><br><span class="line">def sqoop_main():</span><br><span class="line">        database_info()</span><br><span class="line">        db = db_settings()</span><br><span class="line">        conn = get_matche_conn(db).strip().split(&apos;,&apos;)</span><br><span class="line">        rdbms_table_name,hive_table_name = raw_table()</span><br><span class="line">        is_split = is_sqoop_split() </span><br><span class="line">        sqoop_import = sqoop_task_parse(is_split,rdbms_table_name,hive_table_name,conn[0],conn[1],conn[2])</span><br><span class="line">        task_execute(hive_table_name,sqoop_import)</span><br><span class="line"></span><br><span class="line">sqoop_main()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="2、auto-hdfs-to-oracle"><a href="#2、auto-hdfs-to-oracle" class="headerlink" title="2、auto hdfs to oracle"></a>2、auto hdfs to oracle</h1><h1 id="3、问题整理？"><a href="#3、问题整理？" class="headerlink" title="3、问题整理？"></a>3、问题整理？</h1><ul>
<li><p>Oracle导数据需要注意,表明和库明要大写,否则会找不到表。</p>
</li>
<li><p>导parquet格式数据:</p>
<ul>
<li><p>ERROR：Dataset name tmp.barea_code is not alphanumeric (plus ‘_’)</p>
</li>
<li><p>impala 生成的parquet表无法导回oracle,mr生成的parquet能导回oracle</p>
</li>
<li><p>Import null内容处理，select * from d_area_code where area_name is null;</p>
<ul>
<li>–null-string ‘\N’</li>
<li>–null-non-string ‘\N’</li>
<li>rdbms导入数据到Hadoop集群，加入上面两个参数，执行上面SQL可以过滤出结果</li>
<li>不加上面两个参数，导入hdfs之后结果变成’null‘，执行上面SQL无法过滤出结果</li>
<li>如果加入参数，可以过滤出结果,显示’NULL’内容。</li>
</ul>
</li>
<li><p>Export null内容处理和Import道理一致</p>
<ul>
<li>–input-null-string ‘\N’</li>
<li>–input-null-non-string ‘\N’</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>原创文章，转载请注明： 转载自<a href="http://www.itweet.cn" target="_blank" rel="noopener">Itweet</a>的博客<br><code>本博客的文章集合:</code> <a href="http://www.itweet.cn/blog/archive/" target="_blank" rel="noopener">http://www.itweet.cn/blog/archive/</a></p>

            </div>
          

    
      <footer class="post-footer">
		
		<div class="post-tags">
		  
			<a href="/tags/sqoop/">sqoop</a>
		  
		</div>
		

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2016/03/15/Hadoop列式存储引擎Parquet-ORC和snappy压缩/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Hadoop列式存储引擎Parquet-ORC和snappy压缩</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2016/01/26/Hadoop-par1/">
        <span class="next-text nav-default">Hadoop平台架构--硬件篇</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2014 -
    
    2019
    <span class="footer-author">Xu Jiang.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/frostfan/hexo-theme-polarbear">Polar Bear</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
