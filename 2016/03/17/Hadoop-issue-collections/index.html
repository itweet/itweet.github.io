<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.8.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="Hadoop issue collections">




  <meta name="keywords" content="hadoop,">





  <link rel="alternate" href="/atom.xml" title="WHOAMI">




  <link rel="shortcut icon" type="image/x-icon" href="https://raw.githubusercontent.com/itweet/itweet.github.io/master/favicon.ico?v=1.1">



<link rel="canonical" href="http://itweet.github.io/2016/03/17/Hadoop-issue-collections/">


<meta name="description" content="整理在工作中遇到的Hadoop平台遇到的报错信息，以及解决思路。 问题1：基于Yarn统一资源管理平台配置导致 错误信息：Application application_1458180019333_0002 failed 2 times due to AM Containerfor appattempt_1458180019333_0002_000002 exited with exitCode:">
<meta name="keywords" content="hadoop">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop issue collections">
<meta property="og:url" content="http://itweet.github.io/2016/03/17/Hadoop-issue-collections/index.html">
<meta property="og:site_name" content="WHOAMI">
<meta property="og:description" content="整理在工作中遇到的Hadoop平台遇到的报错信息，以及解决思路。 问题1：基于Yarn统一资源管理平台配置导致 错误信息：Application application_1458180019333_0002 failed 2 times due to AM Containerfor appattempt_1458180019333_0002_000002 exited with exitCode:">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-02-27T17:45:54.903Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hadoop issue collections">
<meta name="twitter:description" content="整理在工作中遇到的Hadoop平台遇到的报错信息，以及解决思路。 问题1：基于Yarn统一资源管理平台配置导致 错误信息：Application application_1458180019333_0002 failed 2 times due to AM Containerfor appattempt_1458180019333_0002_000002 exited with exitCode:">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  



    <title> Hadoop issue collections - WHOAMI </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">WHOAMI</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/archives">
                            
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/atom.xml">
                            
                            
                                RSS
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Hadoop issue collections
        
      </h1>

      <time class="post-time">
          3月 17 2016
      </time>
    </header>



    
            <div class="post-content">
            <p>整理在工作中遇到的Hadoop平台遇到的报错信息，以及解决思路。</p>
<h1 id="问题1：基于Yarn统一资源管理平台配置导致"><a href="#问题1：基于Yarn统一资源管理平台配置导致" class="headerlink" title="问题1：基于Yarn统一资源管理平台配置导致"></a>问题1：基于Yarn统一资源管理平台配置导致</h1><ul>
<li><p>错误信息：<br><code>Application application_1458180019333_0002 failed 2 times due to AM Container</code><br><code>for appattempt_1458180019333_0002_000002 exited with exitCode: -1000</code><br><code>For more detailed output, check application tracking page:http://</code><br><code>server-02:8088/proxy/application_1458180019333_0002/Then, click on links to</code><br><code>logs of each attempt.Diagnostics: Not able to initialize user directories in</code><br><code>any of the configured local directories for user hadoop</code><br><code>Failing this attempt. Failing the application.</code></p>
</li>
<li><p>原因：</p>
<p>Actually this is due to the permission issues on some of the yarn local directories. I started using LinuxContainerExecutor (in non secure mode with nonsecure-mode.local-user as kailash) and made corresponding changes. However due to some (unknown) reason NodeManager failed to clean local directories for users, and there still existed directories with previous user (in my case yarn).<br>So to solve this, I first had to find the value of the property yarn.nodemanager.local-dirs (with Cloudera use search option to find this property for YARN service, otherwise look into yarn-site.xml in hadoop conf directory), and then delate the files/directories under usercache for all the node manager nodes. In my case, I used:</p>
</li>
</ul>
<blockquote>
<p>rm -rf /yarn/nm/usercache/*</p>
</blockquote>
<ul>
<li><p>问题分析：</p>
<p>在配置impala基于yarn的完全自由管理，最后发现集群硬件严重不一致，llama项目让impala可以通过yarn申请资源，配置成功后，在执行小的复杂查询时没有任何问题，一旦数据量太大，会出现资源分配的问题，导致无法提供足够资源而报错，impalad mem_limit安装官网也做了限制了，并没有得到解决。最后取消配置，集群一切正常，再提交mapreducer,spark等基于yarn的计算引擎之时，发现各上面错误信息，一个<br>任务都无法执行成功。排查问题步骤。</p>
<ul>
<li>1、通过日志信息，确定大概方向。</li>
<li>2、在开源社区各大国外论坛寻找解决方案，未果。</li>
<li>3、排查源码，是什么导致输出次错误信息，发现是因为权限问题无法初始化。</li>
<li>4、首先尝试找到相关yarn临时目录，备份目录后删除cache目录。重新提交任务初始化。</li>
<li><p>5、整合解决问题时间大概在3个小时。就是一个权限问题。</p>
<ul>
<li><p>配置基于yarn完全资源管理后权限：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l /data01/yarn/nm/usercache/</span><br><span class="line">  total 1</span><br><span class="line">  drwxr-s--- 4 nobody yarn 4096 Feb 24 00:19 hadoop</span><br></pre></td></tr></table></figure>
</li>
<li><p>未配置基于yarn完全资源管理权限：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l /data01/yarn/nm/usercache/</span><br><span class="line">total 1</span><br><span class="line">drwxr-x--- 4 yarn yarn 4096 Mar 17 11:33 hadoop</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p>找到：yarn.nodemanager.local-dirs ，删除这个路径下面的<br>usercache目录中的所有文件包括目录，即可解决。</p>
</li>
</ul>
<h1 id="问题2：Container-xxx-is-running-beyond-physical-memory-limits"><a href="#问题2：Container-xxx-is-running-beyond-physical-memory-limits" class="headerlink" title="问题2：Container xxx is running beyond physical memory limits"></a>问题2：Container xxx is running beyond physical memory limits</h1><ul>
<li><p>日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Container [pid=134663,containerID=container_1430287094897_0049_02_067966] is running beyond physical memory limits. Current usage: 1.0 GB of 1 GB physical memory used; 1.5 GB of 10 GB virtual memory used. Killing container. Dump of the process-tree for</span><br></pre></td></tr></table></figure>
</li>
<li><p>问题分析：<br>从日志可以看出，container使用内存超过虚拟内存的限制，导致如上问题。默认2.1；<br>NodeManager端设置，类似系统层面的overcommit问题,需要调节yarn.nodemanager.vmem-pmem-ratio相关参数，在yarn-site.xml修改:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;10&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> –或者yarn.nodemanager.vmem-check-enabled，false掉 </span><br><span class="line"> &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;false&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="问题3：jvm系常见java-heap-space"><a href="#问题3：jvm系常见java-heap-space" class="headerlink" title="问题3：jvm系常见java heap space"></a>问题3：jvm系常见java heap space</h1><ul>
<li><p>日志</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx java heap space/ java heap space xxx 各种oom信息</span><br></pre></td></tr></table></figure>
</li>
<li><p>问题分析</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">通过日志定位问题，一般调节参数思路，相关mem,heap设置参数：</span><br><span class="line">  -内存：mapreduce.map.memory.mb</span><br><span class="line">  –Heap Size：-Xmx在mapreduce.map.java.opts做相同调整</span><br><span class="line">  –内存：mapreduce.reduce.memory.mb</span><br><span class="line">  –Heap Size：-Xmx在mapreduce.reduce.java.opts做相同调整</span><br></pre></td></tr></table></figure>
</li>
<li><p>yarn资源管理&amp;&amp;优化参考如下文章</p>
<ul>
<li><a href="https://www.itweet.cn/2015/07/24/yarn-resources-manager-allocation/" target="_blank" rel="noopener">yarn-resources-manager-allocation</a></li>
</ul>
</li>
</ul>
<h1 id="问题4-llama-on-yarn使用问题"><a href="#问题4-llama-on-yarn使用问题" class="headerlink" title="问题4: llama on yarn使用问题"></a>问题4: llama on yarn使用问题</h1><ul>
<li><p>日志</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ERROR:  com.cloudera.llama.util.LlamaException: RESERVATION_ASKING_MORE_MB - Reservation &apos;44590e9da89d8e3:a5ba70d7dcb599a3&apos;, expansion &apos;null&apos; is asking for more memory in mb &apos;129008&apos; than capacity &apos;76800&apos; on node &apos;bigdata-server-04&apos;., com.cloudera.llama.util.LlamaException: RESERVATION_ASKING_MORE_MB - Reservation &apos;44590e9da89d8e3:a5ba70d7dcb599a3&apos;, expansion &apos;null&apos; is asking for more memory in mb &apos;129008&apos; than capacity &apos;76800&apos; on node &apos;server-04&apos;.,        at com.cloudera.llama.am.impl.ExpansionReservationsLlamaAM.checkAndUpdateCapacity(ExpansionReservationsLlamaAM.java:170),       at com.cloudera.llama.am.impl.ExpansionReservationsLlamaAM.reserve(ExpansionReservationsLlamaAM.java:129),     at com.cloudera.llama.am.impl.APIContractLlamaAM.reserve(APIContractLlamaAM.java:144),         at com.cloudera.llama.am.LlamaAMServiceImpl.Reserve(LlamaAMServiceImpl.java:132),       at com.cloudera.llama.am.MetricLlamaAMService.Reserve(MetricLlamaAMService.java:140),  at com.cloudera.llama.thrift.LlamaAMService$Processor$Reserve.getResult(LlamaAMService.java:512),      at com.cloudera.llama.thrift.LlamaAMService$Processor$Reserve.getResult(LlamaAMService.java:497),       at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39),         at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39),    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206),   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145),    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615),     at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">WARNING: The following tables are missing relevant table and/or column statistics.</span><br><span class="line">bigdata.terminal_all</span><br></pre></td></tr></table></figure>
</li>
<li><p>分析问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">根据错误日志，去cloudera官网blog各种翻文档，查看相关内存配置参数和限制，集群硬件mem严重不一致，在资源评估分配的时候，impalad分别在这些不同配置mem的节点，</span><br><span class="line">根据官网说法mem_limit也做了限制，依然无法解决。impalad申请内存是非常夸张的方式，如下参数mem调整。</span><br><span class="line"></span><br><span class="line">==&gt; Impala Daemon 内存限制，mem_limit，impalad内存设置导致问题：导致的问题如下，需要限制impalad</span><br><span class="line">内存使用，注意如果一个集群有多种不同硬件配置</span><br><span class="line">yarn.nodemanager.resource.memory-mb这个参数注意限制。</span><br><span class="line"></span><br><span class="line">单表数据量太大，虽然做了各种限制，但是依然会报错，未解决，还不够成熟和yarn集成，取消llama配置，独立部署模式就可以了，一模一样的SQL可以执行成功，我无言以对。</span><br></pre></td></tr></table></figure>
</li>
<li><p>问题小结</p>
<p>问题原因：由于集群中，上中下有三种配置服务器，内存在65-128g，128-256g都有，而hdfs，impala,hbase,yarn,sparkstreaming，prestodb都分布在这些节点上面，impalad是必须和datanode在一个节点，这就涉及到资源分配问题。通过简单对集群角色3组，不同组角色在不同硬件资源，mem,cpu,磁盘有不同的三套配置。llama项目使得impala资源分配交给Yarn。在这样资源严重不一致情况下。llama评估资源在某些节点实际资源就不和大环境统一，从而出现问题。</p>
<p>解决问题方案：尽快引用hadoop 2.7+版本提高的基于标签调度策略，可以给集群不同<br>nodemanager节点打标签，提交任务的时候可以指定什么类型应用提交到那些节点执行，<br>从而在同一个集群中实现异构集群的思路。YARN某些资源池下面拥有某些高性能高配置<br>服务器，某些低性能低配置在一个资源池，某些不上不下配置的服务器，同时拥有高性能标签，低性能标签。调度的时候，会同时存在不同的技术引擎的任务；比如：内存计算代表:spark,tez,impala,drill,presto，磁盘计算：mr,hive,pig,mahout，流计算：sparkstreaming,storm。不同任务指定到不同的资源池。达到基于YARN的统一资源管理和调度。</p>
</li>
</ul>
<h1 id="问题5：升级Namenode-HA出现Hive-metadata信息访问hdfs问题"><a href="#问题5：升级Namenode-HA出现Hive-metadata信息访问hdfs问题" class="headerlink" title="问题5：升级Namenode HA出现Hive metadata信息访问hdfs问题"></a>问题5：升级Namenode HA出现Hive metadata信息访问hdfs问题</h1><ul>
<li><p>日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">he.hadoop.mapreduce.TaskCounter instead</span><br><span class="line">2015-06-20 16:28:54,901 ERROR [main]: exec.Task (SessionState.java:printError(545)) - Failed with exception Wrong FS: hdfs://server-01:8020/apps/hive/temp/temp_base, expected: hdfs://mycluster</span><br><span class="line">java.lang.IllegalArgumentException: Wrong FS: hdfs://server-01:8020/apps/hive/temp/temp_base, expected: hdfs://mycluster</span><br><span class="line">      at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:642)</span><br><span class="line">      at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:181)</span><br></pre></td></tr></table></figure>
</li>
<li><p>分析问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">升级HA之后hive源数据信息hdfs地址问题，排查hive元数据信息发现，原来是hdfs路径</span><br><span class="line">原来时某个节点非HA的地址，升级namenode为HA之后，地址并没有跟着变化，需要手动</span><br><span class="line">修改hive 元数据信息比如存储在mysql中hdfs路径信息为HA的地址，可以解决。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="问题6：HDFS-ERROR-Failed-to-close-inode-330916"><a href="#问题6：HDFS-ERROR-Failed-to-close-inode-330916" class="headerlink" title="问题6：HDFS ERROR - Failed to close inode 330916"></a>问题6：HDFS ERROR - Failed to close inode 330916</h1><ul>
<li><p>日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HDFS报错: Failed to close inode 330916</span><br><span class="line">16/03/15 23:19:17 INFO util.HiveUtil: Run Hive Sql:MSCK REPAIR TABLE TB_INTERF_KW_TEMP</span><br><span class="line"> 16/03/15 23:19:18 ERROR hdfs.DFSClient: Failed to close inode 330916</span><br><span class="line"> org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /tmp/abc/data/dictionary/KwTemp.txt (inode 330916): File does not exist. Holder DFSClient_NONMAPREDUCE_-1085844490_1 does not have any open files.</span><br></pre></td></tr></table></figure>
</li>
<li><p>分析问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">程序使用HDFS api操作的时候，在mr中操作，判断一个目录是否存在，而导致的问题，总之是API和集群沟通问题。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="问题7-根目录空间利用率100-，导致namenode进入安全模式"><a href="#问题7-根目录空间利用率100-，导致namenode进入安全模式" class="headerlink" title="问题7: 根目录空间利用率100%，导致namenode进入安全模式"></a>问题7: 根目录空间利用率100%，导致namenode进入安全模式</h1><p>  最初报错信息如下：<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /tmp/hive/root/e5e52596-727b-414d-a02a-b78aeb44109d. Name node is in safe mode.</span><br><span class="line">Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use &quot;hdfs dfsadmin -safemode leave&quot; to turn safe mode off.</span><br></pre></td></tr></table></figure></p>
<p>1、进入超级用户，尝试让hdfs退出安全模式。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata-server-1 ~]# su - hdfs</span><br><span class="line">Last login: Tue Jun 14 22:06:06 EDT 2016 on pts/0</span><br><span class="line">[hdfs@bigdata-server-1 ~]$ hdfs dfsadmin -safemode leave</span><br><span class="line">OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:</span><br><span class="line">   12493</span><br><span class="line">Try using the -Djava.io.tmpdir= option to select an alternate temp location.</span><br><span class="line"></span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure></p>
<p>2、发现报错，从信息看是空间问题，往下看，尝试之行进入hive客户端,发现如下错误。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@bigdata-server-1 ~]$ hive</span><br><span class="line">OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:</span><br><span class="line">   12570</span><br><span class="line">Try using the -Djava.io.tmpdir= option to select an alternate temp location.</span><br><span class="line"></span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:</span><br><span class="line">   12550</span><br><span class="line">Try using the -Djava.io.tmpdir= option to select an alternate temp location.</span><br><span class="line"></span><br><span class="line">Exception in thread &quot;main&quot; java.io.IOException: Mkdirs failed to create /tmp/hadoop-unjar2180896952508096842</span><br><span class="line">        at org.apache.hadoop.util.RunJar.ensureDirectory(RunJar.java:129)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.run(RunJar.java:198)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</span><br></pre></td></tr></table></figure></p>
<p>3、判断为空间不足，“df -h”验证一下,果然根目录100%了，清理文件，启动down掉服务，退出安全模式，即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@bigdata-server-1 ~]$ df -h</span><br><span class="line">Filesystem                Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/asianux-root   44G   42G     0 100% /</span><br><span class="line">devtmpfs                  3.7G     0  3.7G   0% /dev</span><br><span class="line">tmpfs                     3.8G     0  3.8G   0% /dev/shm</span><br><span class="line"></span><br><span class="line">[hdfs@bigdata-server-1 ~]$ hdfs dfsadmin -safemode leave</span><br><span class="line">[hdfs@bigdata-server-1 ~]$ hdfs dfsadmin -safemode get </span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure></p>
<p>4、总结<br>  造成这样问题的原因，归根结底是因为对集群的监控不到位，比如：服务器监控报警，可以用zabbix／ganglia/nagios来做，未做的原因是因为他是测试环境，不过这个也是不可原谅的错误了。一定要注意监控集群的健康状况，才能避免这样的低级错误。</p>
<p>原创文章，转载请注明： 转载自<a href="http://www.itweet.cn" target="_blank" rel="noopener">Itweet</a>的博客<br><code>本博客的文章集合:</code> <a href="http://www.itweet.cn/blog/archive/" target="_blank" rel="noopener">http://www.itweet.cn/blog/archive/</a></p>

            </div>
          

    
      <footer class="post-footer">
		
		<div class="post-tags">
		  
			<a href="/tags/hadoop/">hadoop</a>
		  
		</div>
		

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2016/03/20/Impala - Hive 性能测试和查询优化/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Impala - Hive 性能测试和查询优化</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2016/03/15/Hadoop列式存储引擎Parquet-ORC和snappy压缩/">
        <span class="next-text nav-default">Hadoop列式存储引擎Parquet-ORC和snappy压缩</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2014 -
    
    2019
    <span class="footer-author">Xu Jiang.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/realXuJiang/hexo-theme-polarbear">Polar Bear</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
