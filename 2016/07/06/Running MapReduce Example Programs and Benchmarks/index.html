<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.8.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="Running MapReduce Example Programs and Benchmarks">




  <meta name="keywords" content="Benchmarks,">





  <link rel="alternate" href="/atom.xml" title="WHOAMI">




  <link rel="shortcut icon" type="image/x-icon" href="https://raw.githubusercontent.com/itweet/itweet.github.io/master/favicon.ico?v=1.1">



<link rel="canonical" href="http://itweet.github.io/2016/07/06/Running MapReduce Example Programs and Benchmarks/">


<meta name="description" content="When using new or updated hardware or software, simple examples and benchmarks help confirm proper operation. Apache Hadoop includes many examples and benchmarks to aid in this task. This chapter from">
<meta name="keywords" content="Benchmarks">
<meta property="og:type" content="article">
<meta property="og:title" content="Running MapReduce Example Programs and Benchmarks">
<meta property="og:url" content="http://itweet.github.io/2016/07/06/Running MapReduce Example Programs and Benchmarks/index.html">
<meta property="og:site_name" content="WHOAMI">
<meta property="og:description" content="When using new or updated hardware or software, simple examples and benchmarks help confirm proper operation. Apache Hadoop includes many examples and benchmarks to aid in this task. This chapter from">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig01_alt.jpg">
<meta property="og:image" content="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig02_alt.jpg">
<meta property="og:image" content="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig03_alt.jpg">
<meta property="og:image" content="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig04_alt.jpg">
<meta property="og:image" content="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig05_alt.jpg">
<meta property="og:image" content="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig06_alt.jpg">
<meta property="og:image" content="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig07_alt.jpg">
<meta property="og:image" content="http://www.informit.com/content/images/chap4_9780134049946/elementLinks/04fig08_alt.jpg">
<meta property="og:image" content="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig09_alt.jpg">
<meta property="og:updated_time" content="2018-07-02T13:09:15.503Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Running MapReduce Example Programs and Benchmarks">
<meta name="twitter:description" content="When using new or updated hardware or software, simple examples and benchmarks help confirm proper operation. Apache Hadoop includes many examples and benchmarks to aid in this task. This chapter from">
<meta name="twitter:image" content="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig01_alt.jpg">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  



    <title> Running MapReduce Example Programs and Benchmarks - WHOAMI </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">WHOAMI</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/archives">
                            
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/atom.xml">
                            
                            
                                RSS
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Running MapReduce Example Programs and Benchmarks
        
      </h1>

      <time class="post-time">
          7月 06 2016
      </time>
    </header>



    
            <div class="post-content">
            <p>When using new or updated hardware or software, simple examples and benchmarks help confirm proper operation. Apache Hadoop includes many examples and benchmarks to aid in this task. This chapter from Hadoop 2 Quick-Start Guide: Learn the Essentials of Big Data Computing in the Apache Hadoop 2 Ecosystem provides instructions on how to run, monitor, and manage some basic MapReduce examples and benchmarks.</p>
<ul>
<li>The steps needed to run the Hadoop MapReduce examples are provided.</li>
<li>An overview of the YARN ResourceManager web GUI is presented.</li>
<li>The steps needed to run two important benchmarks are provided.</li>
<li>The mapred command is introduced as a way to list and kill MapReduce jobs.</li>
</ul>
<p>When using new or updated hardware or software, simple examples and benchmarks help confirm proper operation. Apache Hadoop includes many examples and benchmarks to aid in this task. This chapter provides instructions on how to run, monitor, and manage some basic MapReduce examples and benchmarks.</p>
<h1 id="Running-MapReduce-Examples"><a href="#Running-MapReduce-Examples" class="headerlink" title="Running MapReduce Examples"></a>Running MapReduce Examples</h1><p>All Hadoop releases come with MapReduce example applications. Running the existing MapReduce examples is a simple process—once the example files are located, that is. For example, if you installed Hadoop version 2.6.0 from the Apache sources under /opt, the examples will be in the following directory:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/hadoop-2.6.0/share/hadoop/mapreduce/</span><br></pre></td></tr></table></figure>
<p>In other versions, the examples may be in /usr/lib/hadoop-mapreduce/ or some other location. The exact location of the example jar file can be found using the find command:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ find / -name &quot;hadoop-mapreduce-examples*.jar&quot; -print</span><br></pre></td></tr></table></figure>
<p>For this chapter the following software environment will be used:</p>
<pre><code>- OS: Linux
- Platform: RHEL 6.6
- Hortonworks HDP 2.2 with Hadoop Version: 2.6
</code></pre><p>In this environment, the location of the examples is /usr/hdp/2.2.4.2-2/hadoop-mapreduce. For the purposes of this example, an environment variable called HADOOP_EXAMPLES can be defined as follows:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ export HADOOP_EXAMPLES=/usr/hdp/2.2.4.2-2/hadoop-mapreduce</span><br></pre></td></tr></table></figure>
<p>Once you define the examples path, you can run the Hadoop examples using the commands discussed in the following sections.</p>
<h1 id="Listing-Available-Examples"><a href="#Listing-Available-Examples" class="headerlink" title="Listing Available Examples"></a>Listing Available Examples</h1><p>A list of the available examples can be found by running the following command. In some cases, the version number may be part of the jar file (e.g., in the version 2.6 Apache sources, the file is named hadoop-mapreduce-examples-2.6.0.jar).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yarn jar $HADOOP_EXAMPLES/hadoop-mapreduce-examples.jar</span><br></pre></td></tr></table></figure>
<p>In previous versions of Hadoop, the command hadoop jar . . . was used to run MapReduce programs. Newer versions provide the yarn command, which offers more capabilities. Both commands will work for these examples.</p>
<p>The possible examples are as follows:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">An example program must be given as the first argument.</span><br><span class="line">Valid program names are:</span><br><span class="line">  aggregatewordcount: An Aggregate based map/reduce program that counts</span><br><span class="line">  the words in the input files.</span><br><span class="line">  aggregatewordhist: An Aggregate based map/reduce program that computes</span><br><span class="line">  the histogram of the words in the input files.</span><br><span class="line">  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute</span><br><span class="line">  exact digits of Pi.</span><br><span class="line">  dbcount: An example job that count the pageview counts from a database.</span><br><span class="line">  distbbp: A map/reduce program that uses a BBP-type formula to compute</span><br><span class="line">  exact bits of Pi.</span><br><span class="line">  grep: A map/reduce program that counts the matches of a regex in the</span><br><span class="line">  input.</span><br><span class="line">  join: A job that effects a join over sorted, equally partitioned</span><br><span class="line">  datasets</span><br><span class="line">  multifilewc: A job that counts words from several files.</span><br><span class="line">  pentomino: A map/reduce tile laying program to find solutions to</span><br><span class="line">  pentomino problems.</span><br><span class="line">  pi: A map/reduce program that estimates Pi using a quasi-Monte</span><br><span class="line">  Carlo method.</span><br><span class="line">  randomtextwriter: A map/reduce program that writes 10GB of random</span><br><span class="line">  textual data per node.</span><br><span class="line">  randomwriter: A map/reduce program that writes 10GB of random data</span><br><span class="line">  per node.</span><br><span class="line">  secondarysort: An example defining a secondary sort to the reduce.</span><br><span class="line">  sort: A map/reduce program that sorts the data written by the</span><br><span class="line">  random writer.</span><br><span class="line">  sudoku: A sudoku solver.</span><br><span class="line">  teragen: Generate data for the terasort</span><br><span class="line">  terasort: Run the terasort</span><br><span class="line">  teravalidate: Checking results of terasort</span><br><span class="line">  wordcount: A map/reduce program that counts the words in the</span><br><span class="line">  input files.</span><br><span class="line">  wordmean: A map/reduce program that counts the average length of</span><br><span class="line">  the words in the input files.</span><br><span class="line">  wordmedian: A map/reduce program that counts the median length of</span><br><span class="line">  the words in the input files.</span><br><span class="line">  wordstandarddeviation: A map/reduce program that counts the standard</span><br><span class="line">  deviation of the length of the words in the input files.</span><br></pre></td></tr></table></figure>
<p>To illustrate several features of Hadoop and the YARN ResourceManager service GUI, the pi and terasort examples are presented next. To find help for running the other examples, enter the example name without any arguments. Chapter 6, “MapReduce Programming,” covers one of the other popular examples called wordcount.</p>
<h1 id="Running-the-Pi-Example"><a href="#Running-the-Pi-Example" class="headerlink" title="Running the Pi Example"></a>Running the Pi Example</h1><p>The pi example calculates the digits of π using a quasi-Monte Carlo method. If you have not added users to HDFS (see Chapter 10, “Basic Hadoop Administration Procedures”), run these tests as user hdfs. To run the pi example with 16 maps and 1,000,000 samples per map, enter the following command:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yarn jar $HADOOP_EXAMPLES/hadoop-mapreduce-examples.jar pi 16 1000000</span><br></pre></td></tr></table></figure>
<p>If the program runs correctly, you should see output similar to the following. (Some of the Hadoop INFO messages have been removed for clarity.)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">Number of Maps  = 16</span><br><span class="line">Samples per Map = 1000000</span><br><span class="line">Wrote input for Map #0</span><br><span class="line">Wrote input for Map #1</span><br><span class="line">Wrote input for Map #2</span><br><span class="line">Wrote input for Map #3</span><br><span class="line">Wrote input for Map #4</span><br><span class="line">Wrote input for Map #5</span><br><span class="line">Wrote input for Map #6</span><br><span class="line">Wrote input for Map #7</span><br><span class="line">Wrote input for Map #8</span><br><span class="line">Wrote input for Map #9</span><br><span class="line">Wrote input for Map #10</span><br><span class="line">Wrote input for Map #11</span><br><span class="line">Wrote input for Map #12</span><br><span class="line">Wrote input for Map #13</span><br><span class="line">Wrote input for Map #14</span><br><span class="line">Wrote input for Map #15</span><br><span class="line">Starting Job</span><br><span class="line">...</span><br><span class="line">15/05/13 20:10:30 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">15/05/13 20:10:37 INFO mapreduce.Job:  map 19% reduce 0%</span><br><span class="line">15/05/13 20:10:39 INFO mapreduce.Job:  map 50% reduce 0%</span><br><span class="line">15/05/13 20:10:46 INFO mapreduce.Job:  map 56% reduce 0%</span><br><span class="line">15/05/13 20:10:47 INFO mapreduce.Job:  map 94% reduce 0%</span><br><span class="line">15/05/13 20:10:48 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">15/05/13 20:10:48 INFO mapreduce.Job: Job job_1429912013449_0047 completed</span><br><span class="line">successfully</span><br><span class="line">15/05/13 20:10:48 INFO mapreduce.Job: Counters: 49</span><br><span class="line">        File System Counters</span><br><span class="line">               FILE: Number of bytes read=358</span><br><span class="line">               FILE: Number of bytes written=1949395</span><br><span class="line">               FILE: Number of read operations=0</span><br><span class="line">               FILE: Number of large read operations=0</span><br><span class="line">               FILE: Number of write operations=0</span><br><span class="line">               HDFS: Number of bytes read=4198</span><br><span class="line">               HDFS: Number of bytes written=215</span><br><span class="line">               HDFS: Number of read operations=67</span><br><span class="line">               HDFS: Number of large read operations=0</span><br><span class="line">               HDFS: Number of write operations=3</span><br><span class="line">        Job Counters</span><br><span class="line">               Launched map tasks=16</span><br><span class="line">               Launched reduce tasks=1</span><br><span class="line">               Data-local map tasks=16</span><br><span class="line">               Total time spent by all maps in occupied slots (ms)=158378</span><br><span class="line">               Total time spent by all reduces in occupied slots (ms)=8462</span><br><span class="line">               Total time spent by all map tasks (ms)=158378</span><br><span class="line">               Total time spent by all reduce tasks (ms)=8462</span><br><span class="line">               Total vcore-seconds taken by all map tasks=158378</span><br><span class="line">               Total vcore-seconds taken by all reduce tasks=8462</span><br><span class="line">               Total megabyte-seconds taken by all map tasks=243268608</span><br><span class="line">               Total megabyte-seconds taken by all reduce tasks=12997632</span><br><span class="line">        Map-Reduce Framework</span><br><span class="line">               Map input records=16</span><br><span class="line">               Map output records=32</span><br><span class="line">               Map output bytes=288</span><br><span class="line">               Map output materialized bytes=448</span><br><span class="line">               Input split bytes=2310</span><br><span class="line">               Combine input records=0</span><br><span class="line">               Combine output records=0</span><br><span class="line">               Reduce input groups=2</span><br><span class="line">               Reduce shuffle bytes=448</span><br><span class="line">               Reduce input records=32</span><br><span class="line">               Reduce output records=0</span><br><span class="line">               Spilled Records=64</span><br><span class="line">               Shuffled Maps=16</span><br><span class="line">               Failed Shuffles=0</span><br><span class="line">               Merged Map outputs=16</span><br><span class="line">               GC time elapsed (ms)=1842</span><br><span class="line">               CPU time spent (ms)=11420</span><br><span class="line">               Physical memory (bytes) snapshot=13405769728</span><br><span class="line">               Virtual memory (bytes) snapshot=33911930880</span><br><span class="line">               Total committed heap usage (bytes)=17026777088</span><br><span class="line">        Shuffle Errors</span><br><span class="line">               BAD_ID=0</span><br><span class="line">               CONNECTION=0</span><br><span class="line">               IO_ERROR=0</span><br><span class="line">               WRONG_LENGTH=0</span><br><span class="line">               WRONG_MAP=0</span><br><span class="line">               WRONG_REDUCE=0</span><br><span class="line">        File Input Format Counters</span><br><span class="line">               Bytes Read=1888</span><br><span class="line">        File Output Format Counters</span><br><span class="line">               Bytes Written=97</span><br><span class="line">Job Finished in 23.718 seconds</span><br><span class="line">Estimated value of Pi is 3.14159125000000000000</span><br></pre></td></tr></table></figure>
<p>Notice that the MapReduce progress is shown in the same way as Hadoop version 1, but the application statistics are different. Most of the statistics are self-explanatory. The one important item to note is that the YARN MapReduce framework is used to run the program. (See Chapter 1, “Background and Concepts,” and Chapter 8, “Hadoop YARN Applications,” for more information about YARN frameworks.)</p>
<h1 id="Using-the-Web-GUI-to-Monitor-Examples"><a href="#Using-the-Web-GUI-to-Monitor-Examples" class="headerlink" title="Using the Web GUI to Monitor Examples"></a>Using the Web GUI to Monitor Examples</h1><p>This section provides an illustration of using the YARN ResourceManager web GUI to monitor and find information about YARN jobs. The Hadoop version 2 YARN ResourceManager web GUI differs significantly from the MapReduce web GUI found in Hadoop version 1. Figure 4.1 shows the main YARN web interface. The cluster metrics are displayed in the top row, while the running applications are displayed in the main table. A menu on the left provides navigation to the nodes table, various job categories (e.g., New, Accepted, Running, Finished, Failed), and the Capacity Scheduler (covered in Chapter 10, “Basic Hadoop Administration Procedures”). This interface can be opened directly from the Ambari YARN service Quick Links menu or by directly entering <a href="http://hostname:8088" target="_blank" rel="noopener">http://hostname:8088</a> into a local web browser. For this example, the pi application is used. Note that the application can run quickly and may finish before you have fully explored the GUI. A longer-running application, such as terasort, may be helpful when exploring all the various links in the GUI.</p>
<p><img src="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig01_alt.jpg" alt="Figure 4.1 Hadoop RUNNING Applications web GUI for the pi example"></p>
<p>Metrics table, you will see some new information. First, you will notice that the “Map/Reduce Task Capacity” has been replaced by the number of running containers. If YARN is running a MapReduce job, these containers can be used for both map and reduce tasks. Unlike in Hadoop version 1, the number of mappers and reducers is not fixed. There are also memory metrics and links to node status. If you click on the Nodes link (left menu under About), you can get a summary of the node activity and state. For example, Figure 4.2 is a snapshot of the node activity while the pi application is running. Notice the number of containers, which are used by the MapReduce framework as either mappers or reducers.</p>
<p><img src="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig02_alt.jpg" alt="Figure 4.2 Hadoop YARN ResourceManager nodes status window"></p>
<p>Going back to the main Applications/Running window (Figure 4.1), if you click on the application_14299… link, the Application status window in Figure 4.3 will appear. This window provides an application overview and metrics, including the cluster node on which the ApplicationMaster container is running.</p>
<p><img src="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig03_alt.jpg" alt="Figure 4.3 Hadoop YARN application status for the pi example"></p>
<p>Clicking the ApplicationMaster link next to “Tracking URL:” in Figure 4.3 leads to the window shown in Figure 4.4. Note that the link to the application’s ApplicationMaster is also found in the last column on the main Running Applications screen shown in Figure 4.1.</p>
<p><img src="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig04_alt.jpg" alt="Figure 4.4 Hadoop YARN ApplicationMaster for MapReduce application"></p>
<p>In the MapReduce Application window, you can see the details of the MapReduce application and the overall progress of mappers and reducers. Instead of containers, the MapReduce application now refers to maps and reducers. Clicking job_14299… brings up the window shown in Figure 4.5. This window displays more detail about the number of pending, running, completed, and failed mappers and reducers, including the elapsed time since the job started.</p>
<p><img src="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig05_alt.jpg" alt="Figure 4.5 Hadoop YARN MapReduce job progress"></p>
<p>The status of the job in Figure 4.5 will be updated as the job progresses (the window needs to be refreshed manually). The ApplicationMaster collects and reports the progress of each mapper and reducer task. When the job is finished, the window is updated to that shown in Figure 4.6. It reports the overall run time and provides a breakdown of the timing of the key phases of the MapReduce job (map, shuffle, merge, reduce).</p>
<p><img src="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig06_alt.jpg" alt="Figure 4.6 Hadoop YARN completed MapReduce job summary"></p>
<p>If you click the node used to run the ApplicationMaster (n0:8042 in Figure 4.6), the window in Figure 4.7 opens and provides a summary from the NodeManager on node n0. Again, the NodeManager tracks only containers; the actual tasks running in the containers are determined by the ApplicationMaster.</p>
<p><img src="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig07_alt.jpg" alt="Figure 4.7 Hadoop YARN NodeManager for n0 job summary"></p>
<p>Going back to the job summary page (Figure 4.6), you can also examine the logs for the ApplicationMaster by clicking the “logs” link. To find information about the mappers and reducers, click the numbers under the Failed, Killed, and Successful columns. In this example, there were 16 successful mappers and one successful reducer. All the numbers in these columns lead to more information about individual map or reduce process. For instance, clicking the “16” under “-Successful” in Figure 4.6 displays the table of map tasks in Figure 4.8. The metrics for the Application Master container are displayed in table form. There is also a link to the log file for each process (in this case, a map process). Viewing the logs requires that the yarn.log.aggregation-enable variable in the yarn-site.xml file be set. For more on changing Hadoop settings, see Chapter 9, “Managing Hadoop with Apache Ambari.”</p>
<p><img src="http://www.informit.com/content/images/chap4_9780134049946/elementLinks/04fig08_alt.jpg" alt="Figure 4.8 Hadoop YARN MapReduce logs available for browsing"></p>
<p>If you return to the main cluster window (Figure 4.1), choose Applications/Finished, and then select our application, you will see the summary page shown in Figure 4.9.</p>
<p><img src="http://ptgmedia.pearsoncmg.com/images/chap4_9780134049946/elementLinks/04fig09_alt.jpg" alt="Figure 4.9 Hadoop YARN application summary page"></p>
<p>There are a few things to notice in the previous windows. First, because YARN manages applications, all information reported by the ResourceManager concerns the resources provided and the application type (in this case, MAPREDUCE). In Figure 4.1 and Figure 4.4, the YARN ResourceManager refers to the pi example by its application-id (application_1429912013449_0044). YARN has no data about the actual application other than the fact that it is a MapReduce job. Data from the actual MapReduce job are provided by the MapReduce framework and referenced by a job-id (job_1429912013449_0044) in Figure 4.6. Thus, two clearly different data streams are combined in the web GUI: YARN applications and MapReduce framework jobs. If the framework does not provide job information, then certain parts of the web GUI will not have anything to display.</p>
<p>Another interesting aspect of the previous windows is the dynamic nature of the mapper and reducer tasks. These tasks are executed as YARN containers, and their number will change as the application runs. Users may request specific numbers of mappers and reducers, but the ApplicationMaster uses them in a dynamic fashion. As mappers complete, the ApplicationMaster will return the containers to the ResourceManager and request a smaller number of reducer containers. This feature provides for much better cluster utilization because mappers and reducers are dynamic—rather than fixed—resources.</p>
<h1 id="Running-Basic-Hadoop-Benchmarks"><a href="#Running-Basic-Hadoop-Benchmarks" class="headerlink" title="Running Basic Hadoop Benchmarks"></a>Running Basic Hadoop Benchmarks</h1><p>Many Hadoop benchmarks can provide insight into cluster performance. The best benchmarks are always those that reflect real application performance. The two benchmarks discussed in this section, terasort and TestDFSIO, provide a good sense of how well your Hadoop installation is operating and can be compared with public data published for other Hadoop systems. The results, however, should not be taken as a single indicator for system-wide performance on all applications.</p>
<p>The following benchmarks are designed for full Hadoop cluster installations. These tests assume a multi-disk HDFS environment. Running these benchmarks in the Hortonworks Sandbox or in the pseudo-distributed single-node install from Chapter 2 is not recommended because all input and output (I/O) are done using a single system disk drive.</p>
<h1 id="Running-the-Terasort-Test"><a href="#Running-the-Terasort-Test" class="headerlink" title="Running the Terasort Test"></a>Running the Terasort Test</h1><p>The terasort benchmark sorts a specified amount of randomly generated data. This benchmark provides combined testing of the HDFS and MapReduce layers of a Hadoop cluster. A full terasort benchmark run consists of the following three steps:</p>
<ol>
<li><p>Generating the input data via teragen program.</p>
</li>
<li><p>Running the actual terasort benchmark on the input data.</p>
</li>
<li><p>Validating the sorted output data via the teravalidate program.</p>
</li>
</ol>
<p>In general, each row is 100 bytes long; thus the total amount of data written is 100 times the number of rows specified as part of the benchmark (i.e., to write 100GB of data, use 1 billion rows). The input and output directories need to be specified in HDFS. The following sequence of commands will run the benchmark for 50GB of data as user hdfs. Make sure the /user/hdfs directory exists in HDFS before running the benchmarks.</p>
<ol>
<li>Run teragen to generate rows of random data to sort.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yarn jar $HADOOP_EXAMPLES/hadoop-mapreduce-examples.jar teragen 500000000</span><br><span class="line"> /user/hdfs/TeraGen-50GB</span><br></pre></td></tr></table></figure>
<p>Example results are as follows (date and time prefix removed).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fs.TestDFSIO: ----- TestDFSIO ----- : write</span><br><span class="line">fs.TestDFSIO:            Date &amp; time: Thu May 14 10:39:33 EDT 2015</span><br><span class="line">fs.TestDFSIO:        Number of files: 16</span><br><span class="line">fs.TestDFSIO: Total MBytes processed: 16000.0</span><br><span class="line">fs.TestDFSIO:      Throughput mb/sec: 14.890106361891005</span><br><span class="line">fs.TestDFSIO: Average IO rate mb/sec: 15.690713882446289</span><br><span class="line">fs.TestDFSIO:  IO rate std deviation: 4.0227035201665595</span><br><span class="line">fs.TestDFSIO:     Test exec time sec: 105.631</span><br></pre></td></tr></table></figure>
<p>2.Run terasort to sort the database.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yarn jar  $HADOOP_EXAMPLES/hadoop-mapreduce-client-jobclient-tests.jar</span><br><span class="line">TestDFSIO -read  -nrFiles 16 -fileSize 1000</span><br></pre></td></tr></table></figure>
<p>Example results are as follows (date and time prefix removed). The large standard deviation is due to the placement of tasks in the cluster on a small four-node cluster.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fs.TestDFSIO: ----- TestDFSIO ----- : read</span><br><span class="line">fs.TestDFSIO:            Date &amp; time: Thu May 14 10:44:09 EDT 2015</span><br><span class="line">fs.TestDFSIO:        Number of files: 16</span><br><span class="line">fs.TestDFSIO: Total MBytes processed: 16000.0</span><br><span class="line">fs.TestDFSIO:      Throughput mb/sec: 32.38643494172466</span><br><span class="line">fs.TestDFSIO: Average IO rate mb/sec: 58.72880554199219</span><br><span class="line">fs.TestDFSIO:  IO rate std deviation: 64.60017624360337</span><br><span class="line">fs.TestDFSIO:     Test exec time sec: 62.798</span><br></pre></td></tr></table></figure>
<p>3.Clean up the TestDFSIO data.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yarn jar  $HADOOP_EXAMPLES/hadoop-mapreduce-client-jobclient-tests.jar</span><br><span class="line"> TestDFSIO -clean</span><br></pre></td></tr></table></figure>
<p>Running the TestDFSIO and terasort benchmarks help you gain confidence in a Hadoop installation and detect any potential problems. It is also instructive to view the Ambari dashboard and the YARN web GUI (as described previously) as the tests run.</p>
<h1 id="Managing-Hadoop-MapReduce-Jobs"><a href="#Managing-Hadoop-MapReduce-Jobs" class="headerlink" title="Managing Hadoop MapReduce Jobs"></a>Managing Hadoop MapReduce Jobs</h1><p>Hadoop MapReduce jobs can be managed using the mapred job command. The most important options for this command in terms of the examples and benchmarks are -list, -kill, and -status. In particular, if you need to kill one of the examples or benchmarks, you can use the mapred job -list command to find the job-id and then use mapred job -kill <job-id> to kill the job across the cluster. MapReduce jobs can also be controlled at the application level with the yarn application command (see Chapter 10, “Basic Hadoop Administration Procedures”). The possible options for mapred job are as follows:</job-id></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">$ mapred job</span><br><span class="line">Usage: CLI &lt;command&gt; &lt;args&gt;</span><br><span class="line">       [-submit &lt;job-file&gt;]</span><br><span class="line">       [-status &lt;job-id&gt;]</span><br><span class="line">       [-counter &lt;job-id&gt; &lt;group-name&gt; &lt;counter-name&gt;]</span><br><span class="line">       [-kill &lt;job-id&gt;]</span><br><span class="line">       [-set-priority &lt;job-id&gt; &lt;priority&gt;]. Valid values for priorities</span><br><span class="line">        are: VERY_HIGH HIGH NORMAL LOW VERY_LOW</span><br><span class="line">       [-events &lt;job-id&gt; &lt;from-event-#&gt; &lt;#-of-events&gt;]</span><br><span class="line">       [-history &lt;jobHistoryFile&gt;]</span><br><span class="line">       [-list [all]]</span><br><span class="line">       [-list-active-trackers]</span><br><span class="line">       [-list-blacklisted-trackers]</span><br><span class="line">       [-list-attempt-ids &lt;job-id&gt; &lt;task-type&gt; &lt;task-state&gt;]. Valid values</span><br><span class="line">        for &lt;task-type&gt; are REDUCE MAP. Valid values for &lt;task-state&gt; are</span><br><span class="line">        running, completed</span><br><span class="line">       [-kill-task &lt;task-attempt-id&gt;]</span><br><span class="line">       [-fail-task &lt;task-attempt-id&gt;]</span><br><span class="line">       [-logs &lt;job-id&gt; &lt;task-attempt-id&gt;]</span><br><span class="line"></span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value for given property</span><br><span class="line">-fs &lt;local|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to</span><br><span class="line">  be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar</span><br><span class="line">  files to include in the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated</span><br><span class="line">  archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general command line syntax is</span><br><span class="line">bin/hadoop command [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>
<h1 id="Summary-and-Additional-Resources"><a href="#Summary-and-Additional-Resources" class="headerlink" title="Summary and Additional Resources"></a>Summary and Additional Resources</h1><p>No matter what the size of the Hadoop cluster, confirming and measuring the MapReduce performance of that cluster is an important first step. Hadoop includes some simple applications and benchmarks that can be used for this purpose. The YARN ResourceManager web GUI is a good way to monitor the progress of any application. Jobs that run under the MapReduce framework report a large number of run-time metrics directly (including logs) back to the GUI; these metrics are then presented to the user in a clear and coherent fashion. Should issues arise when running the examples and benchmarks, the mapred job command can be used to kill a MapReduce job.</p>
<p>Additional information and background on each of the examples and benchmarks can be found from the following resources:</p>
<ul>
<li><p>Pi Benchmark</p>
<ul>
<li><a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/examples/pi/package-summary.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/current/api/org/apache/hadoop/examples/pi/package-summary.html</a></li>
</ul>
</li>
<li><p>Terasort Benchmark</p>
<ul>
<li><a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/examples/terasort/package-summary.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/current/api/org/apache/hadoop/examples/terasort/package-summary.html</a></li>
</ul>
</li>
<li><p>Benchmarking and Stress Testing an Hadoop Cluster</p>
<ul>
<li><a href="http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench" target="_blank" rel="noopener">http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench</a> (uses Hadoop V1, will work with V2)</li>
</ul>
</li>
<li><p>Configuring Heterogeneous Storage in HDFS</p>
<ul>
<li><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/admin_heterogeneous_storage_oview.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/latest/topics/admin_heterogeneous_storage_oview.html</a></li>
</ul>
</li>
<li><p>Archival Storage, SSD &amp; Memory</p>
<ul>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html#Storage_Policies:_Hot_Warm_Cold_All_SSD_One_SSD_and_Lazy_Persist" target="_blank" rel="noopener">https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html#Storage_Policies:_Hot_Warm_Cold_All_SSD_One_SSD_and_Lazy_Persist</a></li>
</ul>
</li>
<li><p>The Truth About MapReduce Performance on SSDs</p>
<ul>
<li><a href="http://blog.cloudera.com/blog/2014/03/the-truth-about-mapreduce-performance-on-ssds/" target="_blank" rel="noopener">http://blog.cloudera.com/blog/2014/03/the-truth-about-mapreduce-performance-on-ssds/</a></li>
</ul>
</li>
<li><p>Memory Storage Support in HDFS</p>
<ul>
<li><a href="http://aajisaka.github.io/hadoop-project/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html" target="_blank" rel="noopener">http://aajisaka.github.io/hadoop-project/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html</a></li>
</ul>
</li>
</ul>
<p>原创文章，转载请注明： 转载自<a href="http://www.itweet.cn" target="_blank" rel="noopener">Itweet</a>的博客<br><code>本博客的文章集合:</code> <a href="http://www.itweet.cn/blog/archive/" target="_blank" rel="noopener">http://www.itweet.cn/blog/archive/</a></p>

            </div>
          

    
      <footer class="post-footer">
		
		<div class="post-tags">
		  
			<a href="/tags/Benchmarks/">Benchmarks</a>
		  
		</div>
		

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2016/07/07/Configuring Heterogeneous Storage in HDFS/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Configuring Heterogeneous Storage in HDFS</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2016/07/03/Hive on Hbase 整合测试/">
        <span class="next-text nav-default">Hive on Hbase 整合测试</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2014 -
    
    2019
    <span class="footer-author">Xu Jiang.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/frostfan/hexo-theme-polarbear">Polar Bear</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
