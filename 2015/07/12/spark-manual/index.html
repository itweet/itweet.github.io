<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.8.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="Spark 的安装与使用">




  <meta name="keywords" content="spark,">





  <link rel="alternate" href="/atom.xml" title="WHOAMI">




  <link rel="shortcut icon" type="image/x-icon" href="https://raw.githubusercontent.com/itweet/itweet.github.io/master/favicon.ico?v=1.1">



<link rel="canonical" href="http://itweet.github.io/2015/07/12/spark-manual/">


<meta name="description" content="Spark 的安装与使用">
<meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="spark manual">
<meta property="og:url" content="http://itweet.github.io/2015/07/12/spark-manual/index.html">
<meta property="og:site_name" content="WHOAMI">
<meta property="og:description" content="Spark 的安装与使用">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://www.itweet.cn/screenshots/cluster-overview.png">
<meta property="og:image" content="https://www.itweet.cn/screenshots/streaming-net.png">
<meta property="og:image" content="https://www.itweet.cn/screenshots/streaming-hdfs.png">
<meta property="og:image" content="https://www.itweet.cn/screenshots/streaming-hdfs-1.png">
<meta property="og:image" content="https://www.itweet.cn/screenshots/kafka_streaming.png">
<meta property="og:image" content="https://www.itweet.cn/screenshots/kafka_streaming_1.png">
<meta property="og:updated_time" content="2019-03-05T15:09:47.486Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="spark manual">
<meta name="twitter:description" content="Spark 的安装与使用">
<meta name="twitter:image" content="https://www.itweet.cn/screenshots/cluster-overview.png">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  



    <title> spark manual - WHOAMI </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">WHOAMI</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/archives">
                            
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/atom.xml">
                            
                            
                                RSS
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          spark manual
        
      </h1>

      <time class="post-time">
          7月 12 2015
      </time>
    </header>



    
            <div class="post-content">
            <h1 id="集群概述"><a href="#集群概述" class="headerlink" title="集群概述"></a>集群概述</h1><p>本文章涉及spark安装部署，spark-sql,spark-shell,streaming等等的应用demo…<a href="https://www.itweet.cn/categories/spark/" target="_blank" rel="noopener">saprk文章</a></p>
<h2 id="部署过程详解"><a href="#部署过程详解" class="headerlink" title="部署过程详解"></a>部署过程详解</h2><p>Spark布置环境中组件构成如下图所示。<br><img src="https://www.itweet.cn/screenshots/cluster-overview.png" alt></p>
<ul>
<li><code>Driver Program</code> 简要来说在spark-shell中输入的wordcount语句对应于上图的Driver Program。</li>
<li><code>Cluster Manager</code> 就是对应于上面提到的master，主要起到deploy management的作用</li>
<li><code>Worker Node</code> 与Master相比，这是slave node。上面运行各个executor，executor可以对应于线程。executor处理两种基本的业务逻辑，一种就是driver programme，另一种就是job在提交之后拆分成各个stage，每个stage可以运行一到多个task</li>
</ul>
<p><code>Notes</code>: 在集群（cluster）方式下，Cluster Manager运行在一个<code>jvm</code>进程之中，而worker运行在另一个<code>jvm</code>进程中。在local cluster中，这些<code>jvm</code>进程都在同一台机器中，如果是真正的Standalone或Mesos及Yarn集群，worker与master或分布于不同的主机之上。</p>
<h2 id="JOB的生成和运行"><a href="#JOB的生成和运行" class="headerlink" title="JOB的生成和运行"></a>JOB的生成和运行</h2><p>job生成的简单流程如下</p>
<ul>
<li>1、首先应用程序创建SparkContext的实例，如实例为sc</li>
<li>2、利用SparkContext的实例来创建生成RDD</li>
<li>3、经过一连串的transformation操作，原始的RDD转换成为其它类型的RDD</li>
<li>4、当action作用于转换之后RDD时，会调用SparkContext的runJob方法</li>
<li>5、sc.runJob的调用是后面一连串反应的起点，关键性的跃变就发生在此处</li>
</ul>
<p>调用路径大致如下</p>
<ul>
<li>1、sc.runJob-&gt;dagScheduler.runJob-&gt;submitJob</li>
<li>2、DAGScheduler::submitJob会创建JobSummitted的event发送给内嵌类eventProcessActor</li>
<li>3、eventProcessActor在接收到JobSubmmitted之后调用processEvent处理函数</li>
<li>4、job到stage的转换，生成finalStage并提交运行，关键是调用submitStage</li>
<li>5、在submitStage中会计算stage之间的依赖关系，依赖关系分为宽依赖和窄依赖两种</li>
<li>6、如果计算中发现当前的stage没有任何依赖或者所有的依赖都已经准备完毕，则提交task</li>
<li>7、提交task是调用函数submitMissingTasks来完成</li>
<li>8、task真正运行在哪个worker上面是由TaskScheduler来管理，也就是上面的submitMissingTasks会调用TaskScheduler::submitTasks</li>
<li>9、TaskSchedulerImpl中会根据Spark的当前运行模式来创建相应的backend，如果是在单机运行则创建LocalBackend</li>
<li>10、LocalBackend收到TaskSchedulerImpl传递进来的ReceiveOffers事件</li>
<li><p>11、receiveOffers-&gt;executor.launchTask-&gt;TaskRunner.run</p>
<p>代码片段executor.lauchTask</p>
<p>def launchTask(context: ExecutorBackend, taskId: Long, serializedTask: ByteBuffer) {<br>val tr = new TaskRunner(context, taskId, serializedTask)<br>runningTasks.put(taskId, tr)<br>threadPool.execute(tr)<br>}</p>
<p>最终的逻辑处理切切实实是发生在TaskRunner这么一个executor之内。<br>运算结果是包装成为MapStatus然后通过一系列的内部消息传递，反馈到DAGScheduler</p>
</li>
</ul>
<h1 id="编译spark"><a href="#编译spark" class="headerlink" title="编译spark"></a>编译spark</h1><h2 id="获取源码"><a href="#获取源码" class="headerlink" title="获取源码"></a>获取源码</h2><pre><code>`wget http://mirror.bit.edu.cn/apache/spark/spark-1.4.0/spark-1.4.0.tgz`
`tar -zxf spark-1.4.0.tgz`
`cd spark-1.4.0`
</code></pre><h2 id="编译支持hive，yarn，tachyon"><a href="#编译支持hive，yarn，tachyon" class="headerlink" title="编译支持hive，yarn，tachyon"></a>编译支持hive，yarn，tachyon</h2><h3 id="1、-ERROR-PermGen-space-gt-Help-1"><a href="#1、-ERROR-PermGen-space-gt-Help-1" class="headerlink" title="1、[ERROR] PermGen space -&gt; [Help 1]"></a>1、<code>[ERROR] PermGen space -&gt; [Help 1]</code></h3><pre><code>export MAVEN_OPTS=&quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&quot;
</code></pre><h3 id="2、编译"><a href="#2、编译" class="headerlink" title="2、编译"></a>2、编译</h3><pre><code>mvn -Pyarn -Phive -Phive-thriftserver -Dhadoop.version=2.4.0 -DskipTests clean package
</code></pre><h3 id="3、生成部署包"><a href="#3、生成部署包" class="headerlink" title="3、生成部署包"></a>3、生成部署包</h3><pre><code>./make-distribution.sh --with-tachyon --tgz -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.4.0
</code></pre><h3 id="4、编译spark支持ganglia插件"><a href="#4、编译spark支持ganglia插件" class="headerlink" title="4、编译spark支持ganglia插件"></a>4、编译spark支持ganglia插件</h3><p><a href="https://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/monitoring.html</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export MAVEN_OPTS=&quot;-Xmx3g -XX:MaxPermSize=1512M -XX:ReservedCodeCacheSize=1512m&quot;</span><br><span class="line">mvn clean package -DskipTests -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -Pspark-ganglia-lgpl</span><br></pre></td></tr></table></figure></p>
<p><code>[ERROR] mvn &lt;goals&gt; -rf :spark-mllib_2.10</code></p>
<blockquote>
<p>参考：<a href="http://stackoverflow.com/questions/29354461/spark-1-3-0-build-failure" target="_blank" rel="noopener">http://stackoverflow.com/questions/29354461/spark-1-3-0-build-failure</a><br>     <a href="https://issues.apache.org/jira/browse/SPARK-6532" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-6532</a></p>
</blockquote>
<blockquote>
<p>调试：mvn -DskipTests -X clean package，是个bug已解决！<br>pom.xml文件中修改</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalastyle&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalastyle-maven-plugin&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;0.4.0&lt;/version&gt;</span><br><span class="line">    &lt;configuration&gt;</span><br><span class="line">      &lt;verbose&gt;false&lt;/verbose&gt;</span><br><span class="line">      &lt;failOnViolation&gt;true&lt;/failOnViolation&gt;</span><br><span class="line">      &lt;includeTestSourceDirectory&gt;false&lt;/includeTestSourceDirectory&gt;</span><br><span class="line">      &lt;failOnWarning&gt;false&lt;/failOnWarning&gt;</span><br><span class="line">      &lt;sourceDirectory&gt;$&#123;basedir&#125;/src/main/scala&lt;/sourceDirectory&gt;</span><br><span class="line">      &lt;testSourceDirectory&gt;$&#123;basedir&#125;/src/test/scala&lt;/testSourceDirectory&gt;</span><br><span class="line">      &lt;configLocation&gt;scalastyle-config.xml&lt;/configLocation&gt;</span><br><span class="line">      &lt;outputFile&gt;scalastyle-output.xml&lt;/outputFile&gt;</span><br><span class="line">      &lt;outputEncoding&gt;UTF-8&lt;/outputEncoding&gt;</span><br><span class="line">      &lt;inputEncoding&gt;UTF-8&lt;/inputEncoding&gt; &lt;!--add this line--&gt;</span><br><span class="line">    &lt;/configuration&gt;</span><br><span class="line">    &lt;executions&gt;</span><br><span class="line">      &lt;execution&gt;</span><br><span class="line">        &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">        &lt;goals&gt;</span><br><span class="line">          &lt;goal&gt;check&lt;/goal&gt;</span><br><span class="line">        &lt;/goals&gt;</span><br><span class="line">      &lt;/execution&gt;</span><br><span class="line">    &lt;/executions&gt;</span><br><span class="line">  &lt;/plugin&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>由于windows编译生成部署包那个脚本不能用，所以自己制作部署包！<br>参考官网编译后的目录来构建！</p>
</blockquote>
<h1 id="Spark-Standalone-Mode"><a href="#Spark-Standalone-Mode" class="headerlink" title="Spark Standalone Mode"></a>Spark Standalone Mode</h1><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="1、配置spark-defaults-conf"><a href="#1、配置spark-defaults-conf" class="headerlink" title="1、配置spark-defaults.conf"></a>1、配置spark-defaults.conf</h3><pre><code>$ cat spark-defaults.conf 
# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.
# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;
spark.master            spark://server1:7077
spark.eventLog.enabled  true
spark.eventLog.dir      hdfs://mycluster/tmp/spark
spark.serializer        org.apache.spark.serializer.KryoSerializer
spark.executor.memory   8g
spark.local.dir  /data/spark/localdata
spark.driver.memory   10g
spark.history.fs.logDirectory /data/spark/local
spark.scheduler.mode  FAIR  
</code></pre><h3 id="2、配置saprk-evn-sh"><a href="#2、配置saprk-evn-sh" class="headerlink" title="2、配置saprk-evn.sh"></a>2、配置saprk-evn.sh</h3><pre><code>$ cat spark-env.sh
export JAVA_HOME=/usr/java/jdk1.6.0_31/
export SCALA_HOME=/usr/apps/scala-2.10.4
export HIVE_HOME=/etc/hive
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HADOOP_HOME=/etc/hadoop
export HADOOP_CONF_DIR=/etc/hadoop/etc

export SPARK_MASTER_IP=server1
export SPARK_MASTER_PORT=7077
export SPARK_MASTER_WEBUI_PORT=8090
export SPARK_LOCAL_DIRS=/data/spark/localdata

# - SPARK_MASTER_OPTS
export SPARK_WORKER_CORES=20
export SPARK_WORKER_MEMORY=120g
export SPARK_WORKER_PORT=8991
export SPARK_WORKER_WEBUI_PORT=8992
export SPARK_WORKER_INSTANCES=1
export SPARK_WORKER_DIR=/data/spark/workdir
export SPARK_WORKER_OPTS=&quot;-Dspark.worker.cleanup.enabled=false  -Dspark.worker.cleanup.interval=1800  -Dspark.worker.cleanup.appDataTtl=604800&quot;
export SPARK_DAEMON_MEMORY=2048m
export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER   -Dspark.deploy.zookeeper.url=server1:2181,server2:2181,server3:2181 -Dspark.deploy.zookeeper.dir=/spark&quot;
export SPARK_WORKER_OPTS=&quot;-Dspark.worker.cleanup.enabled=true  -Dspark.worker.cleanup.interval=1800  -Dspark.worker.cleanup.appDataTtl=604800&quot;

export SPARK_JAR=/usr/lib/spark/assembly/target/scala-2.10/spark-assembly-1.4.0-SNAPSHOT-hadoop2.4.0.jar
export SPARK_CLASSPATH=/usr/lib/hive/lib/mysql-connector-java.jar
export HADOOP_CONF_DIR=/usr/lib/hadoop/etc/hadoop
export SPARK_DAEMON_JAVA_OPTS=&quot; -XX:NewRatio=1  -verbose:gc -XX:+PrintGCDetails -Xloggc:/tmp/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/saprk_heapdum.hprof -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=60 -XX:+PrintGCTimeStamps&quot;
export SPARK_LOG_DIR=/var/log/spark
</code></pre><h3 id="3、配置log4j"><a href="#3、配置log4j" class="headerlink" title="3、配置log4j"></a>3、配置log4j</h3><p>根据各自情况配置log日志级别方便调试！</p>
<h3 id="4、配置fairscheduler-xml"><a href="#4、配置fairscheduler-xml" class="headerlink" title="4、配置fairscheduler.xml"></a>4、配置fairscheduler.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ cat fairscheduler.xml</span><br><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;allocations&gt;</span><br><span class="line">  &lt;pool name=&quot;test1&quot;&gt;</span><br><span class="line">    &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt;</span><br><span class="line">    &lt;weight&gt;2&lt;/weight&gt;</span><br><span class="line">    &lt;minShare&gt;20&lt;/minShare&gt;</span><br><span class="line">  &lt;/pool&gt;</span><br><span class="line">  &lt;pool name=&quot;test2&quot;&gt;</span><br><span class="line">     &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt;</span><br><span class="line">     &lt;weight&gt;2&lt;/weight&gt;</span><br><span class="line">     &lt;minShare&gt;10&lt;/minShare&gt;</span><br><span class="line">  &lt;/pool&gt;</span><br><span class="line">  &lt;pool name=&quot;default&quot;&gt;</span><br><span class="line">    &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt;</span><br><span class="line">    &lt;weight&gt;1&lt;/weight&gt;</span><br><span class="line">    &lt;minShare&gt;10&lt;/minShare&gt;</span><br><span class="line">  &lt;/pool&gt;</span><br></pre></td></tr></table></figure>
<h4 id="4-1-spark-sql使用队列"><a href="#4-1-spark-sql使用队列" class="headerlink" title="4.1 spark-sql使用队列"></a>4.1 spark-sql使用队列</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SET spark.sql.thriftserver.scheduler.pool=test1;</span><br><span class="line">select * from default.abc;</span><br><span class="line">........</span><br></pre></td></tr></table></figure>
<h4 id="4-2-应用程序中使用队列"><a href="#4-2-应用程序中使用队列" class="headerlink" title="4.2 应用程序中使用队列"></a>4.2 应用程序中使用队列</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// Assuming sc is your SparkContext variable</span><br><span class="line">sc.setLocalProperty(&quot;spark.scheduler.pool&quot;, &quot;pool1&quot;)</span><br></pre></td></tr></table></figure>
<h1 id="spark-on-yarn"><a href="#spark-on-yarn" class="headerlink" title="spark on yarn"></a>spark on yarn</h1><p>配置基本不变,但是无需启动spark集群，而直接依赖yarn集群！</p>
<p>yarn-spark-sql:<br>    <code>spark-sql --master yarn-client --executor-memory 2g</code></p>
<p>yarn-spark-shell:<br>    <code>spark-shell --master yarn-client --executor-memory 2g</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ spark-sql --master yarn-client --executor-memory 2g --verbose </span><br><span class="line">Run with --help for usage help or --verbose for debug output</span><br></pre></td></tr></table></figure>
<h1 id="使用spark"><a href="#使用spark" class="headerlink" title="使用spark"></a>使用spark</h1><blockquote>
<p>启动saprk集群：<br>  <code>$SPARK_HOME/sbin/start-all.sh</code>  </p>
</blockquote>
<blockquote>
<p>停止集群<br>  <code>$SPARK_HOME/sbin/stop-all.sh</code></p>
</blockquote>
<h2 id="1、saprk-sql"><a href="#1、saprk-sql" class="headerlink" title="1、saprk-sql"></a>1、saprk-sql</h2><h3 id="spark-cli-start"><a href="#spark-cli-start" class="headerlink" title="spark cli start"></a>spark cli start</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ spark-sql --master spark://server1:7077 --executor-memory 16g --total-executor-cores 200</span><br></pre></td></tr></table></figure>
<h3 id="start-thriftserver-sh-jdbc"><a href="#start-thriftserver-sh-jdbc" class="headerlink" title="start-thriftserver.sh jdbc"></a>start-thriftserver.sh jdbc</h3><p>This script accepts all bin/spark-submit command line options, plus a –hiveconf option to specify Hive properties. You may run ./sbin/start-thriftserver.sh –help for a complete list of all available options. By default, the server listens on localhost:10000. You may override this bahaviour via either environment variables, i.e.:<br>export HIVE_SERVER2_THRIFT_PORT=<listening-port><br>export HIVE_SERVER2_THRIFT_BIND_HOST=<listening-host><br>./sbin/start-thriftserver.sh \<br>  –master <master-uri> \<br>  …<br>or system properties:<br>./sbin/start-thriftserver.sh \<br>  –hiveconf hive.server2.thrift.port=<listening-port> \<br>  –hiveconf hive.server2.thrift.bind.host=<listening-host> \<br>  –master <master-uri><br>  …</master-uri></listening-host></listening-port></master-uri></listening-host></listening-port></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-thriftserver.sh spark://server1:7077 --executor-memory 16g --total-executor-cores 200  --conf spark.storage.memoryFraction=0.8 spark-internal spark.shuffle.consolidateFiles=true spark.shuffle.spill.compress=true spark.kryoserializer.buffer.mb 128 --hiveconf hive.server2.thrift.port=9981</span><br></pre></td></tr></table></figure>
<h3 id="spark-sql-beeline"><a href="#spark-sql-beeline" class="headerlink" title="spark-sql beeline"></a>spark-sql beeline</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/bin/spark-sql --master spark://server1:7077 -e &quot;show databases&quot;</span><br><span class="line"></span><br><span class="line">$SPARK_HOME/bin/spark-sql -h</span><br><span class="line"></span><br><span class="line">$SPARK_HOME/bin/beeline -u &quot;jdbc:hive2://server1:10000&quot; -e &quot;show databases&quot; --  outputformat=table</span><br><span class="line"></span><br><span class="line">$SPARK_HOME/bin/beeline -u &quot;jdbc:hive2://server1:10000&quot; -n hsu -p hsu -d org.apache.hive.jdbc.HiveDriver --color=true -e &lt;&lt;EOF &apos;show databases&apos;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="2、使用spark-shell"><a href="#2、使用spark-shell" class="headerlink" title="2、使用spark-shell"></a>2、使用spark-shell</h2><p>这里需要注意，spark-shell必须在主节点启动，否则启动会失败！<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$spark_home/bin/spark-shell --master spark://server1:7077 --executor-memory 2g --total-executor-cores 5</span><br><span class="line"></span><br><span class="line">scala api：</span><br><span class="line">val textFile = spark.textFile(&quot;hdfs://...&quot;)</span><br><span class="line">val errors = textFile.filter(line =&gt; line.contains(&quot;ERROR&quot;))</span><br><span class="line">// Count all the errors</span><br><span class="line">errors.count()</span><br><span class="line">// Count errors mentioning MySQL</span><br><span class="line">errors.filter(line =&gt; line.contains(&quot;MySQL&quot;)).count()</span><br><span class="line">// Fetch the MySQL errors as an array of strings</span><br><span class="line">errors.filter(line =&gt; line.contains(&quot;MySQL&quot;)).collect()</span><br></pre></td></tr></table></figure></p>
<h2 id="3、streaming"><a href="#3、streaming" class="headerlink" title="3、streaming"></a>3、streaming</h2><h3 id="3-1-数据源来自网络端口"><a href="#3-1-数据源来自网络端口" class="headerlink" title="3.1 数据源来自网络端口"></a>3.1 数据源来自网络端口</h3><p>server2利用nc发送数据,这边不断生成数据,streaming不断计算<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nc -lk 9999</span><br><span class="line">hello word</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># spark/bin/spark-submit --class com.sparkjvm.streaming.yarn.NewHdfsWordCount --master spark://server1:7077 ./spark-yarn-1.0-SNAPSHOT.jar server2 9999 10</span><br></pre></td></tr></table></figure>
<p><img src="https://www.itweet.cn/screenshots/streaming-net.png" alt></p>
<p>程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">object NewHdfsWordCount &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    if (args.length &lt; 3) &#123;</span><br><span class="line">      System.err.println(&quot;Usage: HdfsWordCount &lt;master&gt; &lt;directory&gt; &lt;seconds&gt;&quot;)</span><br><span class="line">      System.exit(1)</span><br><span class="line">    &#125;</span><br><span class="line">    //新建StreamingContext</span><br><span class="line">    val ssc = new StreamingContext(new SparkConf(), Seconds(args(2).toInt))</span><br><span class="line"></span><br><span class="line">    //创建FileInputDStream，并指向特定目录</span><br><span class="line">    val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_ONLY_SER)</span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))</span><br><span class="line">    val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)</span><br><span class="line">    wordCounts.print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="3-2-数据来源hdfs"><a href="#3-2-数据来源hdfs" class="headerlink" title="3.2 数据来源hdfs"></a>3.2 数据来源hdfs</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -cat /data/test/a</span><br><span class="line">a</span><br><span class="line">a</span><br><span class="line">a</span><br><span class="line">a</span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<p>提交程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark/bin/spark-submit --class com.sparkjvm.streaming.standalone.HdfsWordCount --master spark://server1:7077 --executor-memory 10G --total-executor-cores 10 ./spark-yarn-1.0-SNAPSHOT.jar hdfs://mycluster/data/test</span><br></pre></td></tr></table></figure></p>
<p>put数据到test目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$  hadoop fs -put b /data/test/b</span><br><span class="line">$  hadoop fs -cat /data/test/b  </span><br><span class="line">b</span><br><span class="line">b</span><br><span class="line">b</span><br><span class="line">b</span><br><span class="line">b</span><br></pre></td></tr></table></figure></p>
<p><img src="https://www.itweet.cn/screenshots/streaming-hdfs.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$  hadoop fs -put c /data/test</span><br><span class="line">$  hadoop fs -cat /data/test/c</span><br><span class="line">c</span><br><span class="line">c</span><br><span class="line">c</span><br><span class="line">c</span><br><span class="line">c</span><br></pre></td></tr></table></figure>
<p><img src="https://www.itweet.cn/screenshots/streaming-hdfs-1.png" alt></p>
<h3 id="3-3-kafka-sparkstreaming"><a href="#3-3-kafka-sparkstreaming" class="headerlink" title="3.3 kafka+sparkstreaming"></a>3.3 kafka+sparkstreaming</h3><p><img src="https://www.itweet.cn/screenshots/kafka_streaming.png" alt></p>
<p>程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">object DirectKafkaWordCount &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    if (args.length &lt; 2) &#123;</span><br><span class="line">      System.err.println(s&quot;&quot;&quot;</span><br><span class="line">        |Usage: DirectKafkaWordCount &lt;brokers&gt; &lt;topics&gt;</span><br><span class="line">        |  &lt;brokers&gt; is a list of one or more Kafka brokers</span><br><span class="line">        |  &lt;topics&gt; is a list of one or more kafka topics to consume from</span><br><span class="line">        |</span><br><span class="line">        &quot;&quot;&quot;.stripMargin)</span><br><span class="line">      System.exit(1)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    StreamingExamples.setStreamingLogLevels()</span><br><span class="line"></span><br><span class="line">    val Array(brokers, topics) = args</span><br><span class="line"></span><br><span class="line">    // Create context with 2 second batch interval</span><br><span class="line">    val sparkConf = new SparkConf().setAppName(&quot;DirectKafkaWordCount&quot;)</span><br><span class="line">    val ssc =  new StreamingContext(sparkConf, Seconds(2))</span><br><span class="line"></span><br><span class="line">    // Create direct kafka stream with brokers and topics</span><br><span class="line">    val topicsSet = topics.split(&quot;,&quot;).toSet</span><br><span class="line">    val kafkaParams = Map[String, String](&quot;metadata.broker.list&quot; -&gt; brokers)</span><br><span class="line">    val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](</span><br><span class="line">      ssc, kafkaParams, topicsSet)</span><br><span class="line"></span><br><span class="line">    // Get the lines, split them into words, count the words and print</span><br><span class="line">    val lines = messages.map(_._2)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))</span><br><span class="line">    val wordCounts = words.map(x =&gt; (x, 1L)).reduceByKey(_ + _)</span><br><span class="line">    wordCounts.print()</span><br><span class="line"></span><br><span class="line">    // Start the computation</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>提交任务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark/bin/spark-submit --class org.sparkjvm.streaming.kafka.DirectKafkaWordCount --master spark://server1:7077 ./org.spark.code-1.0-SNAPSHOT-jar-with-dependencies.jar server1:9092,server2:9092,server3:9092,server4:9092,server5:9092,server6:9092,server7:9092,server8:9092,server9:9092,server10:9092 test</span><br></pre></td></tr></table></figure></p>
<p>kafka发送数据到test topic<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka/bin/kafka-console-producer.sh --broker-list server1:9092 --topic test</span><br><span class="line">a b c</span><br><span class="line">d e f</span><br></pre></td></tr></table></figure></p>
<p><img src="https://www.itweet.cn/screenshots/kafka_streaming_1.png" alt></p>
<h3 id="3-4-kafka-streaming-more-0-13-1"><a href="#3-4-kafka-streaming-more-0-13-1" class="headerlink" title="3.4 kafka+streaming more[0.13.1+]"></a>3.4 kafka+streaming more[0.13.1+]</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">The new API is simpler to use than the previous one.</span><br><span class="line">// Define the Kafka parameters, broker list must be specified</span><br><span class="line">val kafkaParams = Map(&quot;metadata.broker.list&quot; -&gt; &quot;localhost:9092,anotherhost:9092&quot;)</span><br><span class="line"></span><br><span class="line">// Define which topics to read from</span><br><span class="line">val topics = Set(&quot;sometopic&quot;, &quot;anothertopic&quot;)</span><br><span class="line"></span><br><span class="line">// Create the direct stream with the Kafka parameters and topics</span><br><span class="line">val kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](streamingContext, kafkaParams, topics)</span><br><span class="line">Since this direct approach does not have any receivers, you do not have to worry about creating multiple input DStreams to create more receivers. Nor do you have to configure the number of Kafka partitions to be consumed per receiver. Each Kafka partition will be automatically read in parallel.  Furthermore, each Kafka partition will correspond to a RDD partition, thus simplifying the parallelism model.</span><br><span class="line">In addition to the new streaming API, we have also introduced KafkaUtils.createRDD(), which can be used to run batch jobs on Kafka data.</span><br><span class="line">// Define the offset ranges to read in the batch job</span><br><span class="line">val offsetRanges = Array(</span><br><span class="line"> OffsetRange(&quot;some-topic&quot;, 0, 110, 220),</span><br><span class="line">OffsetRange(&quot;some-topic&quot;, 1, 100, 313),</span><br><span class="line">OffsetRange(&quot;another-topic&quot;, 0, 456, 789)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">// Create the RDD based on the offset ranges</span><br><span class="line">val rdd = KafkaUtils.createRDD[String, String, StringDecoder, StringDecoder](sparkContext, kafkaParams, offsetRanges)</span><br><span class="line">If you want to learn more about the API and the details of how it was implemented, take a look at the following.</span><br></pre></td></tr></table></figure>
<p>参考：<br><a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-kafka-integration.html</a></p>
<h2 id="3-5-MLlib"><a href="#3-5-MLlib" class="headerlink" title="3.5 MLlib"></a>3.5 MLlib</h2><pre><code>https://spark.apache.org/docs/latest/mllib-guide.html
</code></pre><blockquote>
<p>在有些情况下，你需要进行一些ETL工作，然后训练一个机器学习的模型，最后进行一些查询，如果是使用Spark，你可以在一段程序中将这三部分的逻辑完成形成一个大的有向无环图（DAG），而且Spark会对大的有向无环图进行整体优化。</p>
</blockquote>
<p>例如下面的程序：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val points = sqlContext.sql(   “SELECT latitude, longitude FROM   historic_tweets”)  </span><br><span class="line"></span><br><span class="line">val model = KMeans.train(points, 10)  </span><br><span class="line"></span><br><span class="line">sc.twitterStream(...)   .map(t =&gt; (model.closestCenter(t.location), 1)).reduceByWindow(“5s”, _ + _)</span><br></pre></td></tr></table></figure></p>
<h2 id="3-6-GraphX"><a href="#3-6-GraphX" class="headerlink" title="3.6 GraphX"></a>3.6 GraphX</h2><pre><code>https://spark.apache.org/docs/latest/graphx-programming-guide.html
</code></pre><h2 id="3-7-Reducer-number"><a href="#3-7-Reducer-number" class="headerlink" title="3.7 Reducer number"></a>3.7 Reducer number</h2><pre><code>In Shark, default reducer number is 1 and is controlled by the property mapred.reduce.tasks. Spark SQL deprecates this property in favor ofspark.sql.shuffle.partitions, whose default value is 200. Users may customize this property via SET:
SET spark.sql.shuffle.partitions=10;
SELECT page, count(*) c
FROM logs_last_month_cached
GROUP BY page ORDER BY c DESC LIMIT 10;
You may also put this property in hive-site.xml to override the default value.
For now, the mapred.reduce.tasks property is still recognized, and is converted to spark.sql.shuffle.partitions automatically.
</code></pre><h2 id="3-8-spark-查看-job-history-日志"><a href="#3-8-spark-查看-job-history-日志" class="headerlink" title="3.8  spark 查看 job history 日志"></a>3.8  spark 查看 job history 日志</h2><ul>
<li><p>(1). 修改配置文件</p>
<p>SPARK_HOME/conf 下:<br>spark-defaults.conf 增加如下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled true </span><br><span class="line">spark.eventLog.dir hdfs://server01:8020/tmp/spark </span><br><span class="line">spark.eventLog.compress true</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>  spark-env.sh 增加如下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://server01:8020/tmp/spark&quot;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>(2). 启动start-history-server.sh</p>
<p>SPARK_HOME/sbin 下: 执行 ./start-history-server.sh</p>
<p>spark job history webui: server01:18080</p>
<p>在spark任务运行完成之后,依然可通过web页面查看日志</p>
</li>
<li><p>(3). history server相关的配置参数描述</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">spark.history.updateInterval </span><br><span class="line">　　默认值：10 </span><br><span class="line">　　以秒为单位，更新日志相关信息的时间间隔</span><br><span class="line"></span><br><span class="line">spark.history.retainedApplications </span><br><span class="line">　　默认值：50 </span><br><span class="line">　　在内存中保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，当再次访问已被删除的应用信息时需要重新构建页面。</span><br><span class="line"></span><br><span class="line">spark.history.ui.port </span><br><span class="line">　　默认值：18080 </span><br><span class="line">　　HistoryServer的web端口</span><br><span class="line"></span><br><span class="line">spark.history.kerberos.enabled </span><br><span class="line">　　默认值：false </span><br><span class="line">　　是否使用kerberos方式登录访问HistoryServer，对于持久层位于安全集群的HDFS上是有用的，如果设置为true，就要配置下面的两个属性</span><br><span class="line"></span><br><span class="line">spark.history.kerberos.principal </span><br><span class="line">　　默认值：用于HistoryServer的kerberos主体名称</span><br><span class="line"></span><br><span class="line">spark.history.kerberos.keytab </span><br><span class="line">　　用于HistoryServer的kerberos keytab文件位置</span><br><span class="line"></span><br><span class="line">spark.history.ui.acls.enable </span><br><span class="line">　　默认值：false </span><br><span class="line">　　授权用户查看应用程序信息的时候是否检查acl。如果启用，只有应用程序所有者和spark.ui.view.acls指定的用户可以查看应用程序信息;否则，不做任何检查</span><br><span class="line"></span><br><span class="line">spark.eventLog.enabled </span><br><span class="line">　　默认值：false </span><br><span class="line">　　是否记录Spark事件，用于应用程序在完成后重构webUI</span><br><span class="line"></span><br><span class="line">spark.eventLog.dir </span><br><span class="line">　　默认值：file:///tmp/spark-events </span><br><span class="line">　　保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建</span><br><span class="line"></span><br><span class="line">spark.eventLog.compress </span><br><span class="line">　　默认值：false </span><br><span class="line">　　是否压缩记录Spark事件，前提spark.eventLog.enabled为true，默认使用的是snappy</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://spark.apache.org/" target="_blank" rel="noopener">https://spark.apache.org/</a></p>
<p>原创文章，转载请注明： 转载自<a href="http://www.itweet.cn" target="_blank" rel="noopener">Itweet</a>的博客<br><code>本博客的文章集合:</code> <a href="http://www.itweet.cn/blog/archive/" target="_blank" rel="noopener">http://www.itweet.cn/blog/archive/</a></p>

            </div>
          

    
      <footer class="post-footer">
		
		<div class="post-tags">
		  
			<a href="/tags/spark/">spark</a>
		  
		</div>
		

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2015/07/12/sqoop-install/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">sqoop install</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2015/07/12/git-manual/">
        <span class="next-text nav-default">git manual</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2014 -
    
    2019
    <span class="footer-author">Xu Jiang.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/realXuJiang/hexo-theme-polarbear">Polar Bear</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
