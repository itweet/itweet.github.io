<!DOCTYPE html>
<html lang="zh-CN">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="set hive-map-sum for hive">




  <meta name="keywords" content="Hive,">





  <link rel="alternate" href="/atom.xml" title="WHOAMI">




  <link rel="shortcut icon" type="image/x-icon" href="https://raw.githubusercontent.com/itweet/itweet.github.io/master/favicon.ico?v=1.1">



<link rel="canonical" href="http://itweet.github.io/2015/08/04/set-hive-map-sum-for-hive/">


<meta name="description" content="1、增加map数量首先调整上一步reducer生成文件数据，下面可以把reduce设置为160，即生成160个文件set mapred.reduce.tasks=160;create table test asselect * from tempdistribute by rand(123); 2、单纯调整map数量，增加map num===================初步 filenum ：">
<meta name="keywords" content="Hive">
<meta property="og:type" content="article">
<meta property="og:title" content="set hive-map-sum for hive">
<meta property="og:url" content="http://itweet.github.io/2015/08/04/set-hive-map-sum-for-hive/index.html">
<meta property="og:site_name" content="WHOAMI">
<meta property="og:description" content="1、增加map数量首先调整上一步reducer生成文件数据，下面可以把reduce设置为160，即生成160个文件set mapred.reduce.tasks=160;create table test asselect * from tempdistribute by rand(123); 2、单纯调整map数量，增加map num===================初步 filenum ：">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://www.itweet.cn/screenshots/hive-map.png">
<meta property="og:updated_time" content="2019-12-25T14:39:06.096Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="set hive-map-sum for hive">
<meta name="twitter:description" content="1、增加map数量首先调整上一步reducer生成文件数据，下面可以把reduce设置为160，即生成160个文件set mapred.reduce.tasks=160;create table test asselect * from tempdistribute by rand(123); 2、单纯调整map数量，增加map num===================初步 filenum ：">
<meta name="twitter:image" content="https://www.itweet.cn/screenshots/hive-map.png">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  <script id="baidu_analytics">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?b75d841b3068f7782eac9af2c8c866e7";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154998721-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154998721-1');
</script>


    <title> set hive-map-sum for hive - WHOAMI </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">WHOAMI</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/archives">
                            
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/atom.xml">
                            
                            
                                RSS
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          set hive-map-sum for hive
        
      </h1>

      <time class="post-time">
          8月 04 2015
      </time>
    </header>



    
            <div class="post-content">
            <h1 id="1、增加map数量"><a href="#1、增加map数量" class="headerlink" title="1、增加map数量"></a>1、增加map数量</h1><p>首先调整上一步reducer生成文件数据，下面可以把reduce设置为160，即生成160个文件<br>set mapred.reduce.tasks=160;<br>create table test as<br>select * from temp<br>distribute by rand(123);</p>
<h1 id="2、单纯调整map数量，增加map-num"><a href="#2、单纯调整map数量，增加map-num" class="headerlink" title="2、单纯调整map数量，增加map num"></a>2、单纯调整map数量，增加map num</h1><p>===================初步 filenum ：150 num , filesize: 1.2 G , map ：7 num, reduce : 100 num ====================================<br>hive (bigdata)&gt; set mapreduce.job.reduces;<br>mapreduce.job.reduces=-1<br>hive (default)&gt; set mapred.map.tasks;<br>mapred.map.tasks=200<br>hive (default)&gt; set mapred.reduce.tasks;<br>mapred.reduce.tasks=-1 –(default： 2)<br>hive (default)&gt; set dfs.block.size;<br>dfs.block.size=134217728<br>hive (bigdata)&gt; set mapred.min.split.size;<br>mapred.min.split.size=1<br>hive (default)&gt; set mapred.max.split.size;<br>mapred.max.split.size=256000000</p>
<p>drop table default.tb_user_terminal_test;<br>create table default.tb_user_terminal_test as  select sum(mdn),usp,times,start_time from bigdata.tb_user_terminal_udp_s2 group by mdn,times,start_time,usp;</p>
<p>– Time taken: 74.709 seconds</p>
<p>====================<br>hive (bigdata)&gt; set mapred.map.tasks;<br>mapred.map.tasks=160<br>hive (bigdata)&gt; set mapreduce.job.reduces;<br>mapreduce.job.reduces=100<br>hive (bigdata)&gt; set mapred.reduce.tasks;<br>mapred.reduce.tasks=150<br>hive (bigdata)&gt; set dfs.block.size;<br>dfs.block.size=16777216<br>hive (bigdata)&gt; set mapred.min.split.size;<br>mapred.min.split.size=1<br>hive (bigdata)&gt; set mapred.max.split.size;<br>mapred.max.split.size=2560000</p>
<p>drop table default.tb_user_terminal_test;<br>create table default.tb_user_terminal_test as  select sum(mdn),usp,times,start_time from bigdata.tb_user_terminal_udp_s2 group by mdn,times,start_time,usp;</p>
<p>– Time taken: 126.13 seconds</p>
<p>===================<br>hive (default)&gt; set mapreduce.job.reduces;<br>mapreduce.job.reduces=100<br>hive (default)&gt; set mapred.map.tasks;<br>mapred.map.tasks=200<br>hive (default)&gt; set mapred.reduce.tasks;<br>mapred.reduce.tasks=100<br>hive (default)&gt; set dfs.block.size;<br>dfs.block.size=134217728<br>hive (default)&gt; set mapred.min.split.size;<br>mapred.min.split.size=1<br>hive (default)&gt; set mapred.max.split.size;<br>mapred.max.split.size=25600000</p>
<p>drop table default.tb_user_terminal_test;<br>create table default.tb_user_terminal_test as  select sum(mdn),usp,times,start_time from bigdata.tb_user_terminal_udp_s2 group by mdn,times,start_time,usp;</p>
<p>– Time taken: 47.179 seconds</p>
<p>===================<br>hive (default)&gt; set mapreduce.job.reduces;<br>mapreduce.job.reduces=100<br>hive (default)&gt; set mapred.map.tasks; – <em><br>mapred.map.tasks=200<br>hive (default)&gt; set mapred.reduce.tasks; – </em><br>mapred.reduce.tasks=58<br>hive (default)&gt; set dfs.block.size;<br>dfs.block.size=134217728       – <em><br>hive (default)&gt; set mapred.min.split.size;<br>mapred.min.split.size=1<br>hive (default)&gt; set mapred.max.split.size;<br>mapred.max.split.size=25600000   – </em></p>
<p>drop table default.tb_user_terminal_test;<br>create table default.tb_user_terminal_test as  select sum(mdn),usp,times,start_time from bigdata.tb_user_terminal_udp_s2 group by mdn,times,start_time,usp;</p>
<p>– Time taken: 40.749 seconds</p>
<p>======================最终调整=== filesize : 1.2g, map ：150 num, reduce : 58 num , file: 150 num ========================</p>
<p>hive (default)&gt; set mapreduce.job.reduces;<br>mapreduce.job.reduces=100<br>hive (default)&gt; set mapred.map.tasks;<br>mapred.map.tasks=200<br>hive (default)&gt; set mapred.reduce.tasks;<br>mapred.reduce.tasks=58<br>hive (default)&gt;  set hive.merge.mapredfiles;<br>hive.merge.mapredfiles=false<br>hive (default)&gt; set dfs.block.size;<br>dfs.block.size=134217728<br>hive (default)&gt; set mapred.min.split.size;<br>mapred.min.split.size=1<br>hive (default)&gt; set mapred.max.split.size;<br>mapred.max.split.size=4560000<br>hive (default)&gt; set hive.groupby.skewindata;<br>set hive.groupby.skewindata=true</p>
<p>drop table default.tb_user_terminal_test;<br>create table default.tb_user_terminal_test as  select sum(mdn),usp,times,start_time from bigdata.tb_user_terminal_udp_s2 group by mdn,times,start_time,usp;</p>
<p>–Time taken: 42.903 seconds</p>
<p><code>由于我们需求是没有reducer，为了提高集群资源利用率，手动提高了map的数量！</code></p>
<p><code>结论：提高了map ：7--&gt;150 num，最后平均跑2h的任务，缩减平均10min!</code></p>
<p>每个任务执行执行效率都比较均衡：<br><img src="https://www.itweet.cn/screenshots/hive-map.png" alt></p>
<p>合理分配map,reduce个数,让某些大任务可以运行集群极限的map,reduce个数，这里怎么确定呢，需要参考<a href="https://www.itweet.cn/2015/07/24/yarn-resources-manager-allocation/" target="_blank" rel="noopener">yarn的资源调优</a>,让任务没有Pending，一起Running，那样就不会有任务拖后腿！提高执行效率！当然这里的优化参数最好针对每个应用内部设置！</p>
<h1 id="3、FileInputFormat中的getSplits–-gt-plitSize由来"><a href="#3、FileInputFormat中的getSplits–-gt-plitSize由来" class="headerlink" title="3、FileInputFormat中的getSplits–&gt;plitSize由来"></a>3、FileInputFormat中的getSplits–&gt;plitSize由来</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">/** Splits files returned by &#123;@link #listStatus(JobConf)&#125; when</span><br><span class="line">   * they&apos;re too big.*/ </span><br><span class="line">  public InputSplit[] getSplits(JobConf job, int numSplits)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">    StopWatch sw = new StopWatch().start();</span><br><span class="line">    FileStatus[] files = listStatus(job);</span><br><span class="line">    </span><br><span class="line">    // Save the number of input files for metrics/loadgen</span><br><span class="line">    job.setLong(NUM_INPUT_FILES, files.length);</span><br><span class="line">    long totalSize = 0;                           // compute total size</span><br><span class="line">    for (FileStatus file: files) &#123;                // check we have valid files</span><br><span class="line">      if (file.isDirectory()) &#123;</span><br><span class="line">        throw new IOException(&quot;Not a file: &quot;+ file.getPath());</span><br><span class="line">      &#125;</span><br><span class="line">      totalSize += file.getLen();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);</span><br><span class="line">    long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);</span><br><span class="line"></span><br><span class="line">    // generate splits</span><br><span class="line">    ArrayList&lt;FileSplit&gt; splits = new ArrayList&lt;FileSplit&gt;(numSplits);</span><br><span class="line">    NetworkTopology clusterMap = new NetworkTopology();</span><br><span class="line">    for (FileStatus file: files) &#123;</span><br><span class="line">      Path path = file.getPath();</span><br><span class="line">      long length = file.getLen();</span><br><span class="line">      if (length != 0) &#123;</span><br><span class="line">        FileSystem fs = path.getFileSystem(job);</span><br><span class="line">        BlockLocation[] blkLocations;</span><br><span class="line">        if (file instanceof LocatedFileStatus) &#123;</span><br><span class="line">          blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          blkLocations = fs.getFileBlockLocations(file, 0, length);</span><br><span class="line">        &#125;</span><br><span class="line">        if (isSplitable(fs, path)) &#123;</span><br><span class="line">          long blockSize = file.getBlockSize();</span><br><span class="line">          long splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line"></span><br><span class="line">          long bytesRemaining = length;</span><br><span class="line">          while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">            String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations,</span><br><span class="line">                length-bytesRemaining, splitSize, clusterMap);</span><br><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class="line">                splitHosts[0], splitHosts[1]));</span><br><span class="line">            bytesRemaining -= splitSize;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          if (bytesRemaining != 0) &#123;</span><br><span class="line">            String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations, length</span><br><span class="line">                - bytesRemaining, bytesRemaining, clusterMap);</span><br><span class="line">            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,</span><br><span class="line">                splitHosts[0], splitHosts[1]));</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);</span><br><span class="line">          splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123; </span><br><span class="line">        //Create empty hosts array for zero length files</span><br><span class="line">        splits.add(makeSplit(path, 0, length, new String[0]));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    sw.stop();</span><br><span class="line">    if (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(&quot;Total # of splits generated by getSplits: &quot; + splits.size()</span><br><span class="line">          + &quot;, TimeTaken: &quot; + sw.now(TimeUnit.MILLISECONDS));</span><br><span class="line">    &#125;</span><br><span class="line">    return splits.toArray(new FileSplit[splits.size()]);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://www.itweet.cn/2015/07/24/yarn-resources-manager-allocation/" target="_blank" rel="noopener">yarn的资源调优</a>,配合此文完成合理资源分配! </p>

            </div>
          

    
      <footer class="post-footer">
		
		<div class="post-tags">
		  
			<a href="/tags/Hive/">Hive</a>
		  
		</div>
		

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2015/08/10/新一代Impala的优势你竟然还不知道/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">新一代Impala的优势你竟然还不知道</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2015/07/27/kafka-use/">
        <span class="next-text nav-default">kafka use</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
    <div style="text-align:center;">
        <button class="btn" id="load-disqus" onclick="disqus.load();">加载 Disqus 评论</button>
    </div>
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2014 -
    
    2023
    <span class="footer-author">Xu Jiang.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/realXuJiang/hexo-theme-polarbear">Polar Bear</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    

<script type="text/javascript">
  var disqus_shortname = 'itweet-cn';
  var disqus_identifier = '2015/08/04/set-hive-map-sum-for-hive/';

  var disqus_title = "set hive-map-sum for hive";


  var disqus = {
    load : function disqus(){
        if(typeof DISQUS !== 'object') {
          (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
          }());
          $('#load-disqus').remove(); ///加载后移除按钮
        }
    }
  }

  
    var disqus_config = function () {
        this.page.url = disqus_url;
        this.page.identifier = disqus_identifier;
        this.page.title = disqus_title;
    };
  

</script>


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
