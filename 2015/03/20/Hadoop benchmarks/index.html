<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.8.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="Hadoop benchmarks">




  <meta name="keywords" content="Benchmarks,">





  <link rel="alternate" href="/atom.xml" title="WHOAMI">




  <link rel="shortcut icon" type="image/x-icon" href="https://raw.githubusercontent.com/itweet/itweet.github.io/master/favicon.ico?v=1.1">



<link rel="canonical" href="http://itweet.github.io/2015/03/20/Hadoop benchmarks/">


<meta name="description" content="一. Hadoop基准测试Hadoop自带了几个基准测试，被打包在几个jar包中。本文主要是cloudera版本测试12345[hsu@server01 ~]$ ls /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop* | egrep &amp;quot;examples|test&amp;quot;">
<meta name="keywords" content="Benchmarks">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop benchmarks">
<meta property="og:url" content="http://itweet.github.io/2015/03/20/Hadoop benchmarks/index.html">
<meta property="og:site_name" content="WHOAMI">
<meta property="og:description" content="一. Hadoop基准测试Hadoop自带了几个基准测试，被打包在几个jar包中。本文主要是cloudera版本测试12345[hsu@server01 ~]$ ls /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop* | egrep &amp;quot;examples|test&amp;quot;">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2018-07-02T13:09:15.479Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hadoop benchmarks">
<meta name="twitter:description" content="一. Hadoop基准测试Hadoop自带了几个基准测试，被打包在几个jar包中。本文主要是cloudera版本测试12345[hsu@server01 ~]$ ls /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop* | egrep &amp;quot;examples|test&amp;quot;">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  



    <title> Hadoop benchmarks - WHOAMI </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">WHOAMI</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/archives">
                            
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/atom.xml">
                            
                            
                                RSS
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Hadoop benchmarks
        
      </h1>

      <time class="post-time">
          3月 20 2015
      </time>
    </header>



    
            <div class="post-content">
            <h1 id="一-Hadoop基准测试"><a href="#一-Hadoop基准测试" class="headerlink" title="一. Hadoop基准测试"></a>一. Hadoop基准测试</h1><h2 id="Hadoop自带了几个基准测试，被打包在几个jar包中。本文主要是cloudera版本测试"><a href="#Hadoop自带了几个基准测试，被打包在几个jar包中。本文主要是cloudera版本测试" class="headerlink" title="Hadoop自带了几个基准测试，被打包在几个jar包中。本文主要是cloudera版本测试"></a>Hadoop自带了几个基准测试，被打包在几个jar包中。本文主要是cloudera版本测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ ls /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop* | egrep &quot;examples|test&quot;</span><br><span class="line">/opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-examples-2.5.0-mr1-cdh5.2.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-examples.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-test-2.5.0-mr1-cdh5.2.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-test.jar</span><br></pre></td></tr></table></figure>
<h3 id="1-、Hadoop-Test"><a href="#1-、Hadoop-Test" class="headerlink" title="(1)、Hadoop Test"></a>(1)、Hadoop Test</h3><h4 id="当不带参数调用hadoop-test-0-20-2-cdh3u3-jar时，会列出所有的测试程序："><a href="#当不带参数调用hadoop-test-0-20-2-cdh3u3-jar时，会列出所有的测试程序：" class="headerlink" title="当不带参数调用hadoop-test-0.20.2-cdh3u3.jar时，会列出所有的测试程序："></a>当不带参数调用hadoop-test-0.20.2-cdh3u3.jar时，会列出所有的测试程序：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-test.jar </span><br><span class="line">An example program must be given as the first argument.</span><br><span class="line">Valid program names are:</span><br><span class="line">DFSCIOTest: Distributed i/o benchmark of libhdfs.</span><br><span class="line">DistributedFSCheck: Distributed checkup of the file system consistency.</span><br><span class="line">MRReliabilityTest: A program that tests the reliability of the MR framework by injecting faults/failures</span><br><span class="line">TestDFSIO: Distributed i/o benchmark.</span><br><span class="line">dfsthroughput: measure hdfs throughput</span><br><span class="line">filebench: Benchmark SequenceFile(Input|Output)Format (block,record compressed and uncompressed), Text(Input|Output)Format (compressed and uncompressed)</span><br><span class="line">loadgen: Generic map/reduce load generator</span><br><span class="line">mapredtest: A map/reduce test check.</span><br><span class="line">minicluster: Single process HDFS and MR cluster.</span><br><span class="line">mrbench: A map/reduce benchmark that can create many small jobs</span><br><span class="line">nnbench: A benchmark that stresses the namenode.</span><br><span class="line">testarrayfile: A test for flat files of binary key/value pairs.</span><br><span class="line">testbigmapoutput: A map/reduce program that works on a very big non-splittable file and does identity map/reduce</span><br><span class="line">testfilesystem: A test for FileSystem read/write.</span><br><span class="line">testmapredsort: A map/reduce program that validates the map-reduce framework&apos;s sort.</span><br><span class="line">testrpc: A test for rpc.</span><br><span class="line">testsequencefile: A test for flat files of binary key value pairs.</span><br><span class="line">testsequencefileinputformat: A test for sequence file input format.</span><br><span class="line">testsetfile: A test for flat files of binary key/value pairs.</span><br><span class="line">testtextinputformat: A test for text input format.</span><br><span class="line">threadedmapbench: A map/reduce benchmark that compares the performance of maps with multiple spills over maps with 1 spill</span><br><span class="line"></span><br><span class="line">   == 这些程序从多个角度对Hadoop进行测试，TestDFSIO、mrbench和nnbench是三个广泛被使用的测试。</span><br></pre></td></tr></table></figure>
<h3 id="2-TestDFSIO-write"><a href="#2-TestDFSIO-write" class="headerlink" title="(2) TestDFSIO write"></a>(2) TestDFSIO write</h3><h4 id="TestDFSIO用于测试HDFS的IO性能，使用一个MapReduce作业来并发地执行读写操作，每个map任务用于读或写每个文件，map的输出用于收集与处理文件相关的统计信息，reduce用于累积统计信息，并产生summary。TestDFSIO的用法如下："><a href="#TestDFSIO用于测试HDFS的IO性能，使用一个MapReduce作业来并发地执行读写操作，每个map任务用于读或写每个文件，map的输出用于收集与处理文件相关的统计信息，reduce用于累积统计信息，并产生summary。TestDFSIO的用法如下：" class="headerlink" title="TestDFSIO用于测试HDFS的IO性能，使用一个MapReduce作业来并发地执行读写操作，每个map任务用于读或写每个文件，map的输出用于收集与处理文件相关的统计信息，reduce用于累积统计信息，并产生summary。TestDFSIO的用法如下："></a>TestDFSIO用于测试HDFS的IO性能，使用一个MapReduce作业来并发地执行读写操作，每个map任务用于读或写每个文件，map的输出用于收集与处理文件相关的统计信息，reduce用于累积统计信息，并产生summary。TestDFSIO的用法如下：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TestDFSIO</span><br><span class="line">Usage: TestDFSIO [genericOptions] -read | -write | -append | -clean [-nrFiles N] [-fileSize Size[B|KB|MB|GB|TB]] [-resFile resultFileName] [-bufferSize Bytes] [-rootDir]</span><br></pre></td></tr></table></figure>
<h4 id="以下的例子将往HDFS中写入10个1000MB的文件："><a href="#以下的例子将往HDFS中写入10个1000MB的文件：" class="headerlink" title="以下的例子将往HDFS中写入10个1000MB的文件："></a>以下的例子将往HDFS中写入10个1000MB的文件：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-test.jar TestDFSIO -write -nrFiles 10 -fileSize 1000</span><br><span class="line">15/01/13 15:14:17 INFO fs.TestDFSIO: TestDFSIO.1.7</span><br><span class="line">15/01/13 15:14:17 INFO fs.TestDFSIO: nrFiles = 10</span><br><span class="line">15/01/13 15:14:17 INFO fs.TestDFSIO: nrBytes (MB) = 1000.0</span><br><span class="line">15/01/13 15:14:17 INFO fs.TestDFSIO: bufferSize = 1000000</span><br><span class="line">15/01/13 15:14:17 INFO fs.TestDFSIO: baseDir = /benchmarks/TestDFSIO</span><br><span class="line">15/01/13 15:14:18 INFO fs.TestDFSIO: creating control file: 1048576000 bytes, 10 files</span><br><span class="line">15/01/13 15:14:19 INFO fs.TestDFSIO: created control files for: 10 files</span><br><span class="line">15/01/13 15:15:23 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write</span><br><span class="line">15/01/13 15:15:23 INFO fs.TestDFSIO:            Date &amp; time: Tue Jan 13 15:15:23 CST 2015</span><br><span class="line">15/01/13 15:15:23 INFO fs.TestDFSIO:        Number of files: 10</span><br><span class="line">15/01/13 15:15:23 INFO fs.TestDFSIO: Total MBytes processed: 10000.0</span><br><span class="line">15/01/13 15:15:23 INFO fs.TestDFSIO:      Throughput mb/sec: 29.67623230554649</span><br><span class="line">15/01/13 15:15:23 INFO fs.TestDFSIO: Average IO rate mb/sec: 29.899526596069336</span><br><span class="line">15/01/13 15:15:23 INFO fs.TestDFSIO:  IO rate std deviation: 2.6268824639446526</span><br><span class="line">15/01/13 15:15:23 INFO fs.TestDFSIO:     Test exec time sec: 64.203</span><br><span class="line">15/01/13 15:15:23 INFO fs.TestDFSIO:</span><br></pre></td></tr></table></figure>
<h3 id="3-TestDFSIO-read"><a href="#3-TestDFSIO-read" class="headerlink" title="(3) TestDFSIO read"></a>(3) TestDFSIO read</h3><h4 id="以下的例子将从HDFS中读取10个1000MB的文件："><a href="#以下的例子将从HDFS中读取10个1000MB的文件：" class="headerlink" title="以下的例子将从HDFS中读取10个1000MB的文件："></a>以下的例子将从HDFS中读取10个1000MB的文件：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-test.jar TestDFSIO -read -nrFiles 10 -fileSize 1000</span><br><span class="line">15/01/13 15:42:35 INFO fs.TestDFSIO: TestDFSIO.1.7</span><br><span class="line">15/01/13 15:42:35 INFO fs.TestDFSIO: nrFiles = 10</span><br><span class="line">15/01/13 15:42:35 INFO fs.TestDFSIO: nrBytes (MB) = 1000.0</span><br><span class="line">15/01/13 15:42:35 INFO fs.TestDFSIO: bufferSize = 1000000</span><br><span class="line">15/01/13 15:42:35 INFO fs.TestDFSIO: baseDir = /benchmarks/TestDFSIO</span><br><span class="line">15/01/13 15:42:36 INFO fs.TestDFSIO: creating control file: 1048576000 bytes, 10 files</span><br><span class="line">15/01/13 15:42:37 INFO fs.TestDFSIO: created control files for: 10 files</span><br></pre></td></tr></table></figure>
<h3 id="4-清空测试数据"><a href="#4-清空测试数据" class="headerlink" title="(4) 清空测试数据"></a>(4) 清空测试数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-test.jar TestDFSIO -clean</span><br><span class="line">15/01/13 15:46:51 INFO fs.TestDFSIO: TestDFSIO.1.7</span><br><span class="line">15/01/13 15:46:51 INFO fs.TestDFSIO: nrFiles = 1</span><br><span class="line">15/01/13 15:46:51 INFO fs.TestDFSIO: nrBytes (MB) = 1.0</span><br><span class="line">15/01/13 15:46:51 INFO fs.TestDFSIO: bufferSize = 1000000</span><br><span class="line">15/01/13 15:46:51 INFO fs.TestDFSIO: baseDir = /benchmarks/TestDFSIO</span><br><span class="line">15/01/13 15:46:52 INFO fs.TestDFSIO: Cleaning up test files</span><br></pre></td></tr></table></figure>
<h3 id="4-nnbench测试-NameNode-benchmark-nnbench"><a href="#4-nnbench测试-NameNode-benchmark-nnbench" class="headerlink" title="(4) nnbench测试[NameNode benchmark (nnbench)]"></a>(4) nnbench测试[NameNode benchmark (nnbench)]</h3><h4 id="nnbench用于测试NameNode的负载，它会生成很多与HDFS相关的请求，给NameNode施加较大的压力。这个测试能在HDFS上模拟创建、读取、重命名和删除文件等操作。nnbench的用法如下："><a href="#nnbench用于测试NameNode的负载，它会生成很多与HDFS相关的请求，给NameNode施加较大的压力。这个测试能在HDFS上模拟创建、读取、重命名和删除文件等操作。nnbench的用法如下：" class="headerlink" title="nnbench用于测试NameNode的负载，它会生成很多与HDFS相关的请求，给NameNode施加较大的压力。这个测试能在HDFS上模拟创建、读取、重命名和删除文件等操作。nnbench的用法如下："></a>nnbench用于测试NameNode的负载，它会生成很多与HDFS相关的请求，给NameNode施加较大的压力。这个测试能在HDFS上模拟创建、读取、重命名和删除文件等操作。nnbench的用法如下：</h4><h5 id="以下例子使用12个mapper和6个reducer来创建1000个文件："><a href="#以下例子使用12个mapper和6个reducer来创建1000个文件：" class="headerlink" title="以下例子使用12个mapper和6个reducer来创建1000个文件："></a>以下例子使用12个mapper和6个reducer来创建1000个文件：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-test.jar nnbench -operation create_write -maps 12 -reduces 6 -blockSize 1 -bytesToWrite 0 -numberOfFiles 1000 -replicationFactorPerFile 3 -readFileAfterOpen true -baseDir /benchmarks/NNBench-`hostname -s`</span><br><span class="line">NameNode Benchmark 0.4</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench: Test Inputs: </span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:            Test Operation: create_write</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:                Start time: 2015-01-13 15:55:33,585</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:            Number of maps: 12</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:         Number of reduces: 6</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:                Block Size: 1</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:            Bytes to write: 0</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:        Bytes per checksum: 1</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:           Number of files: 1000</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:        Replication factor: 3</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:                  Base dir: /benchmarks/NNBench-server01</span><br><span class="line">15/01/13 15:53:33 INFO hdfs.NNBench:      Read file after open: true</span><br><span class="line">15/01/13 15:53:34 INFO hdfs.NNBench: Deleting data directory</span><br><span class="line">15/01/13 15:53:34 INFO hdfs.NNBench: Creating 12 control files</span><br><span class="line"></span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench: -------------- NNBench -------------- : </span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                                Version: NameNode Benchmark 0.4</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                            Date &amp; time: 2015-01-13 15:56:06,539</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench: </span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                         Test Operation: create_write</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                             Start time: 2015-01-13 15:55:33,585</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                            Maps to run: 12</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                         Reduces to run: 6</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                     Block Size (bytes): 1</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                         Bytes to write: 0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                     Bytes per checksum: 1</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                        Number of files: 1000</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                     Replication factor: 3</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:             Successful file operations: 0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench: </span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:         # maps that missed the barrier: 0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                           # exceptions: 0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench: </span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                TPS: Create/Write/Close: 0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench: Avg exec time (ms): Create/Write/Close: 0.0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:             Avg Lat (ms): Create/Write: NaN</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                    Avg Lat (ms): Close: NaN</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench: </span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                  RAW DATA: AL Total #1: 0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                  RAW DATA: AL Total #2: 0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:               RAW DATA: TPS Total (ms): 0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:        RAW DATA: Longest Map Time (ms): 0.0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:                    RAW DATA: Late maps: 0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:              RAW DATA: # of exceptions: 0</span><br><span class="line">15/01/13 15:56:06 INFO hdfs.NNBench:</span><br></pre></td></tr></table></figure>
<h3 id="5-mrbench测试-MapReduce-benchmark-mrbench"><a href="#5-mrbench测试-MapReduce-benchmark-mrbench" class="headerlink" title="(5) mrbench测试[MapReduce benchmark (mrbench)]"></a>(5) mrbench测试[MapReduce benchmark (mrbench)]</h3><h4 id="mrbench会多次重复执行一个小作业，用于检查在机群上小作业的运行是否可重复以及运行是否高效。mrbench的用法如下："><a href="#mrbench会多次重复执行一个小作业，用于检查在机群上小作业的运行是否可重复以及运行是否高效。mrbench的用法如下：" class="headerlink" title="mrbench会多次重复执行一个小作业，用于检查在机群上小作业的运行是否可重复以及运行是否高效。mrbench的用法如下："></a>mrbench会多次重复执行一个小作业，用于检查在机群上小作业的运行是否可重复以及运行是否高效。mrbench的用法如下：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-test.jar mrbench --help</span><br><span class="line">MRBenchmark.0.0.2</span><br><span class="line">Usage: mrbench [-baseDir &lt;base DFS path for output/input, default is /benchmarks/MRBench&gt;] [-jar &lt;local path to job jar file containing Mapper and Reducer implementations, default is current jar file&gt;] [-numRuns &lt;number of times to run the job, default is 1&gt;] [-maps &lt;number of maps for each run, default is 2&gt;] [-reduces &lt;number of reduces for each run, default is 1&gt;] [-inputLines &lt;number of input lines to generate, default is 1&gt;] [-inputType &lt;type of input to generate, one of ascending (default), descending, random&gt;] [-verbose]</span><br></pre></td></tr></table></figure>
<h4 id="以下例子会运行一个小作业50次："><a href="#以下例子会运行一个小作业50次：" class="headerlink" title="以下例子会运行一个小作业50次："></a>以下例子会运行一个小作业50次：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-test.jar mrbench -numRuns 50</span><br><span class="line">MRBenchmark.0.0.2</span><br><span class="line">15/01/13 16:17:19 INFO mapred.MRBench: creating control file: 1 numLines, ASCENDING sortOrder</span><br><span class="line">15/01/13 16:17:20 INFO mapred.MRBench: created control file: /benchmarks/MRBench/mr_input/input_331064064.txt</span><br><span class="line">15/01/13 16:17:20 INFO mapred.MRBench: Running job 0: input=hdfs://server01:8020/benchmarks/MRBench/mr_input output=hdfs://server01:8020/benchmarks/MRBench/mr_output/output_556018847</span><br><span class="line"></span><br><span class="line">DataLines       Maps    Reduces AvgTime (milliseconds)</span><br><span class="line">1               2       1       26748</span><br></pre></td></tr></table></figure>
<p>以上结果表示平均作业完成时间是26秒。</p>
<h4 id="以下例子会运行一个小作业500次："><a href="#以下例子会运行一个小作业500次：" class="headerlink" title="以下例子会运行一个小作业500次："></a>以下例子会运行一个小作业500次：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-test.jar mrbench -numRuns 500 -maps 20 -reduces 10 -inputLines 50 -verbose</span><br><span class="line">MRBenchmark.0.0.2</span><br><span class="line">15/01/14 10:43:53 INFO mapred.MRBench: creating control file: 1 numLines, ASCENDING sortOrder</span><br><span class="line">15/01/14 10:43:54 INFO mapred.MRBench: created control file: /benchmarks/MRBench/mr_input/input_-1773312505.txt</span><br><span class="line">15/01/14 10:43:54 INFO mapred.MRBench: Running job 0: input=hdfs://server01:8020/benchmarks/MRBench/mr_input output=hdfs://server01:8020/benchmarks/MRBench/mr_output/output_-447811996</span><br><span class="line">15/01/14 10:43:54 INFO client.RMProxy: Connecting to ResourceManager at server01/135.33.5.53:8032</span><br><span class="line">15/01/14 10:43:54 INFO client.RMProxy: Connecting to ResourceManager at server01/135.33.5.53:8032</span><br><span class="line">15/01/14 10:43:54 INFO mapred.FileInputFormat: Total input paths to process : 1</span><br><span class="line">15/01/14 10:43:55 INFO mapreduce.JobSubmitter: number of splits:2</span><br><span class="line">15/01/14 10:43:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1420542591388_0112</span><br><span class="line">15/01/14 10:43:55 INFO impl.YarnClientImpl: Submitted application application_1420542591388_0112</span><br><span class="line">15/01/14 10:43:55 INFO mapreduce.Job: The url to track the job: http://server01:8088/proxy/application_1420542591388_0112/</span><br><span class="line">15/01/14 10:43:55 INFO mapreduce.Job: Running job: job_1420542591388_0112</span><br><span class="line">15/01/14 10:44:06 INFO mapreduce.Job: Job job_1420542591388_0112 running in uber mode : false</span><br><span class="line">Total milliseconds for task: 494 = 29859</span><br><span class="line">Total milliseconds for task: 495 = 29878</span><br><span class="line">Total milliseconds for task: 496 = 29908</span><br><span class="line">Total milliseconds for task: 497 = 29943</span><br><span class="line">Total milliseconds for task: 498 = 29897</span><br><span class="line">Total milliseconds for task: 499 = 29919</span><br><span class="line">Total milliseconds for task: 500 = 28881</span><br><span class="line">DataLines       Maps    Reduces AvgTime (milliseconds)</span><br><span class="line">50              40      20      31298</span><br></pre></td></tr></table></figure>
<p>以上结果表示平均作业完成时间是31秒。</p>
<h3 id="6-Hadoop-Examples"><a href="#6-Hadoop-Examples" class="headerlink" title="(6)  Hadoop Examples"></a>(6)  Hadoop Examples</h3><h4 id="除了上文提到的测试，Hadoop还自带了一些例子，比如WordCount和TeraSort，这些例子在hadoop-examples-jar中。"><a href="#除了上文提到的测试，Hadoop还自带了一些例子，比如WordCount和TeraSort，这些例子在hadoop-examples-jar中。" class="headerlink" title="除了上文提到的测试，Hadoop还自带了一些例子，比如WordCount和TeraSort，这些例子在hadoop-examples*.jar中。"></a>除了上文提到的测试，Hadoop还自带了一些例子，比如WordCount和TeraSort，这些例子在hadoop-examples*.jar中。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ ls /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-examples*</span><br><span class="line">/opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-examples-2.5.0-mr1-cdh5.2.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-examples.jar</span><br></pre></td></tr></table></figure>
<h4 id="执行以下命令会列出所有的示例程序："><a href="#执行以下命令会列出所有的示例程序：" class="headerlink" title="执行以下命令会列出所有的示例程序："></a>执行以下命令会列出所有的示例程序：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-examples.jar</span><br><span class="line">  An example program must be given as the first argument.</span><br><span class="line">  Valid program names are:</span><br><span class="line">  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.</span><br><span class="line">  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.</span><br><span class="line">  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.</span><br><span class="line">  dbcount: An example job that count the pageview counts from a database.</span><br><span class="line">  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.</span><br><span class="line">  grep: A map/reduce program that counts the matches of a regex in the input.</span><br><span class="line">  join: A job that effects a join over sorted, equally partitioned datasets</span><br><span class="line">  multifilewc: A job that counts words from several files.</span><br><span class="line">  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.</span><br><span class="line">  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.</span><br><span class="line">  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.</span><br><span class="line">  randomwriter: A map/reduce program that writes 10GB of random data per node.</span><br><span class="line">  secondarysort: An example defining a secondary sort to the reduce.</span><br><span class="line">  sort: A map/reduce program that sorts the data written by the random writer.</span><br><span class="line">  sudoku: A sudoku solver.</span><br><span class="line">  teragen: Generate data for the terasort</span><br><span class="line">  terasort: Run the terasort</span><br><span class="line">  teravalidate: Checking results of terasort</span><br><span class="line">  wordcount: A map/reduce program that counts the words in the input files.</span><br><span class="line">  wordmean: A map/reduce program that counts the average length of the words in the input files.</span><br><span class="line">  wordmedian: A map/reduce program that counts the median length of the words in the input files.</span><br><span class="line">  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.</span><br></pre></td></tr></table></figure>
<h3 id="7-TeraSort-TeraSort-Run-the-actual-TeraSort-benchmark"><a href="#7-TeraSort-TeraSort-Run-the-actual-TeraSort-benchmark" class="headerlink" title="(7) TeraSort[TeraSort: Run the actual TeraSort benchmark]"></a>(7) TeraSort[TeraSort: Run the actual TeraSort benchmark]</h3><h4 id="一个完整的TeraSort测试需要按以下三步执行："><a href="#一个完整的TeraSort测试需要按以下三步执行：" class="headerlink" title="一个完整的TeraSort测试需要按以下三步执行："></a>一个完整的TeraSort测试需要按以下三步执行：</h4><ul>
<li>1、用TeraGen生成随机数据</li>
<li>2、对输入数据运行TeraSort</li>
<li><p>3、用TeraValidate验证排好序的输出数据并不需要在每次测试时都生成输入数据，生成一次数据之后，每次测试可以跳过第一步。</p>
</li>
<li><p>TeraGen的用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop jar hadoop-*examples*.jar teragen &lt;number of 100-byte rows&gt; &lt;output dir&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>以下命令运行TeraGen生成10GB的输入数据，并输出到目录/examples/terasort-input：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-examples.jar teragen 100000000 /examples/terasort-input</span><br><span class="line">			15/01/13 16:57:34 INFO client.RMProxy: Connecting to ResourceManager at server01/135.33.5.53:8032</span><br><span class="line">			15/01/13 16:57:35 INFO terasort.TeraSort: Generating 100000000 using 2</span><br><span class="line">			15/01/13 16:57:35 INFO mapreduce.JobSubmitter: number of splits:2</span><br><span class="line">			15/01/13 16:59:07 INFO mapreduce.Job: Job job_1420542591388_0105 completed successfully</span><br><span class="line">			15/01/13 16:59:08 INFO mapreduce.Job: Counters: 31</span><br><span class="line">			        File System Counters</span><br><span class="line">			                FILE: Number of bytes read=0</span><br><span class="line">			                FILE: Number of bytes written=211922</span><br><span class="line">			                FILE: Number of read operations=0</span><br><span class="line">			                FILE: Number of large read operations=0</span><br><span class="line">			                FILE: Number of write operations=0</span><br><span class="line">			                HDFS: Number of bytes read=170</span><br><span class="line">			                HDFS: Number of bytes written=10000000000</span><br><span class="line">			                HDFS: Number of read operations=8</span><br><span class="line">			                HDFS: Number of large read operations=0</span><br><span class="line">			                HDFS: Number of write operations=4</span><br><span class="line">			        Job Counters </span><br><span class="line">			                Launched map tasks=2</span><br><span class="line">			                Other local map tasks=2</span><br><span class="line">			                Total time spent by all maps in occupied slots (ms)=150416</span><br><span class="line">			                Total time spent by all reduces in occupied slots (ms)=0</span><br><span class="line">			                Total time spent by all map tasks (ms)=150416</span><br><span class="line">			                Total vcore-seconds taken by all map tasks=150416</span><br><span class="line">			                Total megabyte-seconds taken by all map tasks=154025984</span><br><span class="line">			        Map-Reduce Framework</span><br><span class="line">			                Map input records=100000000</span><br><span class="line">			                Map output records=100000000</span><br><span class="line">			                Input split bytes=170</span><br><span class="line">			                Spilled Records=0</span><br><span class="line">			                Failed Shuffles=0</span><br><span class="line">			                Merged Map outputs=0</span><br><span class="line">			                GC time elapsed (ms)=1230</span><br><span class="line">			                CPU time spent (ms)=175090</span><br><span class="line">			                Physical memory (bytes) snapshot=504807424</span><br><span class="line">			                Virtual memory (bytes) snapshot=3230924800</span><br><span class="line">			                Total committed heap usage (bytes)=1363148800</span><br><span class="line">			        org.apache.hadoop.examples.terasort.TeraGen$Counters</span><br><span class="line">			                CHECKSUM=214760662691937609</span><br><span class="line">			        File Input Format Counters </span><br><span class="line">			                Bytes Read=0</span><br><span class="line">			        File Output Format Counters </span><br><span class="line">			                Bytes Written=10000000000</span><br></pre></td></tr></table></figure>
<ul>
<li><p>TeraGen产生的数据每行的格式如下：</p>
<pre><code>&lt;10 bytes key&gt;&lt;10 bytes rowid&gt;&lt;78 bytes filler&gt;\r\n
</code></pre><p>** 其中：<br>1、key是一些随机字符，每个字符的ASCII码取值范围为[32, 126] <br>2、rowid是一个整数，右对齐 <br>3、filler由7组字符组成，每组有10个字符（最后一组8个），字符从’A’到’Z’依次取值        </p>
</li>
<li>以下命令运行TeraSort对数据进行排序，并将结果输出到目录/examples/terasort-output：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-examples.jar terasort /examples/terasort-input /examples/terasort-output</span><br><span class="line">15/01/13 17:08:08 INFO terasort.TeraSort: starting</span><br><span class="line">15/01/13 17:08:10 INFO input.FileInputFormat: Total input paths to process : 2</span><br><span class="line">Spent 187ms computing base-splits.</span><br><span class="line">Spent 3ms computing TeraScheduler splits.</span><br><span class="line">Computing input splits took 192ms</span><br><span class="line">Sampling 10 splits of 76</span><br><span class="line">Making 144 from 100000 sampled records</span><br><span class="line">Computing parititions took 596ms</span><br><span class="line">Spent 791ms computing partitions.terasort /examples/terasort-input /examples/terasort-output</span><br><span class="line">15/01/13 17:09:13 INFO mapreduce.Job: Counters: 50</span><br><span class="line">        File System Counters</span><br><span class="line">                FILE: Number of bytes read=4461968618</span><br><span class="line">                FILE: Number of bytes written=8889668662</span><br><span class="line">                FILE: Number of read operations=0</span><br><span class="line">                FILE: Number of large read operations=0</span><br><span class="line">                FILE: Number of write operations=0</span><br><span class="line">                HDFS: Number of bytes read=10000010260</span><br><span class="line">                HDFS: Number of bytes written=10000000000</span><br><span class="line">                HDFS: Number of read operations=660</span><br><span class="line">                HDFS: Number of large read operations=0</span><br><span class="line">                HDFS: Number of write operations=288</span><br><span class="line">        Job Counters </span><br><span class="line">                Launched map tasks=76</span><br><span class="line">                Launched reduce tasks=144</span><br><span class="line">                Data-local map tasks=75</span><br><span class="line">                Rack-local map tasks=1</span><br><span class="line">                Total time spent by all maps in occupied slots (ms)=933160</span><br><span class="line">                Total time spent by all reduces in occupied slots (ms)=1227475</span><br><span class="line">                Total time spent by all map tasks (ms)=933160</span><br><span class="line">                Total time spent by all reduce tasks (ms)=1227475</span><br><span class="line">                Total vcore-seconds taken by all map tasks=933160</span><br><span class="line">                Total vcore-seconds taken by all reduce tasks=1227475</span><br><span class="line">                Total megabyte-seconds taken by all map tasks=955555840</span><br><span class="line">                Total megabyte-seconds taken by all reduce tasks=1256934400</span><br><span class="line">        Map-Reduce Framework</span><br><span class="line">                Map input records=100000000</span><br><span class="line">                Map output records=100000000</span><br><span class="line">                Map output bytes=10200000000</span><br><span class="line">                Map output materialized bytes=4403942936</span><br><span class="line">                Input split bytes=10260</span><br><span class="line">                Combine input records=0</span><br><span class="line">                Combine output records=0</span><br><span class="line">                Reduce input groups=100000000</span><br><span class="line">                Reduce shuffle bytes=4403942936</span><br><span class="line">                Reduce input records=100000000</span><br><span class="line">                Reduce output records=100000000</span><br><span class="line">                Spilled Records=200000000</span><br><span class="line">                Shuffled Maps =10944</span><br><span class="line">                Failed Shuffles=0</span><br><span class="line">                Merged Map outputs=10944</span><br><span class="line">                GC time elapsed (ms)=45169</span><br><span class="line">                CPU time spent (ms)=2021010</span><br><span class="line">                Physical memory (bytes) snapshot=95792517120</span><br><span class="line">                Virtual memory (bytes) snapshot=357225058304</span><br><span class="line">                Total committed heap usage (bytes)=174283816960</span><br><span class="line">        Shuffle Errors</span><br><span class="line">                BAD_ID=0</span><br><span class="line">                CONNECTION=0</span><br><span class="line">                IO_ERROR=0</span><br><span class="line">                WRONG_LENGTH=0</span><br><span class="line">                WRONG_MAP=0</span><br><span class="line">                WRONG_REDUCE=0</span><br><span class="line">        File Input Format Counters </span><br><span class="line">                Bytes Read=10000000000</span><br><span class="line">        File Output Format Counters </span><br><span class="line">                Bytes Written=10000000000</span><br><span class="line">15/01/13 17:09:13 INFO terasort.TeraSort: done</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="8-terasort-validate-验证是否有序"><a href="#8-terasort-validate-验证是否有序" class="headerlink" title="(8) terasort-validate 验证是否有序"></a>(8) terasort-validate 验证是否有序</h3><h4 id="以下命令运行TeraValidate来验证TeraSort输出的数据是否有序，如果检测到问题，将乱序的key输出到目录-examples-terasort-validate"><a href="#以下命令运行TeraValidate来验证TeraSort输出的数据是否有序，如果检测到问题，将乱序的key输出到目录-examples-terasort-validate" class="headerlink" title="以下命令运行TeraValidate来验证TeraSort输出的数据是否有序，如果检测到问题，将乱序的key输出到目录/examples/terasort-validate"></a>以下命令运行TeraValidate来验证TeraSort输出的数据是否有序，如果检测到问题，将乱序的key输出到目录/examples/terasort-validate</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ sudo hadoop jar /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop-0.20-mapreduce/hadoop-examples.jar teravalidate /examples/terasort-output /examples/terasort-validate</span><br><span class="line">	15/01/13 17:17:37 INFO client.RMProxy: Connecting to ResourceManager at server01/135.33.5.53:8032</span><br><span class="line">	15/01/13 17:17:38 INFO input.FileInputFormat: Total input paths to process : 144</span><br><span class="line">	Spent 93ms computing base-splits.</span><br><span class="line">	Spent 3ms computing TeraScheduler splits.</span><br><span class="line">	15/01/13 17:17:38 INFO mapreduce.JobSubmitter: number of splits:144</span><br><span class="line">	15/01/13 17:17:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1420542591388_0107</span><br><span class="line">	15/01/13 17:17:38 INFO impl.YarnClientImpl: Submitted application application_1420542591388_0107teravalidate /examples/terasort-output /examples/terasort-validate</span><br><span class="line">	15/01/13 17:18:12 INFO mapreduce.Job: Job job_1420542591388_0107 completed successfully</span><br><span class="line">	15/01/13 17:18:12 INFO mapreduce.Job: Counters: 50</span><br><span class="line">	        File System Counters</span><br><span class="line">	                FILE: Number of bytes read=6963</span><br><span class="line">	                FILE: Number of bytes written=15445453</span><br><span class="line">	                FILE: Number of read operations=0</span><br><span class="line">	                FILE: Number of large read operations=0</span><br><span class="line">	                FILE: Number of write operations=0</span><br><span class="line">	                HDFS: Number of bytes read=10000019584</span><br><span class="line">	                HDFS: Number of bytes written=25</span><br><span class="line">	                HDFS: Number of read operations=435</span><br><span class="line">	                HDFS: Number of large read operations=0</span><br><span class="line">	                HDFS: Number of write operations=2</span><br><span class="line">	        Job Counters </span><br><span class="line">	                Launched map tasks=144</span><br><span class="line">	                Launched reduce tasks=1</span><br><span class="line">	                Data-local map tasks=142</span><br><span class="line">	                Rack-local map tasks=2</span><br><span class="line">	                Total time spent by all maps in occupied slots (ms)=685624</span><br><span class="line">	                Total time spent by all reduces in occupied slots (ms)=3384</span><br><span class="line">	                Total time spent by all map tasks (ms)=685624</span><br><span class="line">	                Total time spent by all reduce tasks (ms)=3384</span><br><span class="line">	                Total vcore-seconds taken by all map tasks=685624</span><br><span class="line">	                Total vcore-seconds taken by all reduce tasks=3384</span><br><span class="line">	                Total megabyte-seconds taken by all map tasks=702078976</span><br><span class="line">	                Total megabyte-seconds taken by all reduce tasks=3465216</span><br><span class="line">	        Map-Reduce Framework</span><br><span class="line">	                Map input records=100000000</span><br><span class="line">	                Map output records=432</span><br><span class="line">	                Map output bytes=11664</span><br><span class="line">	                Map output materialized bytes=13830</span><br><span class="line">	                Input split bytes=19584</span><br><span class="line">	                Combine input records=0</span><br><span class="line">	                Combine output records=0</span><br><span class="line">	                Reduce input groups=289</span><br><span class="line">	                Reduce shuffle bytes=13830</span><br><span class="line">	                Reduce input records=432</span><br><span class="line">	                Reduce output records=1</span><br><span class="line">	                Spilled Records=864</span><br><span class="line">	                Shuffled Maps =144</span><br><span class="line">	                Failed Shuffles=0</span><br><span class="line">	                Merged Map outputs=144</span><br><span class="line">	                GC time elapsed (ms)=4014</span><br><span class="line">	                CPU time spent (ms)=334280</span><br><span class="line">	                Physical memory (bytes) snapshot=85470654464</span><br><span class="line">	                Virtual memory (bytes) snapshot=234019295232</span><br><span class="line">	                Total committed heap usage (bytes)=114868879360</span><br><span class="line">	        Shuffle Errors</span><br><span class="line">	                BAD_ID=0</span><br><span class="line">	                CONNECTION=0</span><br><span class="line">	                IO_ERROR=0</span><br><span class="line">	                WRONG_LENGTH=0</span><br><span class="line">	                WRONG_MAP=0</span><br><span class="line">	                WRONG_REDUCE=0</span><br><span class="line">	        File Input Format Counters </span><br><span class="line">	                Bytes Read=10000000000</span><br><span class="line">	        File Output Format Counters </span><br><span class="line">	                Bytes Written=25</span><br><span class="line"></span><br><span class="line">		[hsu@server01 ~]$ hadoop fs -cat /examples/terasort-validate/*                                                           checksum        2fafbaf537afd49</span><br></pre></td></tr></table></figure>
<p>结论：检测通过</p>
<h3 id="10-总结"><a href="#10-总结" class="headerlink" title="(10) 总结"></a>(10) 总结</h3><h4 id="在提交任务目录下会生成两个文件"><a href="#在提交任务目录下会生成两个文件" class="headerlink" title="在提交任务目录下会生成两个文件"></a>在提交任务目录下会生成两个文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hsu@server01 ~]$ LANG=en</span><br><span class="line">[hsu@server01 ~]$ ll</span><br><span class="line">total 16</span><br><span class="line">-rw-r--r-- 1 root root 1142 Jan 13 15:56 NNBench_results.log</span><br><span class="line">-rw-r--r-- 1 root root  903 Jan 13 15:43 TestDFSIO_results.log</span><br></pre></td></tr></table></figure>
<p>约对176838144行数据进行排序，部分数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0000000: 	00 00 00 a7 0d 2a a8 02 da da 00 11 30 30 30 30	  .....*......0000</span><br><span class="line">0000010: 	30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30	  0000000000000000</span><br></pre></td></tr></table></figure></p>
<h4 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h4><pre><code>http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/
https://github.com/intel-hadoop/HiBench
</code></pre><h1 id="二、hive-impala测试"><a href="#二、hive-impala测试" class="headerlink" title="二、hive/impala测试"></a>二、hive/impala测试</h1><h3 id="Impala-hive性能报告："><a href="#Impala-hive性能报告：" class="headerlink" title="Impala/hive性能报告："></a>Impala/hive性能报告：</h3><blockquote>
<blockquote>
<p>下面对event_calling_201410(39.8G)和event_sms_201410(39.8G)做join操作和count(*)：</p>
</blockquote>
</blockquote>
<h4 id="1-count-操作"><a href="#1-count-操作" class="headerlink" title="(1) count(*)操作"></a>(1) count(*)操作</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[yndx-bigdata-hadoop02:21000] &gt; select count(*) from event_calling_201410;</span><br><span class="line">Query: select count(*) from event_calling_201410</span><br><span class="line">+-----------+</span><br><span class="line">| count(*)  |</span><br><span class="line">+-----------+</span><br><span class="line">| 425883373 |</span><br><span class="line">+-----------+</span><br><span class="line">Fetched 1 row(s) in 192.75s</span><br><span class="line">hive (i_bil_hb_m)&gt; select count(*) from event_calling_201410;</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">425883373</span><br><span class="line">Time taken: 386.804 seconds, Fetched: 1 row(s)</span><br><span class="line">[yndx-bigdata-hadoop02:21000] &gt; select count(*) from event_sms_201410;</span><br><span class="line">Query: select count(*) from event_sms_201410</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">| 80675409 |</span><br><span class="line">+----------+</span><br><span class="line">Fetched 1 row(s) in 33.52s</span><br></pre></td></tr></table></figure>
<h4 id="2-两表join操作"><a href="#2-两表join操作" class="headerlink" title="(2) 两表join操作"></a>(2) 两表join操作</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">hive (i_bil_hb_m)&gt; select count(*) from event_calling_201410 c left outer join event_sms_201410 s on(s.calling_nbr=c.calling_nbr);  </span><br><span class="line">Total jobs = 2</span><br><span class="line">Stage-1 is selected by condition resolver.</span><br><span class="line">Launching Job 1 out of 2</span><br><span class="line">Number of reduce tasks not specified. Estimated from input data size: 279</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Starting Job = job_1420542591388_0987, Tracking URL = http://yndx-bigdata-hadoop01:8088/proxy/application_1420542591388_0987/</span><br><span class="line">Kill Command = /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/hadoop/bin/hadoop job  -kill job_1420542591388_0987</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1110; number of reducers: 279</span><br><span class="line">2015-01-15 10:44:01,665 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 19731.27 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 0 days 5 hours 28 minutes 51 seconds 270 msec</span><br><span class="line">Ended Job = job_1420542591388_0987</span><br><span class="line">Launching Job 2 out of 2</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">2015-01-15 10:44:33,709 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 15.28 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 15 seconds 280 msec</span><br><span class="line">Ended Job = job_1420542591388_0988</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1110  Reduce: 279   Cumulative CPU: 19731.27 sec   HDFS Read: 298693978456 HDFS Write: 32922 SUCCESS</span><br><span class="line">Stage-Stage-2: Map: 7  Reduce: 1   Cumulative CPU: 15.28 sec   HDFS Read: 97828 HDFS Write: 12 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 days 5 hours 29 minutes 6 seconds 550 msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">13106534553</span><br><span class="line">Time taken: 413.651 seconds, Fetched: 1 row(s)</span><br><span class="line">[yndx-bigdata-hadoop02:21000] &gt;  select count(*) from event_calling_201410 c left outer join event_sms_201410 s on(s.calling_nbr=c.calling_nbr);</span><br><span class="line">Query: select count(*) from event_calling_201410 c left outer join event_sms_201410 s on(s.calling_nbr=c.calling_nbr)</span><br><span class="line"> +-------------+</span><br><span class="line">| count(*)    |</span><br><span class="line">+-------------+</span><br><span class="line">| 13106534553 |</span><br><span class="line">+-------------+</span><br><span class="line">Fetched 1 row(s) in 525.48s</span><br></pre></td></tr></table></figure>
<h3 id="3-统计结果"><a href="#3-统计结果" class="headerlink" title="(3) 统计结果"></a>(3) 统计结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Action		数据量(G)	   HiveTime(s)	   ImpalaTime(s) Hive结论 Imapla结论</span><br><span class="line">Count(*)	 39.8			386.804			192.75		通过  警告阈值(内存)</span><br><span class="line">join(2)		 39.8*2			413.651			525.48		通过  警告阈值(内存)</span><br></pre></td></tr></table></figure>
<p>原创文章，转载请注明： 转载自<a href="http://www.itweet.cn" target="_blank" rel="noopener">Itweet</a>的博客<br><code>本博客的文章集合:</code> <a href="http://www.itweet.cn/blog/archive/" target="_blank" rel="noopener">http://www.itweet.cn/blog/archive/</a></p>

            </div>
          

    
      <footer class="post-footer">
		
		<div class="post-tags">
		  
			<a href="/tags/Benchmarks/">Benchmarks</a>
		  
		</div>
		

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2015/04/06/程序员必读的书 StackOverflow 创始人推荐/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">程序员必读的书 StackOverflow 创始人推荐</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2015/02/03/apache hadoop federation配置/">
        <span class="next-text nav-default">apache hadoop federation配置</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2014 -
    
    2019
    <span class="footer-author">Xu Jiang.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/frostfan/hexo-theme-polarbear">Polar Bear</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
