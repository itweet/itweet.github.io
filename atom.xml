<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WHOAMI</title>
  <icon>https://www.gravatar.com/avatar/640acd48ce2a9ee0769faec6c0f33311</icon>
  <subtitle>技术人的日常.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://itweet.github.io/"/>
  <updated>2023-05-27T06:48:46.154Z</updated>
  <id>http://itweet.github.io/</id>
  
  <author>
    <name>Xu Jiang</name>
    <email>realJiangXu[at]gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>April_reading</title>
    <link href="http://itweet.github.io/2021/05/18/april_reading/"/>
    <id>http://itweet.github.io/2021/05/18/april_reading/</id>
    <published>2021-05-18T04:19:18.000Z</published>
    <updated>2023-05-27T06:48:46.154Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《若为自由故：自由软件之父理查德·斯托曼传》"><a href="#《若为自由故：自由软件之父理查德·斯托曼传》" class="headerlink" title="《若为自由故：自由软件之父理查德·斯托曼传》"></a>《若为自由故：自由软件之父理查德·斯托曼传》</h2><p>理查德·斯托曼最伟大的不是 Emacs，而是创造了 GUN 自由软件协议。那个时代的 “黑客” 与我们如今理解的 ’黑客‘ 有很大区别。</p><p>斯托曼作为一个伟大的黑客，有爱折腾的坚强固执本性，遇到困难从不言弃、博览群书、伟大的领袖都有的特质。</p><p>很难想象，最早的自由软件，在几个著名的实验室之间相互 copy，利用互联网将改动的代码，由作者进行统一 merge 管理，协作起来非常低效，这大概也是 Linus 创建 GIT 软件的初衷，让世界各地黑客利用网络就能相互协作，实现伟大的想法。</p><p><a href="http://www.gnu.org/philosophy/rms-nyu-2001-transcript.txt" target="_blank" rel="noopener">Free Software: Freedom and Cooperation</a></p><blockquote><p>作为黑客，我们总是想给程序起一些有趣或古怪的名字，因为给程序起名字也是编写程序的乐趣中的一部分。我们也有一种使用递归缩写的传统喜好，用这样的方式表明我们所新写的程序与某个现有的程序具有一定的相似性……在为GNU工程起名字时，我试图寻找一个递归缩写来表达“Something Is Not UNIX”含义，我用26个字母去取代“Something”，但得到的结果都差强人意，看起来并不像是一个单词。于是，我决定把Is这个单词采用缩写的形式，这样我就可以得到一个三个字母缩写词，也就是说是“Something’s Not UNIX”。我尝试了各个字母，最终决定选用“GNU”这个词。</p></blockquote><p>﻿&gt; GNU作为一个双关词，斯托曼建议软件用户在发音时，对第一个字母“g”进行发音（也就是说，把它读作“gah-new”）。一方面是为了避免与表示白尾角马这种非州羚羊的单词“gnu”发生混淆，另一方面也是避免与“新（new）”这个形容词发生混淆。“我们已经为GNU工程奋斗了17年，因此它也一点也不‘新’了。”</p><p>总体而言，不推荐，出差的飞机上坚持看完，内容单薄的自传，书中对斯托曼的个人生活和工作细节挖掘的不深，几次访谈 + 公开资料汇总。</p><h2 id="《褚时健自传》"><a href="#《褚时健自传》" class="headerlink" title="《褚时健自传》"></a>《褚时健自传》</h2><p>褚时健传，一览新中国成立、文化大革命、改革开放横跨多个重要的历史节点，人生经历波澜壮阔，认真、奉献、错误、坚韧，再现新中国成立以来那些国有企业伟大领导对整个国家的奉献，肃然起敬。现实版：电视剧《大江大河1 &amp; 2》，甚至精彩程度更盛。整体采用时间叙事方式，探究褚时健个人生活、工作以及时代背景深入分析，值得细品，我很难评价传记人物，他认真、坚韧、奉献的精神非常值得学习，他的人生态度几经变故，依然初心不改，归来仍是少年。</p><p>少年经历日军轰炸滇越铁路，父亲常年靠铁路运输木材为生，一次日军轰炸铁路受伤，1 年后离世，家里唯一经济来源没有了。</p><p>一个 10 几岁的少年，做为家里的老大，需要担负起家庭责任，赚钱养家，放弃学业，学习考酒技术，经营自家半间考酒房，凭借善于思考、勤奋、踏实、认真的做事方式，优化考酒流程，提升酒的质量，自己兜售酒，学着做起了生意，为未来褚时健的个人发展起到很关键的作用。</p><p>关键转折点，堂哥褚时俊专门到家里鼓励他继续完成学业，到昆明念高中，恰逢西南联大南迁昆明，全国各地有学识的人都在昆明任教，文化气息浓厚，影响了那一代很多人，褚时健就是其中之一。</p><p>国内内战，昆明那个时候尚未解放，褚时健和堂弟都加入共产党，隶属本地的武装力量参与解放战争。</p><p>昆明解放，褚时健在玉溪地委宣传部工作，因文化大革命反右运动，划到右派，派往红光农场接受改造。</p><p>1961年，褚时健被任命为新平县畜牧场副场长、曼蚌糖厂副厂长、戛洒糖厂副厂长、文化大革命中，身为厂长的褚时健也遭到批斗。中共十一届三中全会后，褚时健被平反，后被安排至中共云南省委党校学习。</p><p>辉煌起始，领导给了两个选项，一个是在玉溪卷烟厂当厂长，另一个是在塔甸煤矿当党委书记。最终，他选择进入玉溪卷烟厂担任厂长。</p><p>玉烟时期、创办红塔集团、被捕和审判、种橙晚年。</p><p>一个经历如此丰富的人，人生观、价值观等必然有很大的不同。</p><ul><li><p>人活着就要干事情，干事情就要干好。</p></li><li><p>“做一件事，力气花了，要是马马虎虎地做，那力气也就白花了。认认真真地去做，更划算。”</p></li><li><p>活着的每一天，把每件事情做好，尽好自己的每一个责任，就不白白过这一生。不要去想太多死亡的事情，它来或不来，谁也控制不了。</p></li><li><p>有时，你抱着很大的希望去做一件事，失望会很多，往往你看不到希望了，希望好像又一点点地来了。</p></li><li><p>我们是经历过活了今天就没有明天的人，过去如何、将来如何都不重要，现在、目前，就是一辈子。做人的魅力在于，你是否是真实的自己。</p></li><li><p>这事我有谱气，没有谱气的事不干。</p></li><li><p>拈着手的事情就要干好。</p></li></ul><blockquote><p>衡量一个人成功的标准，不是看这个人站在顶峰的时候，而是看这个人从顶峰跌到低谷之后的反弹力。褚时健在最辉煌的时候入狱，在最低谷时创业走向另一个辉煌，这给我最多的不是他入狱时的为之惋惜，也不是他成功后的激动，而是他在70多岁、在狱中、一身病的情况下仍想着创业、想着十年后的情况，这份看待事物的境界、信心、坚强，让我对其产生的是由衷的钦佩与尊敬。</p></blockquote><blockquote><p>面对挫折的态度。褚时健一生大起大落至少三次，经历了民主革命、反右、三年灾害、计划经济等各困难阶段，他没有想过退缩，一心就是想的干事，往前发展。在他60多岁，到了退休年龄，面对企业发展的种种体制限制，他没有想躺在功劳簿上熬岁月，而是“跑断腿、磨破嘴”去一个个克服，争取实现目标。印象深的一段是：果园成功后，很多年轻人都会去拜访褚时健，请教成功的捷径，当时褚夫人对一个年轻人说“坚定目标，努力去做”类似这么简单的话。反衬出我们现在年轻人心浮气躁和褚时健夫妇面对挫折的态度。</p></blockquote><ul><li><a href="https://zh.wikipedia.org/wiki/%E8%A4%9A%E6%97%B6%E5%81%A5" target="_blank" rel="noopener">褚时健wiki</a></li></ul>]]></content>
    
    <summary type="html">
    
      4 月阅读《褚时健自传》、《若为自由故：自由软件之父理查德·斯托曼传》
    
    </summary>
    
      <category term="Books" scheme="http://itweet.github.io/categories/Books/"/>
    
    
      <category term="2021" scheme="http://itweet.github.io/tags/2021/"/>
    
      <category term="reading" scheme="http://itweet.github.io/tags/reading/"/>
    
  </entry>
  
  <entry>
    <title>Apache ozone object store architecture</title>
    <link href="http://itweet.github.io/2021/01/21/apache-ozone-object-store-architecture/"/>
    <id>http://itweet.github.io/2021/01/21/apache-ozone-object-store-architecture/</id>
    <published>2021-01-20T17:10:39.000Z</published>
    <updated>2023-05-27T06:48:46.154Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>Apache Hadoop Ozone 是一个分布式键值存储，可以有效管理大量小文件。Ozone 旨在与现有的 Apache Hadoop 生态系统很好地配合使用，并且满足了对新的开放源代码对象存储的需求，该对象存储旨在简化操作使用并将其扩展到单个集群数千个节点和数十亿个对象。</p><h2 id="2-了解可伸缩性问题"><a href="#2-了解可伸缩性问题" class="headerlink" title="2. 了解可伸缩性问题"></a>2. 了解可伸缩性问题</h2><p>为了将 Ozone 扩展到数十亿个文件，我们需要解决 HDFS 中存在的两个瓶颈。</p><h3 id="2-1-Namespace-可扩展性"><a href="#2-1-Namespace-可扩展性" class="headerlink" title="2.1 Namespace 可扩展性"></a>2.1 Namespace 可扩展性</h3><p>我们不能再将整个 Namespace 存储在单个节点的内存中。关键点是 Namespace 拥有引用位置区域，因此我们可以仅将 working set 存储在内存中。 Namespace 交由称为 Ozone Manager 的服务管理。</p><h3 id="2-2-Block-Map-可扩展性"><a href="#2-2-Block-Map-可扩展性" class="headerlink" title="2.2 Block Map 可扩展性"></a>2.2 Block Map 可扩展性</h3><p>这是一个较难解决的问题。与 namespace 不同，由于存储节点（DataNodes）定期发送有关系统中每个 block 的报告，因此 Block Map 不具有引用位置。Ozone 将这个问题委托给了一个称为 Hadoop 分布式存储（HDDS）的共享通用存储层。</p><h2 id="3-积木（Building-Blocks）"><a href="#3-积木（Building-Blocks）" class="headerlink" title="3. 积木（Building Blocks）"></a>3. 积木（Building Blocks）</h2><p>Ozone 由以下关键部分组成。</p><ul><li><ol><li>使用现成的键值存储（RocksDB）构建的存储容器。</li></ol></li><li><ol start="2"><li>通过 Apache Ratis 的 RAFT 共识协议。RAFT 是一种共识协议，其与 Paxos 相似，但易于理解和实现。Apache Ratis 是 RAFT 的开源 Java 实现，针对高吞吐进行了优化。</li></ol></li><li><ol start="3"><li>存储容器管理（SCM）- 管理复制容器的生命周期。</li></ol></li><li><ol start="4"><li>Hadoop 分布式存储（HDDS）- 不包含 namespace block 信息的通用分布式存储层。</li></ol></li><li><ol start="5"><li>Ozone Manager（OM） - 实现 Ozone 键值 namespace 命名空间管理器。</li></ol></li></ul><h2 id="4-放在一起-（Putting-it-Together）"><a href="#4-放在一起-（Putting-it-Together）" class="headerlink" title="4. 放在一起 （Putting it Together）"></a>4. 放在一起 （Putting it Together）</h2><p><img src="https://ndu0e1pobsf1dobtvj5nls3q-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Legos.png" alt="积木"></p><h3 id="4-1-存储容器"><a href="#4-1-存储容器" class="headerlink" title="4.1 存储容器"></a>4.1 存储容器</h3><p>在最底层，Ozone 将用户数据存储在存储容器中。容器是块名称及其数据的键-值对的集合。键是在容器内唯一的块名称。值是块数据，范围从 0 字节到 256MB。块名称不必是全局唯一的。</p><p><img src="https://ndu0e1pobsf1dobtvj5nls3q-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Screen-Shot-2018-10-11-at-10.03.58-AM.png" alt></p><p>每个容器都支持一些简单的操作：</p><p><img src="https://ndu0e1pobsf1dobtvj5nls3q-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Screen-Shot-2018-10-11-at-10.04.52-AM.png" alt></p><p>容器使用 RocksDB 存储在磁盘上，并针对较大的值进行了一些优化。</p><h3 id="4-2-RAFT-复制"><a href="#4-2-RAFT-复制" class="headerlink" title="4.2 RAFT 复制"></a>4.2 RAFT 复制</h3><p>分布式文件系统必须容忍单个磁盘/节点的丢失，因此我们需要一种通过网络复制容器的方法。为此，我们引入了一些容器的其他属性。</p><ul><li><ol><li>容器是复制的最小单位。因此，其最大大小限制为 5GB （管理员可配置）。</li></ol></li><li><ol start="2"><li>每个容器可以处于以下两种状态之一：打开或关闭。打开的容器可以接受新的键值存储，而关闭的容器是不可变的。</li></ol></li><li><ol start="3"><li><p>每个容器都有三个副本。</p><p>使用 RAFT 复制打开的容器状态更改（写入）。RAFT 是基于仲裁的协议，其中，仲裁的节点对变更进行投票，在给定的时间，单个节点充当领导者并提出更改建议。因此，所有容器操作都可以实时可靠的复制到至少2个法定数量的节点。</p></li></ol></li></ul><p><img src="https://ndu0e1pobsf1dobtvj5nls3q-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Screen-Shot-2018-10-11-at-10.06.53-AM.png" alt></p><p>容器的副本存储在 DataNodes 上。</p><h4 id="4-2-1-容器生命周期"><a href="#4-2-1-容器生命周期" class="headerlink" title="4.2.1 容器生命周期"></a>4.2.1 容器生命周期</h4><p>容器从打开状态开始，客户端写块来打开容器，然后用 commit 操作来最终确定块数据。写一个块有两个阶段：</p><ul><li><ol><li>写入块数据。根据客户端的速度和网络/磁盘带宽，这可能会花费任意长时间。如果客户端在提交该块之前死亡，则 SCM 将自动垃圾收集不完整的数据。</li></ol></li><li><ol start="2"><li>commit 块。此操作从根本上使该块在容器中可见。</li></ol></li></ul><h3 id="4-3-存储容器管理器（SCM）"><a href="#4-3-存储容器管理器（SCM）" class="headerlink" title="4.3 存储容器管理器（SCM）"></a>4.3 存储容器管理器（SCM）</h3><p>现在，我们知道如何将块存储在容器中以及如何通过网络复制容器。下一步是构建一个集中服务，该服务知道所有容器在集群中存储位置，该服务是 SCM。</p><p>SCM 从所有 DataNode 定期获取报告，告知这些节点上的容器副本及其当前状态。SCM 可以选择三个 DataNode 的集合来存储一个新的开放容器，并指导他们相互形成 RAFT 复制环。</p><p>SCM 还会在容器满的时候，指示 Leader 副本 “close” 容器。SCM 还可以检测复制过的关闭容器，并确保每个关闭容器存在三个副本。</p><h3 id="4-4-Containers-RAFT-SCM-HDDS"><a href="#4-4-Containers-RAFT-SCM-HDDS" class="headerlink" title="4.4. Containers + RAFT + SCM = HDDS!"></a>4.4. Containers + RAFT + SCM = HDDS!</h3><p>通过上述三个构建基块，我们拥有创建 HDDS 的所有部分，这是一个没有全局 namespace 的分布式块存储层。</p><p>DataNodes 现在被分成三组，每组形成一个 rats 复制环。每个环可以有多个打开的容器。</p><p>SCM每30秒从每个DataNode接收一次报告，以通知每个节点上打开和关闭的容器副本。根据此报告，SCM做出决策，例如分配新容器，关闭打开的容器，在磁盘丢失/数据丢失时重新复制关闭的容器。</p><p>SCM的客户可以请求分配新块，然后将块数据写入分配的容器中。客户端还可以读取打开/关闭容器中的块并删除块。关键是HDDS本身并不关心单个容器的内容。内容完全由使用SCM的应用程序管理。</p><p><img src="https://ndu0e1pobsf1dobtvj5nls3q-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Screen-Shot-2018-10-11-at-10.08.09-AM.png" alt></p><h3 id="4-5-添加-Namespace-–-Ozone-Manager"><a href="#4-5-添加-Namespace-–-Ozone-Manager" class="headerlink" title="4.5 添加 Namespace – Ozone Manager"></a>4.5 添加 Namespace – Ozone Manager</h3><p>有了 HDDS，唯一缺少的元素就是全局键值名 namespace，这是由 Ozone Manager 提供的，OM是一个从键名到相应的块集合的映射服务。</p><p>客户端可以将多个块写入HDDS，然后提交它们的 key-&gt; blocks 以原子方式映射到 OM，使 key 在 namespace 中可见。</p><p><img src="https://ndu0e1pobsf1dobtvj5nls3q-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Screen-Shot-2018-10-11-at-10.11.26-AM.png" alt></p><p>OM 将自己的状态存储在 RocksDB 数据库中。</p><h2 id="5-HDDS-Beyond-Ozone"><a href="#5-HDDS-Beyond-Ozone" class="headerlink" title="5. HDDS Beyond Ozone"></a>5. HDDS Beyond Ozone</h2><p>HDDS 可由其他分布式文件系统实现用作块存储层，已讨论并可能在未来实施一些示例：</p><ul><li><ol><li>HDDS 上的 HDFS （HDFS-10419） – HDDS 可用于通过替换现有的 HDFS 块管理器来整齐地解决块空间可伸缩性问题。这个想法类似于HDFS-5477提案。</li></ol></li><li><ol start="2"><li>cBlocks （HDFS-11118） – 由 HDDS 存储支持的可装载 iSCSI 卷的原型。</li></ol></li><li><ol start="3"><li>假设对象存储，该存储也将其命名空间存储在 HDDS 容器中。</li></ol></li></ul><p>还有更多…带上自己的名称空间！</p><p>附原文：<a href="https://blog.cloudera.com/apache-hadoop-ozone-object-store-architecture/" target="_blank" rel="noopener">https://blog.cloudera.com/apache-hadoop-ozone-object-store-architecture/</a></p>]]></content>
    
    <summary type="html">
    
      Apache Ozone - 一个基于 raft 的分布式对象存储系统。
    
    </summary>
    
      <category term="6.824" scheme="http://itweet.github.io/categories/6-824/"/>
    
    
      <category term="2021" scheme="http://itweet.github.io/tags/2021/"/>
    
      <category term="ozone" scheme="http://itweet.github.io/tags/ozone/"/>
    
      <category term="apache" scheme="http://itweet.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>CMU 15-445/645 2019 lab4 实现分析</title>
    <link href="http://itweet.github.io/2021/01/09/cmu-15-445-2019-fall-lab4/"/>
    <id>http://itweet.github.io/2021/01/09/cmu-15-445-2019-fall-lab4/</id>
    <published>2021-01-08T23:14:38.000Z</published>
    <updated>2023-05-27T06:59:27.182Z</updated>
    
    <content type="html"><![CDATA[<p>TODO:</p>]]></content>
    
    <summary type="html">
    
      CMU 15-445/645 (FALL 2019) 数据库系统
    
    </summary>
    
      <category term="database" scheme="http://itweet.github.io/categories/database/"/>
    
    
      <category term="2021" scheme="http://itweet.github.io/tags/2021/"/>
    
  </entry>
  
  <entry>
    <title>CMU 15-445/645 2019 lab3 实现分析</title>
    <link href="http://itweet.github.io/2021/01/08/cmu-15-445-2019-fall-lab3/"/>
    <id>http://itweet.github.io/2021/01/08/cmu-15-445-2019-fall-lab3/</id>
    <published>2021-01-07T23:14:38.000Z</published>
    <updated>2023-05-27T06:59:29.364Z</updated>
    
    <content type="html"><![CDATA[<p>TODO:</p>]]></content>
    
    <summary type="html">
    
      CMU 15-445/645 (FALL 2019) 数据库系统
    
    </summary>
    
      <category term="database" scheme="http://itweet.github.io/categories/database/"/>
    
    
      <category term="2021" scheme="http://itweet.github.io/tags/2021/"/>
    
  </entry>
  
  <entry>
    <title>CMU 15-445/645 2019 lab2 实现分析</title>
    <link href="http://itweet.github.io/2021/01/07/cmu-15-445-2019-fall-lab2/"/>
    <id>http://itweet.github.io/2021/01/07/cmu-15-445-2019-fall-lab2/</id>
    <published>2021-01-06T23:14:38.000Z</published>
    <updated>2023-05-27T06:59:31.343Z</updated>
    
    <content type="html"><![CDATA[<p>TODO:</p>]]></content>
    
    <summary type="html">
    
      CMU 15-445/645 (FALL 2019) 数据库系统
    
    </summary>
    
      <category term="database" scheme="http://itweet.github.io/categories/database/"/>
    
    
      <category term="2021" scheme="http://itweet.github.io/tags/2021/"/>
    
  </entry>
  
  <entry>
    <title>CMU 15-445/645 2019 lab1 实现分析</title>
    <link href="http://itweet.github.io/2021/01/06/cmu-15-445-2019-fall-lab1/"/>
    <id>http://itweet.github.io/2021/01/06/cmu-15-445-2019-fall-lab1/</id>
    <published>2021-01-05T23:14:38.000Z</published>
    <updated>2023-05-27T06:59:15.261Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><a href="https://15445.courses.cs.cmu.edu/fall2019/" target="_blank" rel="noopener">CMU 15-445/645 (FALL 2019)</a> 热门的数据库内核开发入门课程，实验部分几乎涵盖一个单机系统所有核心功能，课程非常注重理论与实践结合，讨论系统设计 trade-off 以及实践中是如何在性能，可靠性，稳定性等因素综合考量。</p><p>课程老师 <a href="https://twitter.com/andy_pavlo" target="_blank" rel="noopener">Andy Pavlo</a> 非常强，实战经验丰富。</p><p>2019 课程一共有 4 个 Lab 实验，今天我们讨论 <a href="https://15445.courses.cs.cmu.edu/fall2019/project1/" target="_blank" rel="noopener">PROJECT #1 - BUFFER POOL</a> 中的核心要点以及实践。</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>todo</p><h2 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h2><p>todo</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>todo</p><p>Clock Replacer test 结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ cd build</span><br><span class="line">$ make clock_replacer_test</span><br><span class="line">$ ./test/clock_replacer_test</span><br><span class="line">Running main() from gmock_main.cc</span><br><span class="line">[==========] Running 5 tests from 1 test suite.</span><br><span class="line">[----------] Global test environment set-up.</span><br><span class="line">[----------] 5 tests from ClockReplacerTest</span><br><span class="line">[ RUN      ] ClockReplacerTest.SampleTest1</span><br><span class="line">[       OK ] ClockReplacerTest.SampleTest1 (0 ms)</span><br><span class="line">[ RUN      ] ClockReplacerTest.SampleTest2</span><br><span class="line">[       OK ] ClockReplacerTest.SampleTest2 (24 ms)</span><br><span class="line">[ RUN      ] ClockReplacerTest.Victim</span><br><span class="line">[       OK ] ClockReplacerTest.Victim (18 ms)</span><br><span class="line">[ RUN      ] ClockReplacerTest.Pin</span><br><span class="line">[       OK ] ClockReplacerTest.Pin (9 ms)</span><br><span class="line">[ RUN      ] ClockReplacerTest.Size</span><br><span class="line">[       OK ] ClockReplacerTest.Size (1572 ms)</span><br><span class="line">[----------] 5 tests from ClockReplacerTest (1624 ms total)</span><br><span class="line"></span><br><span class="line">[----------] Global test environment tear-down</span><br><span class="line">[==========] 5 tests from 1 test suite ran. (1624 ms total)</span><br><span class="line">[  PASSED  ] 5 tests.</span><br></pre></td></tr></table></figure><p>Buffer Pool Manager test 结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ cd build</span><br><span class="line">$ make buffer_pool_manager_test</span><br><span class="line">$ ./test/buffer_pool_manager_test </span><br><span class="line">Running main() from gmock_main.cc</span><br><span class="line">[==========] Running 2 tests from 1 test suite.</span><br><span class="line">[----------] Global test environment set-up.</span><br><span class="line">[----------] 2 tests from BufferPoolManagerTest</span><br><span class="line">[ RUN      ] BufferPoolManagerTest.BinaryDataTest</span><br><span class="line">[       OK ] BufferPoolManagerTest.BinaryDataTest (4 ms)</span><br><span class="line">[ RUN      ] BufferPoolManagerTest.SampleTest</span><br><span class="line">[       OK ] BufferPoolManagerTest.SampleTest (2 ms)</span><br><span class="line">[----------] 2 tests from BufferPoolManagerTest (7 ms total)</span><br><span class="line"></span><br><span class="line">[----------] Global test environment tear-down</span><br><span class="line">[==========] 2 tests from 1 test suite ran. (7 ms total)</span><br><span class="line">[  PASSED  ] 2 tests.</span><br></pre></td></tr></table></figure><h2 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h2><ul><li><a href="https://15445.courses.cs.cmu.edu/fall2019/" target="_blank" rel="noopener">CMU 15-445/645 (FALL 2019)</a></li><li><a href="https://15445.courses.cs.cmu.edu/fall2019/assignments.html" target="_blank" rel="noopener">ASSIGNMENTS</a></li></ul>]]></content>
    
    <summary type="html">
    
      CMU 15-445/645 (FALL 2019) 数据库系统
    
    </summary>
    
      <category term="database" scheme="http://itweet.github.io/categories/database/"/>
    
    
      <category term="2021" scheme="http://itweet.github.io/tags/2021/"/>
    
  </entry>
  
  <entry>
    <title>CMU 15-445/645 2019 PROJECT `#4` - LOGGING &amp; RECOVERY 翻译</title>
    <link href="http://itweet.github.io/2021/01/05/cmu-15-445-2019-fall-project4/"/>
    <id>http://itweet.github.io/2021/01/05/cmu-15-445-2019-fall-project4/</id>
    <published>2021-01-04T23:14:38.000Z</published>
    <updated>2023-05-27T07:46:57.427Z</updated>
    
    <content type="html"><![CDATA[<p>TODO:</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ul><li><a href="https://15445.courses.cs.cmu.edu/fall2019/project4/" target="_blank" rel="noopener">PROJECT #4 - LOGGING &amp; RECOVERY</a></li></ul>]]></content>
    
    <summary type="html">
    
      CMU 15-445/645 (FALL 2019) 数据库系统
    
    </summary>
    
      <category term="database" scheme="http://itweet.github.io/categories/database/"/>
    
    
      <category term="2021" scheme="http://itweet.github.io/tags/2021/"/>
    
  </entry>
  
  <entry>
    <title>CMU 15-445/645 2019 PROJECT `#3` - QUERY EXECUTION 翻译</title>
    <link href="http://itweet.github.io/2021/01/04/cmu-15-445-2019-fall-project3/"/>
    <id>http://itweet.github.io/2021/01/04/cmu-15-445-2019-fall-project3/</id>
    <published>2021-01-03T23:14:38.000Z</published>
    <updated>2023-05-27T07:46:50.070Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OVERVIEW"><a href="#OVERVIEW" class="headerlink" title="OVERVIEW"></a>OVERVIEW</h2><p>The third programming project is to add support for executing queries in your database system. You will implement executors that are responsible for taking query plan nodes and executing them. You will create executors that perform sequential scans, inserts, hash joins, and aggregations. Because the DBMS does not support SQL (yet), your implementation will operate directly on hand-written query plans.<br>第三个编程项目是在数据库系统中添加支持执行查询的功能。您将实现执行器，负责接收查询计划节点并执行它们。您将创建执行器来执行顺序扫描、插入、哈希连接和聚合。由于DBMS尚不支持SQL，因此您的实现将直接在手写的查询计划上操作。</p><p>We will use the Iterator query processing model (i.e., the Volcano model). Every query plan executor implements a <code>Next</code> function. When the DBMS invokes an executor’s <code>Next</code> function, the executor either returns a single tuple or indicates that there are no more tuples. This lets the executor implement a loop that just keeps calling <code>Next</code> on its children to get and process their tuples.<br>我们将使用迭代器查询处理模型（即Volcano模型）。每个查询计划执行器都实现了一个Next函数。当DBMS调用执行器的Next函数时，执行器要么返回一个单独的元组，要么指示没有更多的元组。这使得执行器能够实现一个循环，只需不断调用其子节点的Next来获取和处理它们的元组。</p><p>As optional reading, you may be interested in <a href="http://patshaughnessy.net/2014/10/13/following-a-select-statement-through-postgres-internals" target="_blank" rel="noopener">following a select statement through the PostgreSQL internals</a>.<br>作为可选阅读，您可能会对跟踪 PostgreSQL 内部的 select 语句感兴趣。</p><p>The project is comprised of the following tasks:<br>该项目由以下任务组成：</p><ul><li><p>Task #1 - Creating a Catalog Table<br>任务＃1 - 创建目录表</p></li><li><p>Task #2 - Executors<br>任务＃2 - 执行器</p></li><li><p>Task #3 - Linear Probe Hash Table Returns<br>任务＃3 - 线性探测哈希表返回</p></li></ul><p>This is a single-person project that will be completed individually (i.e., no groups).<br>这是一个单人项目，个人完成（即无组）。</p><blockquote><p> Please post all of your questions about this project on Piazza. Do not email the TAs directly with questions. The instructor and TAs will not debug your code. You may verbally describe test cases on Piazza, but do not post an excessive amount of testing code on Piazza either.<br> 请在 Piazza 上发布有关该项目的所有问题。请勿直接通过电子邮件向助教提问。讲师和助教不会调试您的代码。您可以在 Piazza 上口头描述测试用例，但请勿在 Piazza 上发布过多的测试代码。</p></blockquote><p>Release Date: Oct 21, 2019<br>发布日期：2019年10月21日</p><p>Due Date: Nov 17, 2019 @ 11:59pm<br>截止日期：2019年11月17日 @ 11:59pm</p><h2 id="PROJECT-SPECIFICATION"><a href="#PROJECT-SPECIFICATION" class="headerlink" title="PROJECT SPECIFICATION"></a>PROJECT SPECIFICATION</h2><p>项目规范</p><p>Like the previous projects, we are providing you with stub classes that contain the API that you need to implement. You should not modify the signatures for the pre-defined functions in these classes. If you do this, then it will break the test code that we will use to grade your assignment and you will end up getting no credit for the project. If a class already contains certain member variables, you should not remove them. You may however add private helper functions and member variables to these classes.<br>和之前的项目一样，我们提供了包含您需要实现的 API 的存根类。您不应该修改这些类中预定义函数的签名。如果您这样做，将会破坏我们用于评分的测试代码，您将无法得到该项目的任何学分。如果某个类已经包含某些成员变量，您不应该删除它们。但是，您可以向这些类添加私有辅助函数和成员变量。</p><p>The correctness of this project depends on the correctness of your implementation of previous projects, we will not provide solutions or binary files.<br>该项目的正确性取决于您对之前项目的正确实现，我们不会提供解决方案或二进制文件。</p><h3 id="ADVICE-FROM-THE-TAS"><a href="#ADVICE-FROM-THE-TAS" class="headerlink" title="ADVICE FROM THE TAS"></a>ADVICE FROM THE TAS</h3><p>助教建议</p><ul><li><p>Task #1 is much easier than the others. Task #3 is open-ended. Budget your time wisely.<br>任务＃1比其他任务容易得多。任务＃3是开放式的。明智地分配时间。</p></li><li><p>Tasks #1 and #2 do not rely on a working linear probe hash table implementation. Focus on getting those first.<br>任务＃1和＃2不依赖于一个完全工作的线性探测哈希表实现。首先专注于这些任务。</p></li><li><p>Conceptual understanding is important for this project. It will make your life much easier.<br>对于这个项目，概念理解很重要。它将让你的生活变得更容易。</p></li></ul><h3 id="TASK-1-CREATING-A-CATALOG-TABLE"><a href="#TASK-1-CREATING-A-CATALOG-TABLE" class="headerlink" title="TASK #1 - CREATING A CATALOG TABLE"></a>TASK #1 - CREATING A CATALOG TABLE</h3><p>任务＃1 - 创建目录表</p><p>A database maintains an internal catalog to keep track of meta-data about the database. For example, the catalog is used to answer what tables are present and where. For more details, refer back to <a href="https://15445.courses.cs.cmu.edu/fall2019/schedule.html#sep-09-2019" target="_blank" rel="noopener">Lecture #04 - Database Storage (Part II)</a>.<br>数据库维护一个内部目录，用于跟踪关于数据库的元数据。例如，目录用于回答哪些表存在以及它们的位置。有关更多详细信息，请参考Lecture #04 - Database Storage (Part II)。</p><p>In this warm-up task, you will be modifying <code>src/include/catalog/simple_catalog.h</code> to that allow the DBMS to add new tables to the database and retrieve them using either the name or internal object identifier (table_oid_t). You will implement the <code>CreateTable</code>, <code>GetTable(const std::string &amp;table_name)</code>, and <code>GetTable(table_oid_t table_oid)</code> methods. You should find these tasks straightforward, the goal is to familiarize yourself with the catalog and how it interacts with the rest of the system.<br>在这个热身任务中，你将修改src/include/catalog/simple_catalog.h以允许数据库管理系统向数据库添加新表，并使用表名或内部对象标识符（table_oid_t）来检索表。你需要实现CreateTable、GetTable(const std::string &amp;table_name)和GetTable(table_oid_t table_oid)方法。这些任务应该比较简单，目标是让你熟悉目录以及它如何与系统的其他部分交互。</p><h4 id="TESTING"><a href="#TESTING" class="headerlink" title="TESTING"></a>TESTING</h4><p>There is a sample test in <code>test/catalog/catalog_test.cpp</code>.<br>test/catalog/catalog_test.cpp中有一个样例测试。</p><h3 id="TASK-2-EXECUTORS"><a href="#TASK-2-EXECUTORS" class="headerlink" title="TASK #2 - EXECUTORS"></a>TASK #2 - EXECUTORS</h3><p>In the second task, you will implement executors for sequential scans, inserts, hash joins, and aggregations. For each query plan operator type, there is a corresponding executor object that implements the <code>Init</code> and <code>Next</code> methods. The <code>Init</code> method is for setting up internal state about the invocation of the operator (e.g., retrieving the corresponding table to scan). The <code>Next</code> method provides the iterator interface that returns a single tuple on each invocation (or null if there are no more tuples).<br>在第二个任务中，您将实现顺序扫描（sequential scan）、插入（insert）、哈希连接（hash join）和聚合（aggregation）的执行器。对于每个查询计划操作类型，都有相应的执行器对象，实现了Init和Next方法。Init方法用于设置关于操作调用的内部状态（例如，检索要扫描的相应表）。Next方法提供了迭代器接口，每次调用返回一个元组（如果没有更多元组，则返回空）。</p><p>The executors that you will implement are defined in the following header files:<br>您需要实现的执行器定义在以下头文件中：</p><ul><li>src/include/execution/executors/seq_scan_executor.h</li><li>src/include/execution/executors/insert_executor.h</li><li>src/include/execution/executors/hash_join_executor.h</li><li>src/include/execution/executors/aggregation_executor.h</li></ul><p>We assume executors are single-threaded throughout the entire project. You are also free to add private helper functions and class members as you see fit.<br>我们假设执行器在整个项目中都是单线程的。您可以根据需要添加私有辅助函数和类成员。</p><p>To understand how the executors are created at runtime during query execution, refer to the <code>ExecutorFactory</code> (<code>src/include/execution/executor_factory.h</code>) helper class. Moreover, every executor has an <code>ExecutorContext</code> (<code>src/include/execution/executor_context.h</code>) that maintains additional state about the query. Finally, we have provided sample tests for you in <code>test/execution/executor_test.cpp</code>.<br>要了解在查询执行期间如何在运行时创建执行器，请参阅ExecutorFactory（src/include/execution/executor_factory.h）助手类。此外，每个执行器都有一个ExecutorContext（src/include/execution/executor_context.h），用于维护有关查询的其他状态。最后，我们在test/execution/executor_test.cpp中为您提供了示例测试。</p><p>Plan nodes are the input format or blueprint for the executors that you will be implementing. Plan nodes and executors are both tree-like; they accept tuples from their children and give tuples to their parent. They may rely on conventions about the ordering of their children, which we will describe below.<br>查询计划节点是执行器的输入格式或蓝图，您将要实现的执行器和查询计划节点都是树形结构；它们从其子节点接收元组并将元组提供给其父节点。它们可能依赖于关于子节点排序的约定，我们将在下面进行描述。</p><h4 id="SEQUENTIAL-SCANS"><a href="#SEQUENTIAL-SCANS" class="headerlink" title="SEQUENTIAL SCANS"></a>SEQUENTIAL SCANS</h4><p>Sequential scans iterate over a table and return its tuples one-at-a-time. A sequential scan is specified by a <code>SeqScanPlanNode</code>. The plan node specifies which table to iterate over. The plan node may also contain a predicate; if a tuple does not satisfy the predicate, it is skipped over.<br>顺序扫描（Sequential scans）会对表进行迭代，并逐个返回其元组。顺序扫描由SeqScanPlanNode指定。计划节点指定要迭代的表。计划节点还可以包含谓词；如果元组不满足谓词条件，则跳过该元组。</p><p><code>Hint</code>: Be careful when using the <code>TableIterator</code> object. Make sure that you understand the difference between the pre-increment and post-increment operators. You may find yourself getting strange output by switching between <code>++iter</code> and <code>iter++</code>.<br>提示：在使用TableIterator对象时要小心。确保理解前增量运算符和后增量运算符之间的区别。如果在++iter和iter++之间切换，可能会得到奇怪的输出。</p><h4 id="INSERT"><a href="#INSERT" class="headerlink" title="INSERT"></a>INSERT</h4><p>Inserts add tuples to tables. Inserts are specified by an <code>InsertPlanNode</code>. There are two types of inserts: raw inserts, where the values to be inserted are embedded directly inside the plan node itself, or not-raw inserts, which take the values to be inserted from a child executor. For example, you could have an <code>InsertPlanNode</code> whose child was a <code>SeqScanPlanNode</code> to copy one table into another.<br>插入操作（Inserts）将元组添加到表中。插入操作由InsertPlanNode指定。有两种类型的插入操作：原始插入（raw inserts）和非原始插入（not-raw inserts）。原始插入操作中，要插入的值直接嵌入在计划节点本身中，而非原始插入操作则从子执行器中获取要插入的值。例如，您可以将子执行器设为SeqScanPlanNode，以将一个表复制到另一个表中。</p><h4 id="HASH-JOIN"><a href="#HASH-JOIN" class="headerlink" title="HASH JOIN"></a>HASH JOIN</h4><p>Hash joins are used to combine the results of two child executors together. In this project, you just need to implement a basic hash join. For more details on hash joins, refer to the <a href="https://15445.courses.cs.cmu.edu/fall2019/notes/11-joins.pdf" target="_blank" rel="noopener">Lecture #11 notes</a> on join algorithms. In this project, by convention the left child is used to build the hash table and the right child is used to probe.<br>哈希连接（Hash joins）用于将两个子执行器的结果组合在一起。在这个项目中，您只需要实现一个基本的哈希连接操作。关于哈希连接的更多详细信息，请参考Lecture #11 notes上的连接算法。在这个项目中，按照惯例，左子执行器用于构建哈希表，右子执行器用于探测（probe）。</p><p>We are providing you with a <code>SimpleHashJoinHashTable</code> implementation. We strongly recommend that you use this hash table and obtain full credit for this task before proceeding to Task #3.<br>我们为您提供了一个SimpleHashJoinHashTable的实现。我们强烈建议您使用这个哈希表，并在完成这个任务之前获得满分，然后再继续进行第三个任务。</p><p>We have also provided you with a <code>HashValues</code> function, which may be useful.<br>我们还为您提供了一个HashValues函数，可能会有用。</p><p><code>Hint:</code> You will want to make use of the predicate in the hash join plan node. In particular, take a look at <code>AbstractExpression::EvaluateJoin</code>, which handles the left tuple and right tuple and their respective schemas. Note that this returns a <code>Value</code>, which you can <code>GetAs&lt;bool&gt;</code>.<br>提示：您需要使用哈希连接计划节点中的谓词。特别是，请查看AbstractExpression::EvaluateJoin函数，该函数处理左元组、右元组及其各自的模式。请注意，该函数返回一个Value对象，您可以使用GetAs<bool>方法来获取布尔值。</bool></p><h4 id="AGGREGATION"><a href="#AGGREGATION" class="headerlink" title="AGGREGATION"></a>AGGREGATION</h4><p>Aggregations are used to combine multiple tuple results from a single child executor into a single tuple. In this project, we ask you to implement <code>COUNT, SUM, MIN, and MAX</code>.<br>聚合操作用于将来自单个子执行器的多个元组结果合并为单个元组。在这个项目中，我们要求您实现COUNT、SUM、MIN和MAX聚合函数。</p><p>Note that we provide you with a <code>SimpleAggregationHashTable</code>. We strongly recommend that you use this hash table. We have trimmed down the course project and you do not need to do this, but you may be interested in seeing what it takes to use your <code>LinearProbeHashTable</code> from Project #2 instead.<br>请注意，我们为您提供了一个SimpleAggregationHashTable。我们强烈建议您使用这个哈希表。我们简化了课程项目，您不需要使用Project #2中的LinearProbeHashTable，但您可能会有兴趣看看如何使用它。</p><p><code>Hint</code>: You will want to make use of the having expression in the aggregate plan node. In particular, take a look at <code>AbstractExpression::EvaluateAggregate</code>, which handles the groupbys of the key and the aggregates of the value. Note that this returns a <code>Value</code>, which you can <code>GetAs&lt;bool&gt;`</code>.<br>提示：您需要使用聚合计划节点中的HAVING表达式。特别是，请查看AbstractExpression::EvaluateAggregate函数，该函数处理键的分组和值的聚合。请注意，该函数返回一个Value对象，您可以使用GetAs<bool>方法来获取布尔值。</bool></p><h4 id="TESTING-1"><a href="#TESTING-1" class="headerlink" title="TESTING"></a>TESTING</h4><p>We have provided sample tests in <code>test/execution/executor_test.cpp</code>.<br>我们已经在test/execution/executor_test.cpp中提供了示例测试。</p><h3 id="TASK-3-LINEAR-PROBE-HASH-TABLE-RETURNS"><a href="#TASK-3-LINEAR-PROBE-HASH-TABLE-RETURNS" class="headerlink" title="TASK #3 - LINEAR PROBE HASH TABLE RETURNS"></a>TASK #3 - LINEAR PROBE HASH TABLE RETURNS</h3><p>任务＃3 - 线性探测哈希表返回</p><blockquote><p> Intermission: We strongly recommend submitting to Gradescope at this point. This provides a backup of your submission, and you want to make sure that you get full credit for Tasks #1 and #2 before proceeding further.<br> 中场休息：我们强烈建议在此时提交给Gradescope。这将提供您提交的备份，并确保在继续进行之前获得任务＃1和任务＃2的全部学分。</p></blockquote><p>This task will require you to modify your hash join executor. You will now use your Linear Probe Hash Table from <code>Project #2</code> instead of the simplified hash join hash table, henceforth referred to as <code>JHT</code>.<br>这个任务要求您修改哈希连接执行器。您现在将使用项目＃2中的线性探测哈希表，而不是简化的哈希连接哈希表，以后将称为JHT。</p><p><em>Note</em>: Task #3.1 can be skipped entirely. As long as you receive full credit for Task #3.2, you will also automatically receive full credit for Task #3.1.<br>注意：任务＃3.1可以完全跳过。只要您获得了任务＃3.2的全部学分，您也将自动获得任务＃3.1的全部学分。</p><p>It is likely that you will need to understand <a href="https://en.cppreference.com/w/cpp/language/class_template#Explicit_instantiation" target="_blank" rel="noopener">explicit template instantiation</a>. You have already seen code that used explicit template instantiation in <a href="https://15445.courses.cs.cmu.edu/fall2019/project2" target="_blank" rel="noopener">Project #2</a>.<br>您可能需要理解显式模板实例化。您已经在项目＃2中看到了使用显式模板实例化的代码。</p><h4 id="TASK-3-1-LINEAR-PROBE-HASH-TABLE-TMPTUPLEPAGE"><a href="#TASK-3-1-LINEAR-PROBE-HASH-TABLE-TMPTUPLEPAGE" class="headerlink" title="TASK #3.1 - LINEAR PROBE HASH TABLE, TMPTUPLEPAGE"></a>TASK #3.1 - LINEAR PROBE HASH TABLE, TMPTUPLEPAGE</h4><p>任务＃3.1 - 线性探测哈希表，TmpTuplePage</p><p>This part of the project is intended to be open-ended. Again, receiving full credit for Task #3.2 will automatically give you full credit for Task #3.1.<br>这部分项目旨在开放性。同样，只要您获得了任务＃3.2的全部学分，您将自动获得任务＃3.1的全部学分。</p><p>You are provided, but are not required to use, the following stub classes:<br>我们为您提供了以下存根类，但不要求您使用：</p><ul><li><p>TmpTuplePage at <code>src/include/storage/page/tmp_tuple_page.h</code>. In our implementation, we wanted a simplified version of a TablePage. Although this is not necessary, we found that this will make the development easier.<br>TmpTuplePage位于src/include/storage/page/tmp_tuple_page.h。在我们的实现中，我们需要一个简化版本的TablePage。虽然这不是必需的，但我们发现这将使开发工作更容易。</p></li><li><p>TmpTuple at src/include/storage/table/tmp_tuple.h. In our implementation, this solved certain issues that come from instantiating LinearProbeHashTable. You should think carefully about what those issues are.<br>TmpTuple位于src/include/storage/table/tmp_tuple.h。在我们的实现中，这解决了一些由实例化LinearProbeHashTable引起的问题。您应该仔细考虑这些问题是什么。</p></li></ul><p>To provide more scaffolding to students who prefer a more structured project, implementing missing functionality in <code>TmpTuplePage</code> will earn you full credit for Task #3.1. However, this may constrain you to working towards our solution. You are also free to implement the missing <code>TmpTuplePage</code> functionality and not use any of it.<br>对于希望进行更结构化项目的学生，实现TmpTuplePage中缺失的功能将为您赢得任务＃3.1的全部学分。然而，这可能会限制您朝我们的解决方案发展。您也可以实现缺失的TmpTuplePage功能，但不使用其中的任何部分。</p><h4 id="TASK-3-2-LINEAR-PROBE-HASH-TABLE-HASH-JOIN"><a href="#TASK-3-2-LINEAR-PROBE-HASH-TABLE-HASH-JOIN" class="headerlink" title="TASK #3.2 - LINEAR PROBE HASH TABLE, HASH JOIN"></a>TASK #3.2 - LINEAR PROBE HASH TABLE, HASH JOIN</h4><p>任务＃3.2 - 线性探测哈希表，哈希连接</p><p>Lastly, you will modify your hash join executor in <code>src/include/execution/executors/hash_join_executor.h</code> to use your hash table from <code>Project #2</code>. You may find <code>TmpTuplePage</code> useful, but there are other solutions. As long as the type of your hash table used in the hash join is a <code>LinearProbeHashTable</code>, you will obtain full credit.<br>最后，您将修改src/include/execution/executors/hash_join_executor.h中的哈希连接执行器，以使用您在“项目＃2”中的哈希表。您可能会发现TmpTuplePage有用，但也有其他解决方案。只要在哈希连接中使用的哈希表类型是LinearProbeHashTable，您将获得全部学分。</p><p><em>Note:</em> Depending on how you choose to use the <code>LinearProbeHashTable</code>, you may need to remove the uniqueness constraint from it. Feel free to do so. In our solution involving <code>TmpTuplePage</code> and <code>TmpTuple</code>, we did not need to.<br><em>注意</em>：根据您选择如何使用LinearProbeHashTable，您可能需要从中删除唯一性约束。请随意这样做。在我们涉及TmpTuplePage和TmpTuple的解决方案中，我们不需要这样做。</p><p>You may also add functions to <code>src/include/execution/executor_context.h</code> if you find that helpful.<br>如果发现有用，您还可以将函数添加到src/include/execution/executor_context.h中。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ul><li><a href="https://15445.courses.cs.cmu.edu/fall2019/project3/" target="_blank" rel="noopener">PROJECT #3 - QUERY EXECUTION</a></li></ul>]]></content>
    
    <summary type="html">
    
      CMU 15-445/645 (FALL 2019) 数据库系统
    
    </summary>
    
      <category term="database" scheme="http://itweet.github.io/categories/database/"/>
    
    
      <category term="2021" scheme="http://itweet.github.io/tags/2021/"/>
    
  </entry>
  
  <entry>
    <title>CMU 15-445/645 2019 fall PROJECT `#2` - HASH TABLE 翻译</title>
    <link href="http://itweet.github.io/2021/01/03/cmu-15-445-2019-fall-project2/"/>
    <id>http://itweet.github.io/2021/01/03/cmu-15-445-2019-fall-project2/</id>
    <published>2021-01-02T23:14:38.000Z</published>
    <updated>2023-05-27T07:34:26.010Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OVERVIEW"><a href="#OVERVIEW" class="headerlink" title="OVERVIEW"></a>OVERVIEW</h2><p>概述</p><p>The second programming project is to implement a hash table for the BusTub DBMS that is backed by disk storage. Your hash table is responsible for fast data retrieval without having to search through every record in a database table.<br>第二个编程项目是为BusTub DBMS实现一个支持磁盘存储的哈希表。你的哈希表负责快速检索数据，而无需在数据库表中搜索每条记录。</p><p>You will need to implement a hash table using the linear probing hashing scheme. It is a giant slot array that spans across multiple pages where each block page holds a hash table block. The table will access pages through your buffer pool from Project #1. The table contains a header page to map blocks to pages. Your hash table needs to support growing in size if it runs out of space (i.e., every slot is full).<br>你需要使用线性探测哈希方案实现一个哈希表。它是一个跨越多个页的巨大槽位数组，每个块页都保存一个哈希表块。该表将通过项目#1中的缓冲池从页面访问。该表包含一个头页面来映射块到页面。如果哈希表空间不足（即每个槽位都已满），你的哈希表需要支持扩展大小。</p><p>You will need to complete the following tasks in your hash table implementation:<br>你需要在哈希表实现中完成以下任务：</p><ul><li><p>Page Layouts<br>页面布局</p></li><li><p>Hash Table Implementation<br>哈希表实现</p></li><li><p>Table Resizing<br>表大小调整</p></li><li><p>Concurrency Control<br>并发控制</p></li></ul><p>This is a single-person project that will be completed individually (i.e., no groups).<br>这是一个单人项目，将单独完成（即无小组）。</p><p>Release Date: Sep 30, 2019<br>发布日期：2019年9月30日</p><p>Due Date: Oct 20, 2019 @ 11:59pm<br>截止日期：2019年10月20日 @ 11:59pm</p><h2 id="PROJECT-SPECIFICATION"><a href="#PROJECT-SPECIFICATION" class="headerlink" title="PROJECT SPECIFICATION"></a>PROJECT SPECIFICATION</h2><p>项目规范</p><p>Like the first project, we are providing you with stub classes that contain the API that you need to implement. You should not modify the signatures for the pre-defined functions in these classes. If you do this, then it will break the test code that we will use to grade your assignment you end up getting no credit for the project. If a class already contains certain member variables, you should not remove them. But you may add private helper functions/member variables to these classes in order to correctly realize the functionality.<br>与第一个项目一样，我们为你提供了包含需要实现的API的存根类。你不应修改这些类中预定义函数的签名。如果这样做，将会破坏我们用来评分的测试代码，最终导致你无法获得项目的学分。如果一个类已经包含某些成员变量，你不应该删除它们。但是，你可以在这些类中添加私有辅助functions/member变量以正确实现功能。</p><p>The correctness of the linear probe hash table index depends on the correctness of your buffer pool implementation. We will not provide solutions for the previous programming projects.<br>线性探测哈希表索引的正确性取决于缓冲池实现的正确性。我们不会为之前的编程项目提供解决方案。</p><h3 id="TASK-1-PAGE-LAYOUTS"><a href="#TASK-1-PAGE-LAYOUTS" class="headerlink" title="TASK #1 - PAGE LAYOUTS"></a>TASK #1 - PAGE LAYOUTS</h3><p>任务＃1 - 页面布局</p><p>Your hash table is meant to be accessed through the DBMS’s BufferPoolManager. This means that you cannot allocate memory to store information. Everything must be stored in disk pages so that they can read/written from the DiskManager. If you create a hash table, write its pages to disk, and then restart the DBMS, you should be able to load back the hash table from disk after restarting.<br>你的哈希表是通过DBMS的BufferPoolManager访问的。这意味着你不能分配内存来存储信息。所有内容必须存储在磁盘页中，以便可以通过DiskManager进行读取/写入。如果你创建了一个哈希表，将其页面写入磁盘，然后重新启动DBMS，重新启动后应该能够从磁盘加载回哈希表。</p><p>To support reading/writing hash table blocks on top of pages, you will implement two Page classes to store the data of your hash table. This is meant to teach you how to allocate memory from the BufferPoolManager as pages.<br>为了支持在页面上读取/写入哈希表块，你将实现两个页面类来存储哈希表的数据。这旨在教你如何从BufferPoolManager中分配内存作为页面。</p><ul><li><p>Hash Table Header Page<br>Hash Table Header 页面</p></li><li><p>Hash Table Block Page<br>Hash Table Block 页面</p></li></ul><h4 id="HASH-TABLE-HEADER-PAGE"><a href="#HASH-TABLE-HEADER-PAGE" class="headerlink" title="HASH TABLE HEADER PAGE"></a><code>HASH TABLE HEADER PAGE</code></h4><p><code>HASH TABLE HEADER 页面</code></p><p>This class holds all of the meta-data for the hash table. It is divided into the fields as shown by the table below:<br>这个类保存哈希表的所有元数据。它被划分为下表所示的字段：</p><table><thead><tr><th>变量名称</th><th>大小</th><th>描述</th></tr></thead><tbody><tr><td>page_id_</td><td>4字节</td><td>自身页面ID</td></tr><tr><td>size_</td><td>4字节</td><td>哈希表可以容纳的键值对数量</td></tr><tr><td>next_ind_</td><td>4字节</td><td>添加新条目到block_page_ids_的下一个索引</td></tr><tr><td>lsn_</td><td>4字节</td><td>日志序列号（在第四个项目中使用）</td></tr><tr><td>block_page_ids_</td><td>4080字节</td><td>块页面ID的数组</td></tr></tbody></table><p>The block_page_ids_ array maps block ids to page_id_t ids. The ith element in block_page_ids_ is the page_id for the ith block.<br>block_page_ids_数组将块ID映射到page_id_t ID。block_page_ids_中的第i个元素是第i个块的page_id。</p><p>You must implement your Hash Table Header Page in the designated files. You are only allowed to modify the header file (src/include/page/hash_table_header_page.h) and its corresponding source file (src/page/hash_table_header_page.cpp).<br>你必须在指定的文件中实现你的哈希表头页面。你只能修改头文件（src/include/page/hash_table_header_page.h）及其相应的源文件（src/page/hash_table_header_page.cpp）。</p><h4 id="HASH-TABLE-BLOCK-PAGE"><a href="#HASH-TABLE-BLOCK-PAGE" class="headerlink" title="HASH TABLE BLOCK PAGE"></a>HASH TABLE BLOCK PAGE</h4><p>The Hash Table Block Page holds three arrays:<br>哈希表块页面包含三个数组：</p><ul><li>occupied_ : The ith bit of occupied_ is 1 if the ith index of array_ has ever been occupied.</li><li><p>occupied_：如果array_的第i个索引曾经被占用，则occupied_的第i个位为1。</p></li><li><p>readable_ : The ith bit of readable_ is 1 if the ith index of array_ holds a readable value.</p></li><li><p>readable_：如果array_的第i个索引持有可读的值，则readable_的第i个位为1。</p></li><li><p>array_ : The array that holds the key-value pairs.</p></li><li>array_：保存键值对的数组。</li></ul><p>The number of slots available in a Hash Table Block Page depends on the types of the keys and values being stored. You only need to support fixed-length keys and values. The size of keys/values will be the same within a single hash table instance, but you cannot assume that they will be the same for all instances (e.g., hash table #1 can have 32-bit keys and hash table #2 can have 64-bit keys).<br>哈希表块页面中可用的槽位数量取决于存储的键和值的类型。你只需要支持固定长度的键和值。在单个哈希表实例内，键/值的大小将相同，但你不能假设它们对于所有实例都相同（例如，哈希表＃1可以具有32位键，哈希表＃2可以具有64位键）。</p><p>You must implement your Hash Table Block Page in the designated files. You are only allowed to modify the header file (src/include/page/hash_table_block_page.h) and its corresponding source file (src/page/hash_table_block_page.cpp).<br>你必须在指定的文件中实现你的哈希表块页面。你只能修改头文件（src/include/page/hash_table_block_page.h）及其相应的源文件（src/page/hash_table_block_page.cpp）。</p><p>Each Hash Table Header/Block page corresponds to the content (i.e., the byte array data_) of a memory page fetched by buffer pool. Every time you try to read or write a page, you need to first fetch the page from buffer pool using its unique page_id, then reinterpret cast to either a header or a block page, and unpin the page after any writing or reading operations.<br>每个哈希表 Header/Block 页面对应于通过缓冲池获取的内存页面的内容（即字节数组data_）。每当你尝试读取或写入页面时，你需要使用其唯一的page_id</p><h3 id="TASK-2-HASH-TABLE-IMPLEMENTATION"><a href="#TASK-2-HASH-TABLE-IMPLEMENTATION" class="headerlink" title="TASK #2 - HASH TABLE IMPLEMENTATION"></a>TASK #2 - HASH TABLE IMPLEMENTATION</h3><p>任务 #2 - 哈希表实现</p><p>You will implement a hash table that uses the linear probing hashing scheme. It needs to support insertions (Insert), point search (GetValue), and deletions (Remove).<br>你需要实现一个使用线性探测哈希方案的哈希表。它需要支持插入（Insert）、点查询（GetValue）和删除（Remove）操作。</p><p>Your hash table must support both unique and non-unique keys. Duplicate values for the same key are not allowed. This means that (key_0, value_0) and (key_0, value_1) can exist in the same hash table, but not (key_0, value_0) and (key_0, value_0). The Insert method only returns false if it tries to insert an existing key-value pair.<br>你的哈希表必须支持唯一键和非唯一键。同一个键的重复值是不允许的。这意味着在同一个哈希表中可以存在 (key_0, value_0) 和 (key_0, value_1) 这样的键值对，但不能存在 (key_0, value_0) 和 (key_0, value_0) 这样的情况。Insert 方法只有在尝试插入已存在的键值对时才会返回 false。</p><p>Your hash table implementation must hide the details of the key/value type and associated comparator, like this:<br>你的哈希表实现必须隐藏key/value类型和相关比较器的细节，如下所示：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> KeyType,</span><br><span class="line">          <span class="keyword">typename</span> ValueType,</span><br><span class="line">          <span class="keyword">typename</span> KeyComparator&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearProbeHashTable</span> &#123;</span></span><br><span class="line">   <span class="comment">// ---</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>These classes are already implemented for you:<br>以下类已经为你实现：</p><ul><li><p><code>KeyType:</code> The type of each key in the hash table. This will only be GenericKey, the actual size of GenericKey is specified and instantiated with a template argument and depends on the data type of indexed attribute.<br>KeyType：哈希表中每个键的类型。这将只是 GenericKey，GenericKey 的实际大小由模板参数指定并实例化，取决于索引属性的数据类型。</p></li><li><p><code>ValueType:</code> The type of each value in the hash table. This will only be 64-bit RID.<br>ValueType：哈希表中每个值的类型。这将只是 64 位 RID。</p></li><li><p><code>KeyComparator:</code> The class used to compare whether two KeyType instances are less/greater-than each other. These will be included in the KeyType implementation files.<br>Note that, to compare whether two ValueType instances are equal to each other, you can use the == operator.<br>KeyComparator：用于比较两个 KeyType 实例是否小于/大于彼此的类。这些将包含在 KeyType 实现文件中。<br>请注意，要比较两个 ValueType 实例是否相等，可以使用 == 运算符。</p></li></ul><h3 id="TASK-3-TABLE-RESIZING"><a href="#TASK-3-TABLE-RESIZING" class="headerlink" title="TASK #3 - TABLE RESIZING"></a>TASK #3 - TABLE RESIZING</h3><p>任务 #3 - 表格调整大小</p><p>The linear probing hashing scheme uses a fized-size table. When the hash table is full, then any insert operation will get stuck in an infinite loop because the system will walk through the entire slot array and not find a free space. If your hash table detects that it is full, then it must resize itself to be twice the current size (i.e., if currently has n slots, then the new size will be 2×n).<br>线性探测哈希方案使用固定大小的表格。当哈希表已满时，任何插入操作都将陷入无限循环，因为系统将遍历整个槽位数组并找不到空闲空间。如果哈希表检测到已满，那么它必须调整自身的大小为当前大小的两倍（即，如果当前有n个槽位，则新的大小将是2×n）。</p><p>Since any write operation could lead to a change of header_page_id in your hash table, it is your responsibility to update header_page_id in the header page (src/include/page/header_page.h) to ensure that the container is durable on disk.<br>由于任何写操作都可能导致哈希表中 header_page_id 的变化，所以你有责任更新头页（src/include/page/header_page.h）中的 header_page_id，以确保容器在磁盘上是持久的。</p><h3 id="TASK-4-CONCURRENCY-CONTROL"><a href="#TASK-4-CONCURRENCY-CONTROL" class="headerlink" title="TASK #4 - CONCURRENCY CONTROL"></a>TASK #4 - CONCURRENCY CONTROL</h3><p>任务 #4 - 并发控制</p><p>Up to this this point you could assume that your hash table only supported single-threaded execution. In this last task, you will modify your implementation so that it supports multiple threads reading/writing the table at the same time.<br>到目前为止，你可以假设你的哈希表仅支持单线程执行。在最后一个任务中，你将修改你的实现，使其支持多个线程同时读写表格。</p><p>You will need to have latches on each block so that when one thread is writing to a block other threads are not reading or modifying that index as well. You should also allow multiple readers to be reading the same block at the same time.<br>你需要在每个块上设置闩锁，以便当一个线程正在向一个块写入时，其他线程不能同时读取或修改该索引。你还应该允许多个读取器同时读取同一个块。</p><p>You will need to latch the whole hash table when you need to resize. When resize is called, the size that the table was when resize was called is passed in as an argument. This is so that if the table was resized while a thread was waiting for the latch, it can immediately give up the latch and attempt insertion again.<br>当需要调整大小时，你需要给整个哈希表上锁。在调用调整大小时，传入调整大小时表格的大小作为参数。这样，如果在一个线程等待闩锁时表格被调整大小，它可以立即释放闩锁并再次尝试插入操作。</p><h3 id="REQUIREMENTS-AND-HINTS"><a href="#REQUIREMENTS-AND-HINTS" class="headerlink" title="REQUIREMENTS AND HINTS"></a>REQUIREMENTS AND HINTS</h3><p>要求和提示</p><ul><li><p>You are not allowed to use a global scope latch to protect your data structure for each operation. In other words, you may not lock the whole container and only unlock the latch when operations are done.<br>不允许使用全局范围的闩锁来保护你的数据结构。换句话说，你不能锁定整个容器，只有在操作完成时才释放闩锁。</p></li><li><p>We are providing you a ReaderWriterLatch (src/include/common/rwlatch.h) that you can use in your hash table. There are also helper functions in the Page base class to acquire and release latches (src/include/page/page.h).<br>我们提供了一个 ReaderWriterLatch (src/include/common/rwlatch.h) 可以在你的哈希表中使用。在 Page 基类中也有一些辅助函数可以获取和释放闩锁 (src/include/page/page.h)。</p></li><li><p>Sone of your hash table functions will be given a Transaction (src/include/concurrency/transaction.h) object. This object provides methods to store the page on which you have acquired latch while traversing through the Hash Table.<br>你的一些哈希表函数将获得一个 Transaction (src/include/concurrency/transaction.h) 对象。这个对象提供了在遍历哈希表过程中存储已获得闩锁页面的方法。</p></li></ul><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ul><li><a href="https://15445.courses.cs.cmu.edu/fall2019/project2/" target="_blank" rel="noopener">PROJECT #2 - HASH TABLE</a></li></ul>]]></content>
    
    <summary type="html">
    
      CMU 15-445/645 (FALL 2019) project1
    
    </summary>
    
      <category term="database" scheme="http://itweet.github.io/categories/database/"/>
    
    
      <category term="2021" scheme="http://itweet.github.io/tags/2021/"/>
    
  </entry>
  
  <entry>
    <title>CMU 15-445/645 2019 PROJECT `#1` - BUFFER POOL 翻译</title>
    <link href="http://itweet.github.io/2021/01/02/cmu-15-445-2019-fall-project1/"/>
    <id>http://itweet.github.io/2021/01/02/cmu-15-445-2019-fall-project1/</id>
    <published>2021-01-01T23:14:38.000Z</published>
    <updated>2023-05-27T07:34:57.970Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OVERVIEW"><a href="#OVERVIEW" class="headerlink" title="OVERVIEW"></a>OVERVIEW</h2><p>概览</p><p>During the semester, you will be building a new disk-oriented storage manager for the BusTub DBMS. Such a storage manager assumes that the primary storage location of the database is on disk.<br>在本学期中，您将构建一个面向磁盘的存储管理器，用于 BusTub 数据库管理系统。这样的存储管理器假设数据库的主要存储位置位于磁盘上。</p><p>The first programming project is to implement a buffer pool in your storage manager. The buffer pool is responsible for moving physical pages back and forth from main memory to disk. It allows a DBMS to support databases that are larger than the amount of memory that is available to the system. The buffer pool’s operations are transparent to other parts in the system. For example, the system asks the buffer pool for a page using its unique identifier (<code>page_id_t</code>) and it does not know whether that page is already in memory or whether the system has to go retrieve it from disk.<br>第一个编程项目是在您的存储管理器中实现一个缓冲池。缓冲池负责在主存储器和磁盘之间移动物理页面。它使得 DBMS 能够支持比系统可用内存更大的数据库。缓冲池的操作对系统中的其他部分是透明的。例如，系统使用唯一标识符（page_id_t）向缓冲池请求页面，并不知道该页面是否已经在内存中，或者系统是否需要从磁盘中检索它。</p><p>Your implementation will need to be thread-safe. Multiple threads will be accessing the internal data structures at the same and thus you need to make sure that their critical sections are protected with latches (these are called “locks” in operating systems).<br>您的实现需要是线程安全的。多个线程将同时访问内部数据结构，因此您需要确保它们的临界区受到锁（在操作系统中称为 “锁”）的保护。</p><p>You will need to implement the following two components in your storage manager:<br>您需要在存储管理器中实现以下两个组件：</p><ul><li><p>Clock Replacement Policy<br>时钟替换策略（Clock Replacement Policy）</p></li><li><p>Buffer Pool Manager<br>缓冲池管理器（Buffer Pool Manager）</p></li></ul><p>All of the code in this programming assignment must be written in C++ (specifically C++17). It is generally sufficient to know C++11. If you have not used C++ before, here is a short tutorial on the language. More detailed documentation of language internals is available on cppreference. A Tour of C++ and Effective Modern C++ are also digitally available from the CMU library.<br>本编程任务中的所有代码必须使用 C++ 编写（具体来说是 C++17）。通常了解 C++11 就足够了。如果您之前没有使用过 C++，这里有一个关于该语言的简短教程。更详细的语言内部文档可以在 cppreference 上找到。《C++ 编程之旅》和《现代 C++ 实用效果》也可以在 CMU 图书馆的数字库中找到。</p><p>There are many tutorials and walkthroughs available to teach you how to use gdb effectively. Here are some that we have found useful:<br>有很多教程和实例可以教您如何有效使用 gdb 进行调试。以下是一些我们认为有用的教程：</p><ul><li><p>Debugging Under Unix: gdb Tutorial<br>在 Unix 下调试：gdb 教程</p></li><li><p>GDB Tutorial: Advanced Debugging Tips For C/C++ Programmers<br>GDB 教程：C/C++ 程序员的高级调试技巧</p></li><li><p>Give me 15 minutes &amp; I’ll change your view of GDB [VIDEO]<br>给我 15 分钟，我将改变您对 GDB 的看法 [视频]</p></li></ul><p>This is a single-person project that will be completed individually (i.e. no groups).<br>这是一个个人项目，将单独完成（即无小组）。</p><p>Release Date: Sep 11, 2019<br>发布日期：2019年9月11日</p><p>Due Date: Sep 27, 2019 @ 11:59pm<br>截止日期：2019年9月27日晚上11:59</p><h2 id="PROJECT-SPECIFICATION"><a href="#PROJECT-SPECIFICATION" class="headerlink" title="PROJECT SPECIFICATION"></a>PROJECT SPECIFICATION</h2><p>For each of the following components, we are providing you with stub classes that contain the API that you need to implement. You should not modify the signatures for the pre-defined functions in these classes. If you modify the signatures, the test code that we use for grading will break and you will get no credit for the project. You also should not add additional classes in the source code for these components. These components should be entirely self-contained.<br>对于以下每个组件，我们提供了包含您需要实现的 API 的存根类。您不应修改这些类中预定义函数的签名。如果您修改了签名，我们用于评分的测试代码将出错，您将无法得分。您也不应在源代码中为这些组件添加额外的类。这些组件应该是完全独立的。</p><p>If a class already contains data members, you should not remove them. For example, the <code>BufferPoolManager</code> contains <code>DiskManager</code> and <code>Replacer</code> objects. These are required to implement the functionality that is needed by the rest of the system. On the other hand, you may need to add data members to these classes in order to correctly implement the required functionality. You can also add additional helper functions to these classes. The choice is yours.<br>如果一个类已经包含数据成员，您不应将它们删除。例如，BufferPoolManager 包含 DiskManager 和 Replacer 对象。这些对象在实现系统的其余功能所需的功能方面是必需的。另一方面，您可能需要向这些类添加数据成员，以正确实现所需的功能。您还可以向这些类添加额外的辅助函数。选择权在您手中。</p><p>You are allowed to use any built-in C++17 containers in your project unless specified otherwise. It is up to you to decide which ones you want to use. Note that these containers are not thread-safe and that you will need to include latches in your implementation to protect them. You may not bring in additional third-party dependencies (e.g. boost).<br>您可以在项目中使用任何内置的 C++17 容器，除非另有规定。您可以自行决定要使用哪些容器。请注意，这些容器不是线程安全的，您需要在实现中加入保护它们的锁。您不得引入额外的第三方依赖项（例如 boost）。</p><h3 id="TASK-1-CLOCK-REPLACEMENT-POLICY"><a href="#TASK-1-CLOCK-REPLACEMENT-POLICY" class="headerlink" title="TASK #1 - CLOCK REPLACEMENT POLICY"></a>TASK #1 - CLOCK REPLACEMENT POLICY</h3><p>任务 #1 - CLOCK REPLACEMENT POLICY</p><p>This component is responsible for tracking page usage in the buffer pool. You will implement a new sub-class called <code>ClockReplacer</code> in <code>src/include/buffer/clock_replacer.h</code> and its corresponding implementation file in <code>src/buffer/clock_replacer.cpp</code>. ClockReplacer extends the abstract Replacer class (<code>src/include/buffer/replacer.h</code>), which contains the function specifications.<br>该组件负责在缓冲池中跟踪页面的使用情况。您需要在 src/include/buffer/clock_replacer.h 中实现一个名为 ClockReplacer 的新子类，并在 src/buffer/clock_replacer.cpp 中编写相应的实现文件。ClockReplacer 继承了抽象类 Replacer（src/include/buffer/replacer.h），其中包含函数规范。</p><p>The size of the <code>ClockReplacer</code> is the same as buffer pool since it contains placeholders for all of the frames in the <code>BufferPoolManager</code>. However, not all the frames are considered as in the <code>ClockReplacer</code>. The <code>ClockReplacer</code> is initialized to have no frame in it. Then, only the newly unpinned ones will be considered in the <code>ClockReplacer</code>. Adding a frame to or removing a frame from a replacer is implemented by changing a reference bit of a frame. The clock hand initially points to the placeholder of frame 0. For each frame, you need to track two things: 1. Is this frame currently in the <code>ClockReplacer</code>? 2. Has this frame recently been unpinned (ref flag)?<br>ClockReplacer 的大小与缓冲池相同，因为它包含了缓冲池中所有帧的占位符。然而，并不是所有的帧都被视为在 ClockReplacer 中。ClockReplacer 初始化时没有帧在其中。然后，只有新取消固定的帧才会被视为在 ClockReplacer 中。向替换器添加帧或从替换器中删除帧是通过改变帧的引用位来实现的。时钟指针最初指向帧0的占位符。对于每个帧，您需要跟踪两个信息：1. 此帧当前是否在 ClockReplacer 中？2. 此帧是否最近取消固定（引用标志）？</p><p>In some scenarios, the two are the same. For example, when you unpin a page, both of the above are true. However, the frame stays in the ClockReplacer until it is pinned or victimized, but its ref flag is modified by the clock hand.<br>在某些情况下，这两个条件是相同的。例如，当您取消固定页面时，上述两个条件都为真。但是，该帧会一直保留在 ClockReplacer 中，直到它被固定或作为牺牲者，但其引用标志会受到时钟指针的修改。</p><p>You will need to implement the clock policy discussed in the class. You will need to implement the following methods:<br>您需要实现在课堂上讨论的时钟置换策略。您需要实现以下方法：</p><ul><li><p>Victim(T<em>) : Starting from the current position of clock hand, find the first frame that is both in the <code>ClockReplacer</code> and with its ref flag set to false. If a frame is in the <code>ClockReplacer</code>, but its ref flag is set to true, change it to false instead. This should be the only method that updates the clock hand.<br>Victim(T</em>)：从时钟指针的当前位置开始，找到第一个既在 ClockReplacer 中且其引用标志设置为 false 的帧。如果帧在 ClockReplacer 中，但其引用标志设置为 true，则将其改为 false。这应该是唯一更新时钟指针的方法。</p></li><li><p>Pin(T) : This method should be called after a page is pinned to a frame in the BufferPoolManager. It should remove the frame containing the pinned page from the ClockReplacer.<br>Pin(T)：在将页面固定到 BufferPoolManager 中的帧后，应调用此方法。它应从 ClockReplacer 中删除包含固定页面的帧。</p></li><li><p>Unpin(T) : This method should be called when the pin_count of a page becomes 0. This method should add the frame containing the unpinned page to the ClockReplacer.<br>Unpin(T)：当页面的 pin_count 变为 0 时，应调用此方法。此方法应将包含取消固定页面的帧添加到 ClockReplacer 中。</p></li><li><p>Size() : This method returns the number of frames that are currently in the ClockReplacer.<br>Size()：该方法返回当前在 ClockReplacer 中的帧数。</p></li></ul><p>The implementation details are up to you. You are allowed to use built-in STL containers. You can assume that you will not run out of memory, but you must make sure that the operations are thread-safe.<br>具体的实现细节由您决定。您可以使用内置的 STL 容器。您可以假设不会耗尽内存，但必须确保操作是线程安全的。</p><h3 id="TASK-2-BUFFER-POOL-MANAGER"><a href="#TASK-2-BUFFER-POOL-MANAGER" class="headerlink" title="TASK #2 - BUFFER POOL MANAGER"></a>TASK #2 - BUFFER POOL MANAGER</h3><p>Next, you need to implement the buffer pool manager in your system (BufferPoolManager). The BufferPoolManager is responsible for fetching database pages from the DiskManager and storing them in memory. The BufferPoolManager can also write dirty pages out to disk when it is either explicitly instructed to do so or when it needs to evict a page to make space for a new page.<br>接下来，您需要在系统中实现缓冲池管理器（BufferPoolManager）。缓冲池管理器负责从磁盘管理器获取数据库页面并将其存储在内存中。当缓冲池管理器被明确指示或需要清除页面以为新页面腾出空间时，它还可以将脏页面写回磁盘。</p><p>To make sure that your implementation works correctly with the rest of the system, we will provide you with some of the functions already filled in. You will also not need to implement the code that actually reads and writes data to disk (this is called the DiskManager in our implementation). We will provide that functionality for you.<br>为确保您的实现与系统的其余部分正确配合，我们将为您提供一些已填充的函数。您也不需要实现实际读写数据到磁盘的代码（在我们的实现中称为磁盘管理器）。我们将为您提供该功能。</p><p>All in-memory pages in the system are represented by Page objects. The <code>BufferPoolManager</code> does not need to understand the contents of these pages. But it is important for you as the system developer to understand that Page objects are just containers for memory in the buffer pool and thus are not specific to a unique page. That is, each Page object contains a block of memory that the <code>DiskManager</code> will use as a location to copy the contents of a physical page that it reads from disk. The <code>BufferPoolManager</code> will reuse the same Page object to store data as it moves back and forth to disk. This means that the same Page object may contain a different physical page throughout the life of the system. The Page object’s identifer (page_id) keeps track of what physical page it contains; if a Page object does not contain a physical page, then its page_id must be set to INVALID_PAGE_ID.<br>系统中的所有内存页面都由Page对象表示。缓冲池管理器不需要了解这些页面的内容。但是，作为系统开发人员，理解Page对象只是缓冲池中内存的容器非常重要，因此不特定于唯一页面。也就是说，每个Page对象都包含一个内存块，磁盘管理器将使用该内存块作为从磁盘读取的物理页面内容的位置。缓冲池管理器将重复使用同一Page对象在内存和磁盘之间存储数据。这意味着同一个Page对象在系统的生命周期内可能包含不同的物理页面。Page对象的标识符（page_id）跟踪其所包含的物理页面；如果一个Page对象不包含物理页面，则其page_id必须设置为INVALID_PAGE_ID。</p><p>Each Page object also maintains a counter for the number of threads that have “pinned” that page. Your <code>BufferPoolManager</code> is not allowed to free a Page that is pinned. Each Page object also keeps track of whether it is dirty or not. It is your job to record whether a page was modified before it is unpinned. Your <code>BufferPoolManager</code> must write the contents of a dirty Page back to disk before that object can be reused.<br>每个Page对象还维护一个计数器，用于统计已“固定”该页面的线程数量。您的缓冲池管理器不允许释放已固定的Page。每个Page对象还会跟踪其是否为脏页。您的任务是在取消固定之前记录页面是否已修改。在可以重用该对象之前，您的缓冲池管理器必须将脏页的内容写回磁盘。</p><p>Your BufferPoolManager implementation will use the ClockReplacer class that you created in the previous steps of this assignment. It will use the ClockReplacer to keep track of when Page objects are accessed so that it can decide which one to evict when it must free a frame to make room for copying a new physical page from disk.<br>您的BufferPoolManager实现将使用您在之前步骤中创建的ClockReplacer类。它将使用ClockReplacer跟踪Page对象的访问情况，以便在必须释放帧以为新的物理页面从磁盘复制时，可以决定要驱逐的页面。</p><p>You will need to implement the following functions defined in the header file (src/include/buffer/buffer_pool_manager.h) in the source file (src/buffer/buffer_pool_manager.cpp):<br>您需要在头文件（src/include/buffer/buffer_pool_manager.h）和源文件（src/buffer/buffer_pool_manager.cpp）中实现以下函数：</p><ul><li><p>FetchPageImpl(page_id)</p></li><li><p>NewPageImpl(page_id)</p></li><li><p>UnpinPageImpl(page_id, is_dirty)</p></li><li><p>FlushPageImpl(page_id)</p></li><li><p>DeletePageImpl(page_id)</p></li><li><p>FlushAllPagesImpl()</p></li></ul><p>For FetchPageImpl,you should return NULL if no page is available in the free list and all other pages are currently pinned. FlushPageImpl should flush a page regardless of its pin status.<br>对于FetchPageImpl，如果在空闲列表中没有可用页面且所有其他页面当前都被固定，您应返回NULL。FlushPageImpl应无论页面的固定状态如何都将其刷新。</p><p>Refer to the function documentation for details on how to implement these functions. Don’t touch the non-impl versions, we need those to grade your code.<br>有关如何实现这些函数的详细信息，请参阅函数文档。不要触碰非impl版本，我们需要这些版本来评估您的代码。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ul><li><a href="https://15445.courses.cs.cmu.edu/fall2019/project1/" target="_blank" rel="noopener">PROJECT #1 - BUFFER POOL</a></li></ul>]]></content>
    
    <summary type="html">
    
      CMU 15-445/645 (FALL 2019) 数据库系统
    
    </summary>
    
      <category term="database" scheme="http://itweet.github.io/categories/database/"/>
    
    
      <category term="2021" scheme="http://itweet.github.io/tags/2021/"/>
    
  </entry>
  
  <entry>
    <title>Lecture2-rpc-and-threads</title>
    <link href="http://itweet.github.io/2020/03/22/lecture2-rpc-and-threads/"/>
    <id>http://itweet.github.io/2020/03/22/lecture2-rpc-and-threads/</id>
    <published>2020-03-21T16:47:06.000Z</published>
    <updated>2023-05-27T06:48:46.154Z</updated>
    
    <content type="html"><![CDATA[<p>golang - 大肆夸赞一番</p><ul><li><p>I/O concurrency</p></li><li><p>CPU Parallelism</p></li><li><p>Thread Challenges</p></li><li><p>多线程爬虫 Demo</p><ul><li>sync.Mutex </li><li>lock</li><li>thread</li><li>Goroutine</li></ul></li><li><p>primary-backup replication</p></li><li><p>state transfer</p><ul><li>memrory: 内存状态的转移到另外机器，实现起来比较简单</li></ul></li><li><p>replication state machine</p><ul><li>OP: 操作状态转移，相对复杂，需要考虑很多种情况</li><li>what state？</li><li>VMware Fault Tolerance (FT) </li></ul></li><li><p>如何解决脑裂？</p><ul><li>网络分区导致脑裂？</li><li>主从副本 primary crash 导致的脑裂？</li></ul></li></ul><ul><li>MapReduce: Simplified Data Processing on Large Clusters</li><li>The Google File System</li><li>The Design of a Practical System for Fault-Tolerant Virtual Machines</li></ul>]]></content>
    
    <summary type="html">
    
      MIT 6.824 - Distributed Systems (Spring 2020)
    
    </summary>
    
      <category term="6.824" scheme="http://itweet.github.io/categories/6-824/"/>
    
    
      <category term="spring" scheme="http://itweet.github.io/tags/spring/"/>
    
      <category term="6.824" scheme="http://itweet.github.io/tags/6-824/"/>
    
      <category term="2020" scheme="http://itweet.github.io/tags/2020/"/>
    
      <category term="mit" scheme="http://itweet.github.io/tags/mit/"/>
    
  </entry>
  
  <entry>
    <title>Lecture1-interoduction</title>
    <link href="http://itweet.github.io/2020/03/22/lecture1-interoduction/"/>
    <id>http://itweet.github.io/2020/03/22/lecture1-interoduction/</id>
    <published>2020-03-21T16:46:50.000Z</published>
    <updated>2020-05-05T15:31:58.314Z</updated>
    
    <content type="html"><![CDATA[<p>MIT 6.824: Distributed Systems Spring，涉及到分布式系统相关知识，能学到不少干货，我个人之前看过设计数据密集型应用一书，很多内容和思想较为全面，课程中涉及到的基本都是核心知识点，课程中的 4 个 labs 是真的值得花时间设计和实现一下，对学习分布式一定会有很大收获。我会慢慢跟进，把过程中有意思的部分整理成文​发到博客中。</p><p>What is 6.824 about?</p><p>6.824 is a core 12-unit graduate subject with lectures, readings, programming labs, an optional project, a mid-term exam, and a final exam. It will present abstractions and implementation techniques for engineering distributed systems. Major topics include fault tolerance, replication, and consistency. Much of the class consists of studying and discussing case studies of distributed systems.</p><p>Prerequisites: 6.004 and one of 6.033 or 6.828, or equivalent. Substantial programming experience will be helpful for the lab assignments. </p><h2 id="Lecture-interoduction"><a href="#Lecture-interoduction" class="headerlink" title="Lecture interoduction"></a>Lecture interoduction</h2><h3 id="Quick-Preview"><a href="#Quick-Preview" class="headerlink" title="Quick Preview"></a>Quick Preview</h3><ul><li>parrlelism</li><li>fault-tolerant</li><li>physical</li><li><p>secvrits / isolated</p></li><li><p>Challenges</p><ul><li>challenges</li><li>partial failure</li><li>performance</li></ul></li><li><p>Course Structure</p><ul><li>lectures</li><li>papers - 每周一篇 paper<ul><li>比如：mapreduce paper</li></ul></li><li>examples</li><li>labs  <ul><li>lab1 - MapReduce</li><li>lab2 - Raft fault-tolerant</li><li>lab3 - k/v server</li><li>lab4 - sharded k/v server</li></ul></li><li>project (optional)</li></ul></li><li><p>Infrastructure abstractions</p><ul><li>Storage</li><li>Comms </li><li>computer</li></ul></li><li><p>Impl</p><ul><li>RPC、Threads、consistency</li></ul></li><li><p>Performance</p><ul><li>scalability - 2X computers -&gt; 2X throughput</li></ul></li><li><p>fault-tolerant</p><ul><li>availability              |   Non-volatile storage</li><li>recoverability            |   Replication</li></ul></li><li><p>Topic - consistency</p><ul><li>Put(k,v)</li><li>Get(K) -&gt; v</li><li>examples</li></ul></li></ul><p><img src="https://www.itweet.cn/screenshots/6.824/dist-kv.png" alt="dist kv"><br>图，分布式 kv 一致性，思考如何在分布式加多数据副本情况下，变更数据保障不同 client 读取到的数据是一致的，有什么成熟的算法？</p><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>因为没有必要让工程师花费所有的时间来编写分布式系统软件，也要有大量的面向普通业务的开发工作，因此，这些不是很熟悉分布式系统的普通工程师确实需要某种框架可以让他们轻松大胆地编写他们想进行的任何分析工作，例如：排序算法、Web索引或链接分析器等，并在写这些应用程序时无所畏惧，同时无需担心它是否能在数以千计的计算机上运行的诸多细节，比如：如果将这些工作分散到这些计算机上、如何组织所需任何数据的移动、如果应对不可避免的故障。因此寻找和创造一种框架使得非专业人员也能轻松开发和运行大型分布式计算系统，已处理海量的数据，正是 MapReduce 全部意义所在。</p><p><img src="https://www.itweet.cn/screenshots/6.824/mapreduce-principle.png" alt="mapreduce principle"></p><ul><li>Map(k,v)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">split v into words   # K is filename, ignore</span><br><span class="line"></span><br><span class="line">for each word w</span><br><span class="line">    emit(w, &quot;1&quot;)</span><br></pre></td></tr></table></figure><ul><li>Reduce(k,v)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emit(len(v))</span><br></pre></td></tr></table></figure><p>reference：<a href="http://www.itweet.cn/2018/04/23/mapreduce-osdi04/" target="_blank" rel="noopener">http://www.itweet.cn/2018/04/23/mapreduce-osdi04/</a></p><h3 id="homework-lab1-mapreduce"><a href="#homework-lab1-mapreduce" class="headerlink" title="homework: lab1 mapreduce"></a>homework: lab1 mapreduce</h3><ul><li>Reference：<ul><li>6.824 - Spring 2020: <a href="https://pdos.csail.mit.edu/6.824/" target="_blank" rel="noopener">https://pdos.csail.mit.edu/6.824/</a></li><li>6.824 Lab 1: MapReduce: <a href="https://pdos.csail.mit.edu/6.824/labs/lab-mr.html" target="_blank" rel="noopener">https://pdos.csail.mit.edu/6.824/labs/lab-mr.html</a></li><li>MapReduce: Simplified Data Processing on Large Clusters</li><li>video：<a href="https://www.youtube.com/watch?v=cQP8WApzIQQ" target="_blank" rel="noopener">https://www.youtube.com/watch?v=cQP8WApzIQQ</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      MIT 6.824 - Distributed Systems (Spring 2020).
    
    </summary>
    
      <category term="6.824" scheme="http://itweet.github.io/categories/6-824/"/>
    
    
      <category term="spring" scheme="http://itweet.github.io/tags/spring/"/>
    
      <category term="6.824" scheme="http://itweet.github.io/tags/6-824/"/>
    
      <category term="2020" scheme="http://itweet.github.io/tags/2020/"/>
    
      <category term="mit" scheme="http://itweet.github.io/tags/mit/"/>
    
  </entry>
  
  <entry>
    <title>Spark guava 库引发的系列问题</title>
    <link href="http://itweet.github.io/2019/12/24/spark-guava-library-problem/"/>
    <id>http://itweet.github.io/2019/12/24/spark-guava-library-problem/</id>
    <published>2019-12-23T19:43:38.000Z</published>
    <updated>2019-12-25T14:39:06.180Z</updated>
    
    <content type="html"><![CDATA[<p>最近在扩展 spark structured streaming 功能的时候，遇到几个疑难问题，今天一并记录一下，方便以后翻旧帐。。。</p><p>一切都要从 maven-shade-plugin 插件开始说起，我在扩展 spark structured streaming 功能时，需要充分考虑未来的可扩展性和易用性，最好能沉淀出一个小框架，呈现为一个 lib 库，基于这个库能够快速的扩展新的数据源。</p><p>首先，我先调研了一下现有需要支持的数据源，总共 4+，基本都是在公司内部大规模使用的分布式数据存储系统。现有需要实现，数据源需要支持 read -&gt; process -&gt; write 功能。中间 process 需要充分考虑自定义处理逻辑的能力。优先支持流式 API 功能，其次支持批量 API 功能。</p><p>经过一番调研和思考，我抽象出 source，process，sink，JobConf 三种接口以及若干方法。source 和 sink 对应 4+ 数据源，每种数据源都会有一个基本的数据对象，支持对象进行读写数据源，读写时支持基本的谓词下推，每种数据源根据存储规则，进行最大化的并行 scan。Process 接口支持基本的数据处理转换方法，基于 spark 的 dataset 进行封装。JobConf 实现配置化数据源信息的自动加载和多方数据来源的 merge，JobConf 实现了类似 SpringBoot 的自动化配置功能，只需要把支持的数据源的相关配置到 <code>xxx-dev.propergies</code>，根据默认配置 job.profiles.active 指定 dev 则默认去 resources 中载入其中的带 <code>xxx.</code> 开头的全部配置，在具体的 API 代码中 new 相关的 source &amp; sink 对象则自动加载相应配置，实现任务的提交和计算。整体打包为一个 shade jar 包，运行时通过 <code>-Djob.config.location = /path/xxx-prod.propergies</code> 在外部实现动态加载配置。</p><p>我主要从以下几个方面考虑：</p><p>易用性：提交时只需要一个 jar 包和一个外部的配置文件，进行提交任务并计算。</p><p>可扩展性：抽象出一个基本的框架，简称：A 模块。新增数据源，只需要依赖基础的 A 模块，配置文件中配置相关 source，sink 配置信息。然后，编写 Process 的业务转换逻辑，利用 JobConf 进行本地调试和上线，能降低大部分的调试功能。</p><p>快速入门：A 模块，支持的数据源都提供相互读写、数据缓缓的测试用例，代码级指导，快速扩展新的面相特定业务的模块功能。提供详细的 README 指导如何在 IDEA 和 线上集群发起任务。</p><p>进入正题，在使用我提供的库扩展新功能时，主要遇到如下两个问题：</p><p>问题1: <code>java.lang.ClassNotFoundException: kafka.DefaultSource</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">19/12/23 13:34:54 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: java.lang.ClassNotFoundException: Failed to find data source: kafka. Please find packages at http://spark.apache.org/third-party-projects.html</span><br><span class="line">at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:639)</span><br><span class="line">at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:159)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:721)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: kafka.DefaultSource</span><br><span class="line">at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:622)</span><br><span class="line">at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:622)</span><br><span class="line">at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:622)</span><br><span class="line">at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:622)</span><br><span class="line">at scala.util.Try.orElse(Try.scala:84)</span><br><span class="line">at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:622)</span><br><span class="line">... 11 more</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>我确定，相关 spark structured streaming 依赖的包都已经打进去了，提交任务依然报错。经过一番 Debug，发现时因为我们自己扩展的 spark datasource 代码中有一个文件 <code>resources/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister</code>, 而 Kafka 的官方扩展里面也有一个名称一样的文件：</p><ul><li>kafka-0-10-sql： org.apache.spark.sql.kafka010.KafkaSourceProvider</li><li>xxx-engine-xxx: org.apache.spark.sql.xxx.xxxSourceProvider</li></ul><p>因为打包优先级的问题，造成我们的文件覆盖了 Kafka 的文件内容，造成提交到集群时一直报如上错误。Spark 就是依靠此文件中的内容，动态的加载相关的 class 文件，进而实现动态加载自定义数据源的功能。</p><p><strong>我们如何解决此问题？</strong></p><p>方案一：</p><p>在 <code>resources/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister</code> 文件中，加入 Kafka 以及自己扩展的几个 xxxSourceProvider，可解决问题。</p><p>方案二：</p><p>使用 maven assembly 和 shade 插件进行同名 service 文件内容合并。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;transformers&gt;</span><br><span class="line">    &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;/&gt;</span><br><span class="line">&lt;/transformers&gt;</span><br></pre></td></tr></table></figure><p>方案三：</p><p>大型工程中，一般避免生成 <code>uber-jar</code> 包，springboot 就是一个例子，网络上面充斥着 springboot 各种 jar 依赖冲突问题。模块划分清晰，生成的 jar 只有自己写的代码，依赖的第三方 lib 统一放到一个文件夹中，写一个脚本去控制启动进程，或发起一个任务。build 直接生成一个tag.gz，自带 jar、配置文件、启动脚本。线上直接解压修改配置就能跑。</p><p>问题2: IDEA 运行的非常成功，打成统一的 <code>uber-jar</code> 到线上提交到集群报错：java.lang.NoSuchMethodError: com.google.common.hash.Hashing.farmHashFingerprint64()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.NoSuchMethodError: com.google.common.hash.Hashing.farmHashFingerprint64()Lcom/google/common/hash/HashFunction;</span><br><span class="line">at com.aliyun.openservices.aliyun.log.producer.internals.Utils.generateProducerHash(Utils.java:31)</span><br><span class="line">at com.aliyun.openservices.aliyun.log.producer.LogProducer.&lt;init&gt;(LogProducer.java:91)</span><br></pre></td></tr></table></figure><p>遇到此坑的人不在少数：</p><ul><li><p>SparkR：<a href="https://sparkr.atlassian.net/browse/SPARKR-211" target="_blank" rel="noopener">https://sparkr.atlassian.net/browse/SPARKR-211</a></p></li><li><p>Hive：<a href="https://issues.apache.org/jira/browse/HIVE-7387" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HIVE-7387</a></p></li></ul><p>经过一番排查发现我们依赖的一个模块使用 guava 版本 22.0，而 spark 集群自带 14.0，导致冲突，而无法正常工作。做为运行在 spark 集群上的任务，spark 加载 guava 包优先级高于我们自己的包。</p><p>我们依赖的包使用到 guava 版本 22.0 中比较新的方法，而在 14.0 版本还没有这样的方法。在不能修改对方代码的前提下，有如下方案：</p><ul><li>spark 集群的包升级一下，风险较高，容易造成未知问题。</li><li>另外一种方式是利用 maven 插件重命名自己的 guava 包。</li></ul><p>利用 maven 插件 shade 重命名包解决问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;relocations&gt;</span><br><span class="line">    &lt;relocation&gt;</span><br><span class="line">        &lt;pattern&gt;com.google.common&lt;/pattern&gt;</span><br><span class="line">        &lt;shadedPattern&gt;com.xxx.xxx.shaded.com.google.common&lt;/shadedPattern&gt;</span><br><span class="line">    &lt;/relocation&gt;</span><br><span class="line">&lt;/relocations&gt;</span><br></pre></td></tr></table></figure><p>如何减少此类问题，依赖大型工程，需要考虑自己项目引入包的版本是否和大型工程依赖的包有冲突，尽量使用下载量最大的版本。避免使用到一些新特性，而造成上面问题。</p>]]></content>
    
    <summary type="html">
    
      扩展 spark 支持各种新的数据源读写功能，由于 guava 冲突造成的系列问题。
    
    </summary>
    
      <category term="JDP" scheme="http://itweet.github.io/categories/JDP/"/>
    
    
      <category term="Spark" scheme="http://itweet.github.io/tags/Spark/"/>
    
      <category term="2019" scheme="http://itweet.github.io/tags/2019/"/>
    
  </entry>
  
  <entry>
    <title>Debugging the build process in fusiondb website</title>
    <link href="http://itweet.github.io/2019/09/25/Debugging-the-build-process-in-fusiondb-website/"/>
    <id>http://itweet.github.io/2019/09/25/Debugging-the-build-process-in-fusiondb-website/</id>
    <published>2019-09-25T07:46:20.000Z</published>
    <updated>2019-12-25T14:39:06.180Z</updated>
    
    <content type="html"><![CDATA[<p>曾经在构建 <code>Core Data &amp; Core AI</code> 统一分析平台 <a href="http://www.fusionlab.cn/zh-cn/" target="_blank" rel="noopener"><code>JDP</code></a> 时，就面临一个编写文档的窘境，系统功能构建太快，早期由于没有规划和约束导致大量的功能并未编写文档以及遵守良好的编程风格，导致最后查无所依，软件维护成本高昂。与一些公开的公益性质的开源持续集成服务也没用很好的结合，造成大量的工作依赖手工完成，甚至自己组装服务器在家进行各种压测以及持续集成的工作。</p><p>如上一些原因，我就在思考如何才能在软件开发初期较早的版本就开始规避类似的问题呢？</p><p>首先，我们完全遵守国际化的标注进行软件的开发工作，利用 Git 和 Github 提供的相关软件开发流程管理工具，标准化整个 PR 的提交流程，参见: <a href="https://github.com/FusionDB/fusiondb.github.io/pull/2" target="_blank" rel="noopener">Build an official website</a>。</p><ul><li>可信赖、可持续发展的软件系统，文档和代码一定同等重要</li><li>Issues 或 Jira 管理全部的软件 Feature 开发，做到追溯，查有所依</li><li>Pull requet 经过严格的 Review/QA</li><li>每个 Pull requet 做到功能的独立性，不掺杂任何无关的代码提交，清晰的区分 bugfix、feature、hotfix</li><li>模块众多，逻辑划分清晰，做到每个 PR 带有相应模块标识</li><li>每个新功能的开发或者大的变更都应编写相关设计与实现文档进行集体 review</li><li>每个新功能或者已有功能的变更，都需要详细记录在相关的设计文档中</li><li>每增加一个功能，需要相应的编写用户使用文档，进行详细的阐述新功能的使用方法，带有示例代码</li></ul><p>有点跑偏了，我们今天主要讨论，构建一个可信赖的 Open Source 项目的官方网站。需要涉及哪些内容，有哪些可以信赖的开源服务，帮助我们快速达成目标。</p><ul><li>首先，确定官方网站一定是静态的 HTML 页面，需要方便随时修改并发布</li><li>文档项目主流都是基于 Markdown 语法，故需要能对 markdown 渲染成静态 HTML</li><li>需要能对接持续集成工具自动化的 Build 并发布到线上<ul><li>需要支持全文检索文档功能，方便用户使用，调研一圈选定：Algolia index 服务</li><li>Algolia index 使用，按照官方文档的方式嵌入到每次 Build 的流程自动 Push 到此服务</li></ul></li><li>集成 google analytics &amp; tagmanager 服务方便分析网站的整个访问指标<ul><li>按照 google analytics &amp; tagmanager 服务集成文档说明</li><li>注册账号申请相应的使用 ID，在 fusiondb website 文档项目配置文件配置</li></ul></li><li>SEO 优化，比如：官方网站关键词的选择，搜索引擎友好，带来流量提升</li></ul><p>接下来，关于内容的构建，极为关键。文档的质量和内容直接决定未来用户基数。为了解决一开始我提出的一些问题，我单独构建出一个 Design 栏目，追踪整个项目的设计理念以及每个 Feature 的详细设计，包括调研、实现、设计的权衡。Apache 一些比较成熟的顶级项目基本都有详设的文档，比如: </p><ul><li><p><a href="https://docs.google.com/document/d/1zLFiA1VuaWeVxeTDXNg8bL6GP3BVoOZBkewFtEnjEoo/edit#" target="_blank" rel="noopener">SPIP: Spark API for Table Metadata</a></p></li><li><p><a href="https://docs.google.com/document/d/1yrKXEIRATfxHJJ0K3t6wUgXAtZq8D-XgvEnvl2uUcr0/edit#heading=h.oqvjdqvo5bkh" target="_blank" rel="noopener">[DESIGN] Ground Source Sink Concepts in Flink</a></p></li></ul><p>在 Google Docs 上直接多人协作和 Review，一起修订，在社区发起广泛会议讨论，投票提案阶段，提案通过，进入研发排期。</p><p>我终于要进入正题啦。。。</p><p>感慨大前端好复杂，N 种构建工具、前后端分离框架、微服务真是够复杂的，曾经就简单写过 JQuery、JavaScript、HTML、CSS。现在的大前端真是复杂多了，各种新概念词。</p><p>为调试一个遇到的 Bug，3 个多小时没搞定，专门请教了专业人士，最终才解决问题。</p><p>Bug 有些诡异：<code>yarn start</code> 渲染的静态页面没问题，<code>yarn build &amp;&amp; yarn serve</code> 出来的界面有问题。初步定位 <code>build</code> 和 <code>start</code> 执行的命令不一样，多次调试无果，最后发现是较低级错误，在代码中判断 <code>development</code> 和 <code>production</code> 执行的代码不一致，导致某个页面渲染获取到了空值，页面渲染异常。</p><p>我们来学习下，如何进行大前端 Debug 吧，适合初学小白。。。</p><p>项目主要是 Node.js 为运行环境，可以利用 Node.js 提供的 debug 工具进行调试。</p><p>我们主要介绍使用以下几种方法进行代码的调试：</p><ul><li>VS Code debugger (Manual-Config)</li><li>Chrome DevTools for Node</li></ul><h2 id="VS-Code-debugger-Manual-Config"><a href="#VS-Code-debugger-Manual-Config" class="headerlink" title="VS Code debugger (Manual-Config)"></a>VS Code debugger (Manual-Config)</h2><p>在编辑器中进行调试将会非常方便，可以跳过在 Chrome DevTools 相关的繁琐配置。可以将断点设置在你代码的具体某一行，并且能直观的看到相应的上下文信息。</p><p>我在这里不会过多解释 VS-Code 如何进行调试，学习 <a href="https://code.visualstudio.com/docs/editor/debugging" target="_blank" rel="noopener">VS Code debug 技巧</a>。我们仅共享一下 Gatsby.js 相关的 vs-code debug 配置。</p><p>关于 <code>.vscode/launch.json</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;version&quot;: &quot;0.2.0&quot;,</span><br><span class="line">  &quot;configurations&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;Gatsby develop&quot;,</span><br><span class="line">      &quot;type&quot;: &quot;node&quot;,</span><br><span class="line">      &quot;request&quot;: &quot;launch&quot;,</span><br><span class="line">      &quot;protocol&quot;: &quot;inspector&quot;,</span><br><span class="line">      &quot;program&quot;: &quot;$&#123;workspaceRoot&#125;/node_modules/gatsby/dist/bin/gatsby&quot;,</span><br><span class="line">      &quot;args&quot;: [&quot;develop&quot;],</span><br><span class="line">      &quot;stopOnEntry&quot;: false,</span><br><span class="line">      &quot;runtimeArgs&quot;: [&quot;--nolazy&quot;],</span><br><span class="line">      &quot;sourceMaps&quot;: false</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;Gatsby build&quot;,</span><br><span class="line">      &quot;type&quot;: &quot;node&quot;,</span><br><span class="line">      &quot;request&quot;: &quot;launch&quot;,</span><br><span class="line">      &quot;protocol&quot;: &quot;inspector&quot;,</span><br><span class="line">      &quot;program&quot;: &quot;$&#123;workspaceRoot&#125;/node_modules/gatsby/dist/bin/gatsby&quot;,</span><br><span class="line">      &quot;args&quot;: [&quot;build&quot;],</span><br><span class="line">      &quot;stopOnEntry&quot;: false,</span><br><span class="line">      &quot;runtimeArgs&quot;: [&quot;--nolazy&quot;],</span><br><span class="line">      &quot;sourceMaps&quot;: false</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 gatsby-node.js 中设置断点并使用 VS Code 中的 Start debugging 命令后，您可以看到最终结果：</p><p><img src="https://www.itweet.cn/screenshots/vscode-debug.png" alt></p><blockquote><p>注意: 如果断点没有在 <code>const value = createFilePath({ node, getNode })</code> 生效，你可以运行 <code>gatsby clean</code> 命令删除 <code>.cache</code> 和 <code>public</code> 目录，然后重试。</p></blockquote><h2 id="Chrome-DevTools-for-Node"><a href="#Chrome-DevTools-for-Node" class="headerlink" title="Chrome DevTools for Node"></a>Chrome DevTools for Node</h2><h3 id="Running-Gatsby-with-the-inspect-flag"><a href="#Running-Gatsby-with-the-inspect-flag" class="headerlink" title="Running Gatsby with the inspect flag"></a>Running Gatsby with the inspect flag</h3><p>在项目目录中运行如下命令，而不是 <code>npm run develop</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node --inspect-brk --no-lazy node_modules/gatsby/dist/bin/gatsby develop</span><br></pre></td></tr></table></figure><ul><li>–inspect-brk will enable Node’s inspector agent which will allow you to connect a debugger. It will also pause execution until the debugger is connected and then wait for you to resume it.</li><li>–no-lazy - 这将迫使 Node 的 V8 引擎禁用延迟编译，便于设置断点。</li></ul><h3 id="Connecting-DevTools"><a href="#Connecting-DevTools" class="headerlink" title="Connecting DevTools"></a>Connecting DevTools</h3><p>在 Chrome 浏览器中键入 <code>chrome://inspect</code>，通过单击 <code>inspect</code> 连接到远程目标。</p><p><img src="https://www.itweet.cn/screenshots/chrome-devtools-inspect.png" alt></p><p>你应该能看到 Chrome DevTools 启动，代码执行在 <code>gatsby.js</code> 入口文件的开始处暂停。</p><p><img src="https://www.itweet.cn/screenshots/chrome-devtools-init.png" alt></p><h3 id="设置-Sources-Code"><a href="#设置-Sources-Code" class="headerlink" title="设置 Sources Code"></a>设置 <code>Sources Code</code></h3><p>在浏览器中可以选择添加一个本地的目录到 <code>Sources</code>。</p><h3 id="Using-DevTools"><a href="#Using-DevTools" class="headerlink" title="Using DevTools"></a>Using DevTools</h3><p>添加本地项目到 <code>Sources</code> 中，就可以在浏览器的代码页面单机代码行号设置断点，进行愉快的调试了。</p><h2 id="完"><a href="#完" class="headerlink" title="完"></a>完</h2><p>总体来看在 IDE 中进行代码调试是非常方便的，而使用 Chrome DevTools 并不是那么的方便。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://nodejs.org/en/docs/guides/debugging-getting-started/" target="_blank" rel="noopener">debugging-getting-started</a></li><li><a href="https://www.youtube.com/watch?v=Xb_0awoShR8" target="_blank" rel="noopener">Debugging with Node.js - Paul Irish talk at Node Summit 2017</a></li><li><a href="https://github.com/gatsbyjs/gatsby/blob/master/docs/docs/debugging-the-build-process.md" target="_blank" rel="noopener">debugging-the-build-process</a></li></ul>]]></content>
    
    <summary type="html">
    
      利用大前端技术构建 FusionDB 纯静态的官方网站，遇到的一些小坑，纯粹记录一下备忘。
    
    </summary>
    
      <category term="Web Design" scheme="http://itweet.github.io/categories/Web-Design/"/>
    
    
      <category term="2019" scheme="http://itweet.github.io/tags/2019/"/>
    
      <category term="Debug" scheme="http://itweet.github.io/tags/Debug/"/>
    
  </entry>
  
  <entry>
    <title>How to quickly deploy flink on jdp platform</title>
    <link href="http://itweet.github.io/2019/07/30/how-to-quickly-deploy-flink-on-jdp-platform/"/>
    <id>http://itweet.github.io/2019/07/30/how-to-quickly-deploy-flink-on-jdp-platform/</id>
    <published>2019-07-30T08:20:53.000Z</published>
    <updated>2019-12-25T14:39:06.179Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境依赖"><a href="#环境依赖" class="headerlink" title="环境依赖"></a>环境依赖</h2><ul><li><p>JAVA_HOME = /usr/jdk64/jdk1.8.0_112</p></li><li><p>HADOOP_CONF_DIR = /etc/hadoop/conf  # 包括 HDFS_CONF_DIR and YARN_CONF_DIR</p></li></ul><p><code>注意</code>: 安装官方文档安装的 JDP 3.2 集群，自带如上相关依赖，本演示是 Flink on Yarn 模式。</p><h2 id="安装-JDP-安装包中自带的-Flink-软件包"><a href="#安装-JDP-安装包中自带的-Flink-软件包" class="headerlink" title="安装 JDP 安装包中自带的 Flink 软件包"></a>安装 JDP 安装包中自带的 Flink 软件包</h2><p>目前 JDP 仅支持 Redhat 系列 7.x 的操作系统，故我们使用 yum 安装 Flink。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install flink_3_2_0_0_108 -y</span><br></pre></td></tr></table></figure><h2 id="修改-Flink-默认配置文件"><a href="#修改-Flink-默认配置文件" class="headerlink" title="修改 Flink 默认配置文件"></a>修改 Flink 默认配置文件</h2><ol><li>在 JDP 集群中安装 Flink on Yarn 集群，仅需配置 env.java.home </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep -rn &quot;env.java.home&quot; /usr/jdp/current/flink/conf/flink-conf.yaml</span><br><span class="line">19:env.java.home: /usr/jdk64/jdk1.8.0_112</span><br></pre></td></tr></table></figure><p>JDP 集群中安装 Flink on Yarn <code>可选配置</code>，因为 JDP HADOOP_CONF_DIR 默认值和 Flink 配置默认值一致，故不需要配置。也可显示的在 config.sh 中 export HADOOP_CONF_DIR=xxx。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep -rn &quot;HADOOP_CONF_DIR&quot; config.sh | head -1</span><br><span class="line">19:export HADOOP_CONF_DIR=/etc/hadoop/conf</span><br></pre></td></tr></table></figure><ol start="2"><li>添加 Flink 用户，并授予 hdfs 组权限，方便 Flink 读写 HDFS。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd flink -g hdfs</span><br></pre></td></tr></table></figure><ol start="3"><li>创建 Flink 需要的log、run等目录。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/log/flink</span><br><span class="line">mkdir /var/run/flink</span><br></pre></td></tr></table></figure><ol start="4"><li>为新创建的 log、run 目录授予 Flink 读写权限</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown flink:root -R /var/log/flink</span><br><span class="line">chown flink:root -R /var/run/flink</span><br></pre></td></tr></table></figure><ol start="5"><li>初始化 flink 用户在 hdfs 的 home 目录</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su - flink</span><br><span class="line"></span><br><span class="line">hadoop fs -mkdir /user/flink</span><br></pre></td></tr></table></figure><h2 id="启动一个-Flink-集群运行于-YARN-上"><a href="#启动一个-Flink-集群运行于-YARN-上" class="headerlink" title="启动一个 Flink 集群运行于 YARN 上"></a>启动一个 Flink 集群运行于 YARN 上</h2><ol><li>需要切换到 flink，执行启动一个 Flink on Yarn 集群</li></ol><p>1.1 Start a long-running Flink cluster on YARN<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">su - flink</span><br><span class="line"></span><br><span class="line">cd /usr/jdp/current/flink</span><br><span class="line"></span><br><span class="line">./bin/yarn-session.sh -n 2 -tm 2024 -s 3 # 集群资源太小，配置不合理容易无法分配到资源，导致任务hang住。Specify the -s flag for the number of processing slots per Task Manager. We recommend to set the number of slots to the number of processors per machine.</span><br><span class="line"></span><br><span class="line">./bin/yarn-session.sh -n 2 -tm 2024 -s 3 -d # -d 表示后台运行。- The Flink YARN client has been started in detached mode. In order to stop Flink on YARN, use the following command or a YARN web interface to stop it:</span><br><span class="line">yarn application -kill application_1564483050501_0002</span><br></pre></td></tr></table></figure></p><p><code>注意</code>：启动时的提示<code>Setting HADOOP_CONF_DIR=/etc/hadoop/conf because no HADOOP_CONF_DIR was set.</code>，如果未设置 HADOOP_CONF_DIR，默认值 /etc/hadoop/conf；而 JDP 集群的 HADOOP_CONF_DIR 刚好在此处。</p><p>1.2 Run a Flink job on YARN</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -m yarn-cluster -p 4 -yjm 1024m -ytm 4096m ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure><ol start="3"><li>测试 flink on yarn 集群</li></ol><p>3.1 Examples 经典案例：WordCount</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run ./examples/batch/WordCount.jar</span><br><span class="line"></span><br><span class="line">flink list</span><br></pre></td></tr></table></figure><p>3.2 快速启动 flink sql clinet </p><p>进入 SQL Client</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/sql-client.sh embedded</span><br></pre></td></tr></table></figure><p>Running SQL Queries:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; SELECT &apos;Hello World&apos;;</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; SELECT name, COUNT(*) AS cnt FROM (VALUES (&apos;Bob&apos;), (&apos;Alice&apos;), (&apos;Greg&apos;), (&apos;Bob&apos;)) AS NameTable(name) GROUP BY name;</span><br></pre></td></tr></table></figure><p>退出 SQL Client</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; quit;</span><br></pre></td></tr></table></figure><p>提交 <code>Flink WordCount</code> 程序，因为 YARN 容器资源 limit 导致失败，需要调整启动集群的资源配比。</p><p><code>org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested resource type=[vcores] &lt; 0 or greater than maximum allowed allocation. Requested resource=&lt;memory:1024, vCores:4&gt;, maximum allowed allocation=&lt;memory:6656, vCores:3&gt;, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=&lt;memory:6656, vCores:3&gt;</code></p><ol start="4"><li>停止 Flink 集群，Flink 集群 -d 运行模式，需要通过 yarn 命令才能停止，具体如下：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yarn application --list</span><br><span class="line"></span><br><span class="line">yarn application -kill application_1556800373836_0018</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单演示如果在 JDP 平台快速部署 Flink 集群，JDP 发型包中默认已经自带 Flink rpm 包。yum 直接安装好之后配置一个env.java.home，就可以直接启动 flink on yarn 模式了，flink 本身模式比较多，Yarn Setup 支持 <code>Start a long-running Flink cluster on YARN</code>，<code>Run a Flink job on YARN</code>。今天我们演示的主要是前者，已经在 YARN 启动好一个 long-running 的 Flink 集群，在提交 Flink job 到此集群。主要遇到的问题是资源分配问题，导致任务无法申请到资源或请求资源超限而失败。Flink on Yarn 的 Job Manager 和 Task Manager 均运行在 YARN Container 中，资源请求受限于 YARN 对单个 Contrainer 的限制。由于我是个人环境，资源有限，安装过程我曾故意调整 YARN Container 资源 Limit 上限。如果是生产环境一般不会遇到此类问题，除非乱配置 Flink 提交的资源参数。</p><p>接下来，我会简单介绍一下，Flink sql-training 项目，帮助快速入门 Flink 的具体使用。</p><p>参考：</p><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">flink yarn setup</a></li><li><a href="https://github.com/ververica/sql-training" target="_blank" rel="noopener">flink sql-training</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/sqlClient.html" target="_blank" rel="noopener">flink sql client</a></li></ul>]]></content>
    
    <summary type="html">
    
      如何快速部署 Flink 集群在 JDP 平台上。
    
    </summary>
    
      <category term="BigData" scheme="http://itweet.github.io/categories/BigData/"/>
    
    
      <category term="2019" scheme="http://itweet.github.io/tags/2019/"/>
    
      <category term="Flink" scheme="http://itweet.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Fql-training</title>
    <link href="http://itweet.github.io/2019/07/10/fql-training/"/>
    <id>http://itweet.github.io/2019/07/10/fql-training/</id>
    <published>2019-07-10T03:28:30.000Z</published>
    <updated>2019-12-25T14:39:06.179Z</updated>
    
    <content type="html"><![CDATA[<p>今天，介绍一个快速入门 FusionDB 的一个<a href="https://github.com/FusionDB/fql-training" target="_blank" rel="noopener">GitHub工程</a>，使用 FQL 实现跨不同数据源的联邦查询功能。现提供 Docker 版的 FusionDB，安装有 Docker 的机器都可以快速体验 FusionDB 的功能。</p><ul><li>fql-training: <a href="https://github.com/FusionDB/fql-training" target="_blank" rel="noopener">https://github.com/FusionDB/fql-training</a></li><li>FusionDB Document: <a href="http://www.fusionlab.cn/zh-cn/fdb/quickstart.html" target="_blank" rel="noopener">http://www.fusionlab.cn/zh-cn/fdb/quickstart.html</a></li></ul><h1 id="FusionDB-FQL-Training"><a href="#FusionDB-FQL-Training" class="headerlink" title="FusionDB FQL Training"></a>FusionDB FQL Training</h1><p><strong>This repository provides a training for FusionDB FQL.</strong></p><p>In this training you will learn to:</p><ul><li>run SQL queries on FusionDB</li><li>run SQL++ queries on FusionDB</li><li>use FusionDB’s SQL JDBC Server</li><li>write the result of SQL queries to RDBMS (MySQL、PostgreSQL、Oracle)、S3、ADLS  and HDFS</li></ul><h3 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h3><ul><li>Remote HDFS or RDBMS (MySQL、PostgreSQL、Oracle)/S3/ADLS/GCP/OSS etc.</li><li>Optional: Jupyter Notebook、PSequel</li></ul><p>You <strong>only need <a href="https://hub.docker.com/r/fusiondb/fusiondb" target="_blank" rel="noopener">Docker</a></strong> to run this training. <br><br>You don’t need Java, Scala, or an IDE.</p><p>For more information, please refer to <a href="http://www.fusionlab.cn/zh-cn/fdb/quickstart.html" target="_blank" rel="noopener">FusionDB Document</a></p><h3 id="Quickstart"><a href="#Quickstart" class="headerlink" title="Quickstart"></a>Quickstart</h3><p>Fusiondb is a simple and powerful federated database engine.</p><ul><li>Start FusionDB</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name fdb -p 54322:54322 -itd fusiondb/fusiondb:0.1.0-beta</span><br></pre></td></tr></table></figure><ul><li>Check FusionDB server</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a|grep fdb</span><br><span class="line"></span><br><span class="line">lsof -i :54322</span><br></pre></td></tr></table></figure><ul><li>Psycopg2 connecting FusionDB</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import psycopg2</span><br><span class="line">import pandas as pd</span><br><span class="line">connection = psycopg2.connect(&quot;host=localhost port=54322 dbname=default user=fdb sslmode=disable&quot;)</span><br><span class="line">df = pd.read_sql(sql=&quot;SELECT * FROM VALUES (1, 1), (1, 2) AS t(a, b);&quot;, con=connection)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line">## Load mysql table</span><br><span class="line"></span><br><span class="line">df =pd.read_sql(sql=&quot;load &apos;mysql&apos; options(&apos;url&apos;=&apos;jdbc:mysql://localhost:53306/fdb_test&apos;,&apos;dbtable&apos;=&apos;person&apos;,&apos;user&apos;= &apos;root&apos;,&apos;password&apos;=&apos;root&apos;) AS mysql_t2;&quot;, con=connection)</span><br><span class="line">df =pd.read_sql(sql=&quot;SELECT * FROM mysql_t2;&quot;, con=connection)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line">## Load Postgres table</span><br><span class="line"></span><br><span class="line">df =pd.read_sql(sql=&quot;load &apos;postgresql&apos; options(&apos;url&apos;=&apos;jdbc:postgresql://localhost:15430/fdb&apos;,&apos;dbtable&apos;=&apos;person&apos;,&apos;user&apos;= &apos;fdb&apos;,&apos;password&apos;=&apos;fdb123&apos;) AS gp_t1;&quot;, con=connection)</span><br><span class="line">df =pd.read_sql(sql=&quot;SELECT * FROM gp_t1;&quot;, con=connection)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## MySQL Table Join PostgreSQL Table</span><br><span class="line"></span><br><span class="line">df =pd.read_sql(sql=&quot;CREATE table test as SELECT mysql_t2.* FROM mysql_t2 LEFT JOIN gp_t1 ON mysql_t2.id = gp_t1.id;&quot;, con=connection)</span><br><span class="line">df =pd.read_sql(sql=&quot;SELECT * FROM test;&quot;, con=connection)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line">## Load oracle table </span><br><span class="line"></span><br><span class="line">df =pd.read_sql(sql=&quot;load oracle options(&apos;url&apos;=&apos;jdbc:oracle:thin:SYSTEM/oracle@//localhost:49161/xe&apos;,&apos;dbtable&apos;=&apos;FDBTEST20&apos;,&apos;user&apos;= &apos;SYSTEM&apos;,&apos;password&apos;=&apos;oracle&apos;) AS ora_t1;&quot;, con=connection)</span><br><span class="line">df =pd.read_sql(sql=&quot;SELECT * FROM ora_t1 limit 10;&quot;, con=connection)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Load HDFS parquet</span><br><span class="line"></span><br><span class="line">df =pd.read_sql(sql=&quot;load &apos;hdfs://jdp-1:8020/tmp/spark-tpcds-data/web_site&apos; format parquet AS web_site;&quot;, con=connection)</span><br><span class="line">df =pd.read_sql(sql=&quot;SELECT * FROM web_site limit 10;&quot;, con=connection)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line">## Save table to hdfs</span><br><span class="line"></span><br><span class="line">df =pd.read_sql(sql=&quot;save overwrite web_site TO &apos;hdfs://jdp-1:8020/tmp/web_site_test&apos; FORMAT parquet;&quot;, con=connection)</span><br><span class="line">df =pd.read_sql(sql=&quot;load &apos;hdfs://jpd-1:8020/tmp/web_site_test&apos; format parquet AS mysql_t2_par;&quot;, con=connection)</span><br><span class="line">df =pd.read_sql(sql=&quot;SELECT * FROM mysql_t2_par limit 10;&quot;, con=connection)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p>For more information, please refer to <a href="http://www.fusionlab.cn/zh-cn/fdb/quickstart.html" target="_blank" rel="noopener">FusionDB Document</a></p>]]></content>
    
    <summary type="html">
    
      FusionDB FQL Training
    
    </summary>
    
      <category term="FusionDB" scheme="http://itweet.github.io/categories/FusionDB/"/>
    
    
      <category term="FusionDB®" scheme="http://itweet.github.io/tags/FusionDB%C2%AE/"/>
    
      <category term="2019" scheme="http://itweet.github.io/tags/2019/"/>
    
  </entry>
  
  <entry>
    <title>JDP Release 3.2.0</title>
    <link href="http://itweet.github.io/2019/06/04/jdp-release-3.2.0/"/>
    <id>http://itweet.github.io/2019/06/04/jdp-release-3.2.0/</id>
    <published>2019-06-03T18:25:36.000Z</published>
    <updated>2019-12-25T14:39:06.178Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Release-Note"><a href="#Release-Note" class="headerlink" title="Release Note"></a>Release Note</h1><p>JDP 3.2 是 3.x 系列中的第三个版本，经过一段漫长时间的研发测试，测试已达到我们对软件可用性与稳定性的基准，今天全票通过，决定发布正式版本。此版本拥有几大全新的功能以及组件，夯实 JDP 的的基石，立足于 3.1 版本，我们新增 FusionDB 跨多云协作分布式数据库引擎组件，未来 FusionDB 将做为 JDP 的核心组件进行研发，期待 FusionDB 未来真正的统一多源异构，跨云数据分析的联邦数据库的伟大愿景。目前一台 8G、6c 的虚拟机可安装和部署，实现在笔记本电脑也能体验 JDP 的完整功能。</p><p>为什么会如此漫长的时间，才发布 3.2.0.0 版本，各种缘由待我一一道来：</p><p>由于个人做了一个错误的决定，导致整个研发周期拉长，核心功能时间缩短，错过了最佳的研发时机，在此检讨。3.2.0.0 单方面新增对 Hadoop 3.1.1、Spark 2.4.0 版本的支持，由于社区还未支持，为了让 Spark 2.4.0 支持运行在 Hadoop3 上，新增适配 Hive3 以及 Hadoop3 的工作，导致花费了大量时间，并且进行了一些 Bugfix，而社区版 Spark 需要 3.0 才会支持 Hadoop3，目前还未发布。</p><p>如上，功能 release ，才开始着手 FusionDB 的设计与研发，导致没有抓住重点，致使 FusionDB 原本计划要实现的功能点，延期至下一个版本，未来版本我们会完全依托 Github project、issues 来统一管理与规划版本功能点和迭代，让更多小伙伴参与进来。</p><p>研发尾声，手忙脚乱，惊慌失措的做了一个半成品 CoreAI 组件，由于依赖了一堆 Docker 的东西，复杂度增加，经讨论决定此组件暂不 release，即不开放给 3.2.0.0 版本的用户使用，未来软件设计和软件质量符合我们对软件 release 基准时，在开放给用户使用。</p><p>如果，想要体验 JDP 3.2 版本，请访问<a href="http://www.fusionlab.cn/zh-cn/docs/intro/quickstart.html" target="_blank" rel="noopener">快速入门</a> 页面，提供云盘压缩包下载，您可以在私有网络的情况下安装使用 JDP。我们在此列出主要的版本变更，按组件进行分组：</p><ul><li>JDP 3.2 <ul><li>ambari-2.7.3.0</li><li>clickhouse 3.2.0.0-108-19.4.0</li><li>jdp-select 3.2.0.0-108</li><li>kafka 3.2.0.0-108-1.0.1</li><li>superset 3.2.0.0-108-0.26.3</li><li>zookeeper 3.2.0.0-108-3.4.6</li><li>flink 3.2.0.0-108-1.7.0</li><li>spark 3.2.0.0-108-2.4.0</li><li>livy2 3.2.0.0-108-0.5.0</li><li>hbase 3.2.0.0-108-2.0.2</li><li>hadoop 3.2.0.0-108-3.1.1</li><li>fusiondb 3.2.0.0-108-0.1.0-incubator</li><li>coreai 3.2.0.0-108-0.1.0-incubator</li></ul></li></ul><p>相关 JDP Roadmap 访问<a href="https://github.com/fusionlabcn/jdp" target="_blank" rel="noopener">Github</a></p><p>JDP 核心  </p><ul><li>企业级 Core Data &amp; Core AI 统一分析平台。</li><li>One-stop solution：开箱即用 &amp; 简单 &amp; 易用。</li><li>高性能：万亿数据秒级响应，提供PB级数据存储。</li><li>扩展性：规模化部署，可达几百台集群规模。</li><li>可视化：可视化的数据收集、数据存储、数据分析、数据报表。</li><li>可靠性：集群提供副本容错机制，硬件故障不会造成数据丢失。</li><li>简单运维：日志、监控、报警、配置、服务一栈式管理。</li><li>简单易用：提供标准SQL，拖拽式数据分析。</li><li>全流程数据闭环：Load Data -&gt; FQL Analysis -&gt; Save Data</li><li>FusionDB: 强大的批流统一、异构数据融合分布式数据库引擎。</li><li>真正的统一多源异构，跨多云协作、分析、处理数据海量数据的联邦数据库。</li><li>FQL for Everyone &amp; All in FQL 愿景。</li></ul><h1 id="Next"><a href="#Next" class="headerlink" title="Next"></a>Next</h1><h2 id="3-1-0-0"><a href="#3-1-0-0" class="headerlink" title="3.1.0.0"></a>3.1.0.0</h2><p>JDP 未来计划新增一些简单、易用、强大的 Batch &amp; Stream 统一的一体化功能。全新的设计，在混口饭吃的工作中，抽出时间在设计与实现下一代数据流平台，而且 JDP 定位为一款真正企业级可用的一体化数据流平台，需要实现足够简单且强大的数据流处理和管理能力，这是我们设计 JDP 的初衷。</p><p>JDP 设计的组件和相关技术非常驳杂，整个研发过程漫长且耗费硬件资源，开发维护不易，我们已经在尽力的努力克服外部困难，规范化整个 Devops 流程，希望能改善目前效率的问题。</p><p>next JDP 版本中，会新增 Hadoop 3.2.x 版本以及 Spark、Flink 相关组件，一切的原因是为了更好；我们的流分析引擎核心 Runtime 支持 Spark 、Flink，用户无需感知这些 Runtime 技术，一体化的部署、运维、监控均有 JDP 实现，外部接口兼容标准的数据库协议，大大降低用户的成本，我们会有一层自己的数据库 DSL 设计，我们称为 FQL，其实是 SQL++，就这样。</p><p>JDP 3.2 中计划，会启动另外一个 <code>stack</code> 孵化工作，探索 k8s + ai 技术，毕竟行行转 ai，容器我一直比较抵触，体积超大，带状态服务支持弱，但 ai 各种 py 库，真的很适合容器化，升级方便，维护性成本高，难以规模化部署运维，涉及网络、存储、计算、调度、容器、分布式等技术，显然是庞然大物，国内专业玩 k8s 的公司，貌似基本难有做大的，企业级市场，基础设施难挣钱，业务挣钱，难有企业重视，基本都是使用层面，扯远啦；回归正题，JDP 需要去很好的解决这些问题，一切是为了更好，实现一体化流分析平台的价值，容器化 JDP 某些模块，长期看是利好的，可以降低成本。。。</p><p>未来的时间，我会逐渐披露更多细节，敬请期待。。。</p><h2 id="3-2-0-0"><a href="#3-2-0-0" class="headerlink" title="3.2.0.0"></a>3.2.0.0</h2><p>我们在 3.1.0.0 发布时的 Release 披露了我们 3.2.0.0 的 roadmap，我们来一一分析我们做到了那些？顺带披露一下我们 Next 版本的核心特性。</p><p>如 3.1.0.0 所述，我们 FusionDB 的设想由来已久，只是一直没有提上日程，而在 3.2.0.0 版本中我们终于 release 第一个 FusionDB 0.1.0 版本，我们做到了+1，我们终于开始做属于自己的分布式联邦数据库系统了，欧耶！</p><p>如 3.1.0.0 所述，将支持 Hadoop3 版本，其他组件完成相应适配融合，一下子把整个 JDP 变重了，不保证未来不会移除此组件，过程困难重重。我们做到了+1，欧耶！</p><p>如 3.1.0.0 所述，底层 Runtime 支持 Spark 、Flink，未做到，目前仅构建出 Flink，未做适配工作，技术栈选择需慎重，我们会综合考虑，是否未来会启用，我们未做到-1，啊，好失败！</p><p>如 3.1.0.0 所述，会启动另外一个 <code>stack</code> 孵化工作，探索 k8s + ai 技术，起名 CoreAI，未正式 release。容器技术选择需谨慎，会影响你的产品快速分发的，相信我，还是需要慎重选择呀。。。，我们未做到-1，啊，好失败！</p><p>Next</p><ul><li><p>Data Lake Storage</p><ul><li>Unified storage layer，Support S3,OSS,ADLS,GCP</li><li>ACID transactions</li><li>Time Travel (data versioning) Transaction log records details Cloud-Native workload</li></ul></li><li><p>FusionDB</p><ul><li>Standalone metastore server</li><li>Supports UPDATE, DELETE and MERGE</li><li>Unified batch &amp; streaming syntax</li><li>Predicate Pushdown in DataSource</li><li>Add stream compute management</li><li>Adaptive execution</li></ul></li></ul><p>未来的时间，我们会更多专注于分布式数据库、分布式计算引擎，致力于提供易于使用、高性能、简单的统一分析平台而努力。</p><p>文末，附 JDP 3.2 近照一张。</p><p><img src="http://www.fusionlab.cn/zh-cn/docs/intro/img/ambari%E2%80%93dashboard.png" alt></p><p>单机、分布式系统都能一体化的管理。。。</p><p>[1] JDP 官网: <a href="http://www.fusionlab.cn/" target="_blank" rel="noopener">http://www.fusionlab.cn/</a></p><p>[2] JDP 快速启动：<a href="http://www.fusionlab.cn/zh-cn/docs/intro/quickstart.html" target="_blank" rel="noopener">http://www.fusionlab.cn/zh-cn/docs/intro/quickstart.html</a></p>]]></content>
    
    <summary type="html">
    
      JDP 3.2 是 3.x 系列中的第三个版本，经过一段漫长时间的研发测试，测试已达到我们对软件可用性与稳定性的基准，今天全票通过，决定发布正式版本。
    
    </summary>
    
      <category term="JDP" scheme="http://itweet.github.io/categories/JDP/"/>
    
    
      <category term="JDP,3.2.0" scheme="http://itweet.github.io/tags/JDP-3-2-0/"/>
    
  </entry>
  
  <entry>
    <title>JDP Release 3.1.0</title>
    <link href="http://itweet.github.io/2018/12/26/jdp-release-3.1.0/"/>
    <id>http://itweet.github.io/2018/12/26/jdp-release-3.1.0/</id>
    <published>2018-12-26T08:35:36.000Z</published>
    <updated>2019-12-25T14:39:06.177Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Release-Note"><a href="#Release-Note" class="headerlink" title="Release Note"></a>Release Note</h1><p>JDP 3.1 是 3.x 系列中的第二个版本，经过一段长久时间的研发测试，今天决定发布正式版本。此版本没有任何全新的features，立足于 3.1 版本，我们把 mpack 深度与 Ambari 融合，真正做到一体化的产品体验。Ambari 默认 Stack 是 JDP，支持的版本是3.0.0和3.1.0；Stack 中组件的全新升级，均使用最新的稳定版本。新版本对于资源的使用量进行了优化，目前一台4g、2c的虚拟机可安装和部署，实现在笔记本电脑也能体验 JDP 的完整功能。</p><p>要下载 JDP 3.1，请访问<a href="http://www.fusionlab.cn/zh-cn/docs/intro/quickstart.html" target="_blank" rel="noopener">快速入门</a> 页面，提供云盘压缩包下载，您可以在私有网络的情况下安装使用 JDP。我们在此列出主要的版本变更，按组件进行分组：</p><ul><li>JDP 3.1 <ul><li>ambari-2.7.0.0-0</li><li>clickhouse 3.1.0.0-108-18.14.13</li><li>jdp-select 3.1.0.0-108</li><li>kafka 3.1.0.0-108-1.0.1</li><li>nifi 3.1.0.0-108-1.7.0</li><li>superset 3.1.0.0-108-0.26.3</li><li>zookeeper 3.1.0.0-108-3.4.6</li></ul></li></ul><p>相关 JDP Roadmap 访问<a href="https://github.com/fusionlabcn/jdp" target="_blank" rel="noopener">Github</a></p><p>JDP 核心  </p><ul><li>一体化的流分析平台，在未来 3.3 版本中。</li><li>数据流 Kafka，实时处理引擎</li><li>海量数据即席查询 ClickHouse</li><li>数据可视化 Superset</li><li>分布式协调服务 Zookeeper</li><li>利用 JDP 实现一站式企业级数据仓库平台</li></ul><h1 id="Next"><a href="#Next" class="headerlink" title="Next"></a>Next</h1><p>JDP 未来计划新增一些简单、易用、强大的 Batch &amp; Stream 统一的一体化功能。全新的设计，在混口饭吃的工作中，抽出时间在设计与实现下一代数据流平台，而且 JDP 定位为一款真正企业级可用的一体化数据流平台，需要实现足够简单且强大的数据流处理和管理能力，这是我们设计 JDP 的初衷。</p><p>JDP 设计的组件和相关技术非常驳杂，整个研发过程漫长且耗费硬件资源，开发维护不易，我们已经在尽力的努力克服外部困难，规范化整个 Devops 流程，希望能改善目前效率的问题。</p><p>next JDP 版本中，会新增 Hadoop 3.2.x 版本以及 Spark、Flink 相关组件，一切的原因是为了更好；我们的流分析引擎核心 Runtime 支持 Spark 、Flink，用户无需感知这些 Runtime 技术，一体化的部署、运维、监控均有 JDP 实现，外部接口兼容标准的数据库协议，大大降低用户的成本，我们会有一层自己的数据库 DSL 设计，我们称为 FQL，其实是 SQL++，就这样。</p><p>JDP 3.3 中计划，会启动另外一个 <code>stack</code> 孵化工作，探索 k8s + ai 技术，毕竟行行转 ai，容器我一直比较抵触，体积超大，带状态服务支持弱，但 ai 各种 py 库，真的很适合容器化，升级方便，维护性成本高，难以规模化部署运维，涉及网络、存储、计算、调度、容器、分布式等技术，显然是庞然大物，国内专业玩 k8s 的公司，貌似基本难有做大的，企业级市场，基础设施难挣钱，业务挣钱，难有企业重视，基本都是使用层面，扯远啦；回归正题，JDP 需要去很好的解决这些问题，一切是为了更好，实现一体化流分析平台的价值，容器化 JDP 某些模块，长期看是利好的，可以降低成本。。。</p><h2 id="未来的时间，我会逐渐披露更多细节，敬请期待。。。"><a href="#未来的时间，我会逐渐披露更多细节，敬请期待。。。" class="headerlink" title="未来的时间，我会逐渐披露更多细节，敬请期待。。。"></a>未来的时间，我会逐渐披露更多细节，敬请期待。。。</h2><p>好久没更新啦，扯句闲话，我遇到我偶像 Linus Torvalds 年轻时刚去美国硅谷工作时一样的情况，境遇如此相似；以后尽量产出一些高质量的内容，在分享吧，码子不易，文末附 JDP 3.1 美图一张。</p><p><img src="http://www.fusionlab.cn/zh-cn/docs/intro/img/ambari%E2%80%93dashboard.png" alt></p><p>单机、分布式系统都能一体化的管理。。。</p><p>[1] JDP 官网: <a href="http://www.fusionlab.cn/" target="_blank" rel="noopener">http://www.fusionlab.cn/</a></p><p>[2] JDP 快速启动：<a href="http://www.fusionlab.cn/zh-cn/docs/intro/quickstart.html" target="_blank" rel="noopener">http://www.fusionlab.cn/zh-cn/docs/intro/quickstart.html</a></p>]]></content>
    
    <summary type="html">
    
      JDP 3.1 是 3.x 系列中的第二个版本，经过一段长久时间的研发测试，今天决定发布正式版本。
    
    </summary>
    
      <category term="JDP" scheme="http://itweet.github.io/categories/JDP/"/>
    
    
      <category term="JDP,3.1.0" scheme="http://itweet.github.io/tags/JDP-3-1-0/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow-on-apache-hadoop-yarn</title>
    <link href="http://itweet.github.io/2018/09/06/tensorflow-on-apache-hadoop-yarn/"/>
    <id>http://itweet.github.io/2018/09/06/tensorflow-on-apache-hadoop-yarn/</id>
    <published>2018-09-05T22:15:23.000Z</published>
    <updated>2019-12-25T14:39:06.177Z</updated>
    
    <content type="html"><![CDATA[<p>现在人工智能正处于风口浪尖，大数据和人工智能注定要融合，如何融合，以什么方式融合？</p><p>如今大家都在探索阶段，每个技术型的公司，都有自己的人工智能与大数据融合的方案。</p><p>今天，我们就来介绍一下，大数据领域最核心的Apache Hadoop和人工智能技术融合进展。</p><p>今天我们介绍 Apache Yarn 是如何融合目前最热门的深度学习框架TensorFlow以及其他框架。</p><p>Tensorflow on Apache Hadoop YARN.pdf </p><p>云盘：链接:<a href="https://pan.baidu.com/s/1v2loT8O30R4RyomM5wTt-A" target="_blank" rel="noopener">https://pan.baidu.com/s/1v2loT8O30R4RyomM5wTt-A</a>  密码:azc7</p>]]></content>
    
    <summary type="html">
    
      今天我们介绍 Apache Yarn 是如何融合目前最热门的深度学习框架TensorFlow以及其他框架
    
    </summary>
    
      <category term="AI" scheme="http://itweet.github.io/categories/AI/"/>
    
    
      <category term="Tensorflow" scheme="http://itweet.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Parquet-library-fatal-error</title>
    <link href="http://itweet.github.io/2018/08/26/parquet-library-fatal-error/"/>
    <id>http://itweet.github.io/2018/08/26/parquet-library-fatal-error/</id>
    <published>2018-08-26T06:58:41.000Z</published>
    <updated>2019-12-25T14:39:06.176Z</updated>
    
    <content type="html"><![CDATA[<p>parquet-Hadoop 1.9.0 大量close-wait问题。</p><p>一个开源基础库引发的严重产品事故，整个事件场内block超过4小时以上。</p><p>我们某服务部署上线，界面化的DAG任务，主要跑机器学习全流程作业，运行一段时间，发现后端数据平台查询、预览数据功能全部不可用，导致大量上游业务系统block住。</p><p>报错关键信息：</p><ol><li>UnknowHostException</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.net.UnknownHostException: node199mm1002</span><br></pre></td></tr></table></figure><p>排查Hadoop集群平台是否正常，如上主机是否可到达，发现均正常，集群无问题。</p><ol start="2"><li>Too many open files</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java.net.SocketException: Too many open files; Host Details: local host is : &quot;java.net.UnknownHostException: tserver-98989-bmhtk: tserver-98989-bmhtk: System error&quot;: destination host is: &quot;node199mm1002&quot;:25000; </span><br><span class="line">  at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:776)</span><br></pre></td></tr></table></figure><p>操作系统，根据ulimit -a查看系统fd配置，主机fd上限为640000，tserver容器的fd上限为65536。通过kubectl进入tserver容器，使用命令“watch ‘ss | grep CLOSE-WAIT | wc -l’”查看，发现65278个close-wait，快崩了。</p><p><img src="https://www.itweet.cn/screenshots/close-wait.png" alt="close wait"></p><p>基本可确定是大量，close-wait连接泄漏问题，对连接泄漏列表进一步分析：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ss | grep CLOSE-WAIT &gt; close_wait.txt</span><br><span class="line">cat close_wait.txt| grep CLOSE_WAIT | grep 50010|wc -l</span><br><span class="line">cat close_wait.txt| grep CLOSE_WAIT | grep http|wc -l</span><br></pre></td></tr></table></figure><p>发现两个特征，一个是属于http相关相关close-wait，占比2%，剩余的全是连接50010端口出现的close-wait，查询Hadoop相关端口，发现50010是DataNode的数据转发端口，初步怀疑是代码连接Hadoop DataNode查询数据没有关闭流导致。</p><ol start="3"><li>DataManager Service：listFiles error IOException  </li></ol><p>排查DataManager Service相关代码，发现listFiles函数，主要功能，读取HDFS获取某个目录下的parquet集合，然后进行读取parquet的meta信息，前端进行展示大小，行数，还会读取任意问题进行预览，或者随机预览数据。<br>并未发现有任何异常情况，系统每次调用此接方法导致阻塞，一步步跟进调试，发现已经进入parquet相关的代码。</p><p>经过认证检查每一行代码，发现流相关操作均进行了关闭，并没有任何问题。</p><p>验证思考：缩小问题范围，对listFiles函数，调用的全部方法进行最小化单元测试，最终在开发环境preview方法中，实现了连接HDFS读取parquet文件，使用rest接口，单独调用此方法，每次调用close-wait都会增加2，并且长期存在。</p><p>监控命令“watch ‘ss | grep CLOSE-WAIT | wc -l’”，疯狂访问，一直增加不曾下降。</p><p>猜测是Parquet依赖库，有连接泄漏问题，于是向社区求助，我们引入的parquet 1.9.0上下几个版本的commit以及bugfix全看了一遍，重点关注close stream相关。</p><p>PR <a href="https://github.com/apache/parquet-mr/pull/388" target="_blank" rel="noopener">PARQUET-783</a> 提到相关问题：</p><p>···<br>This PR addresses <a href="https://issues.apache.org/jira/browse/PARQUET-783" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/PARQUET-783</a>.</p><p>ParquetFileReader opens a SeekableInputStream to read a footer. In the process, it opens a new FSDataInputStream and wraps it. However, H2SeekableInputStream does not override the close method. Therefore, when ParquetFileReader closes it, the underlying FSDataInputStream is not closed. As a result, these stale connections can exhaust a clusters’ data nodes’ connection resources and lead to mysterious HDFS read failures in HDFS clients, e.g.</p><p>org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-905337612-172.16.70.103-1444328960665:blk_1720536852_646811517<br>···</p><p>根据社区，反馈：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">downgrade parquet to 1.8.2 to fix 1.9.0 connection leaks</span><br></pre></td></tr></table></figure><p>降级parquet库版本为1.8.2修复此问题，升级最新版本也是能解决问题，不过部分接口大动，导致不兼容，经过考虑决定降级库，而不该任何代码解决问题。</p><p>修复代码 by <a href="https://github.com/apache/parquet-mr/pull/388/files" target="_blank" rel="noopener">https://github.com/apache/parquet-mr/pull/388/files</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void close() throws IOException &#123;</span><br><span class="line">  stream.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>整个过程，由于全部服务容器化部署，导致排查问题周期比较长，获取日志还是比较原始的命令行方式，门槛较高，由于在现场发现问题，不能及时进行问题的追溯，只能根据只言片语的反馈进行问题追踪。如果不是在开发环境，复现此问题，整个问题排查起来异常困难，开始思考是否有tracking工具，可以快速定位问题。代码的最小化单元测试，只能定位范围，最终一直无法确定是某个函数直接导致的问题，代码编写中函数功能单一性拆分是一门学问。</p><p>期间，重启大发又一次发挥了奇效。在开发环境复现问题，并修复bug，patch修复各个产品线的不同版本。</p><p>如果，你还在用有连接泄漏问题的Parque版本，快快升级吧。</p>]]></content>
    
    <summary type="html">
    
      一个开源基础库引发的严重产品事故，整个事件场内block超过4小时以上。
    
    </summary>
    
      <category term="BigData" scheme="http://itweet.github.io/categories/BigData/"/>
    
    
      <category term="Parquet" scheme="http://itweet.github.io/tags/Parquet/"/>
    
  </entry>
  
</feed>
